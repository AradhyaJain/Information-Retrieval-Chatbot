{
  "best_metric": 0.6614677064587082,
  "best_model_checkpoint": "./trained_model/checkpoint-10002",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 10002,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001999600079984003,
      "grad_norm": 8.486483573913574,
      "learning_rate": 4.9966673332000266e-05,
      "loss": 2.29,
      "step": 10
    },
    {
      "epoch": 0.003999200159968006,
      "grad_norm": 23.02979850769043,
      "learning_rate": 4.9933346664000536e-05,
      "loss": 2.3425,
      "step": 20
    },
    {
      "epoch": 0.00599880023995201,
      "grad_norm": 8.020190238952637,
      "learning_rate": 4.99000199960008e-05,
      "loss": 2.3202,
      "step": 30
    },
    {
      "epoch": 0.007998400319936013,
      "grad_norm": 6.065441608428955,
      "learning_rate": 4.986669332800107e-05,
      "loss": 2.2276,
      "step": 40
    },
    {
      "epoch": 0.009998000399920015,
      "grad_norm": 6.884082794189453,
      "learning_rate": 4.9833366660001333e-05,
      "loss": 2.2507,
      "step": 50
    },
    {
      "epoch": 0.01199760047990402,
      "grad_norm": 7.542108058929443,
      "learning_rate": 4.9800039992001604e-05,
      "loss": 2.2436,
      "step": 60
    },
    {
      "epoch": 0.013997200559888023,
      "grad_norm": 8.273296356201172,
      "learning_rate": 4.976671332400187e-05,
      "loss": 2.2028,
      "step": 70
    },
    {
      "epoch": 0.015996800639872025,
      "grad_norm": 7.5410661697387695,
      "learning_rate": 4.973338665600214e-05,
      "loss": 2.2457,
      "step": 80
    },
    {
      "epoch": 0.017996400719856028,
      "grad_norm": 6.637073040008545,
      "learning_rate": 4.97000599880024e-05,
      "loss": 2.1048,
      "step": 90
    },
    {
      "epoch": 0.01999600079984003,
      "grad_norm": 7.577936172485352,
      "learning_rate": 4.9666733320002665e-05,
      "loss": 2.1358,
      "step": 100
    },
    {
      "epoch": 0.021995600879824034,
      "grad_norm": 6.453656196594238,
      "learning_rate": 4.9633406652002935e-05,
      "loss": 2.1912,
      "step": 110
    },
    {
      "epoch": 0.02399520095980804,
      "grad_norm": 7.3829121589660645,
      "learning_rate": 4.96000799840032e-05,
      "loss": 2.1957,
      "step": 120
    },
    {
      "epoch": 0.025994801039792043,
      "grad_norm": 8.003046035766602,
      "learning_rate": 4.956675331600347e-05,
      "loss": 2.1056,
      "step": 130
    },
    {
      "epoch": 0.027994401119776045,
      "grad_norm": 6.657829284667969,
      "learning_rate": 4.953342664800373e-05,
      "loss": 2.1079,
      "step": 140
    },
    {
      "epoch": 0.029994001199760048,
      "grad_norm": 6.863305568695068,
      "learning_rate": 4.9500099980004e-05,
      "loss": 2.0691,
      "step": 150
    },
    {
      "epoch": 0.03199360127974405,
      "grad_norm": 7.502652168273926,
      "learning_rate": 4.9466773312004266e-05,
      "loss": 2.0581,
      "step": 160
    },
    {
      "epoch": 0.033993201359728054,
      "grad_norm": 6.225324630737305,
      "learning_rate": 4.9433446644004536e-05,
      "loss": 2.0105,
      "step": 170
    },
    {
      "epoch": 0.035992801439712056,
      "grad_norm": 7.140869140625,
      "learning_rate": 4.94001199760048e-05,
      "loss": 1.9497,
      "step": 180
    },
    {
      "epoch": 0.03799240151969606,
      "grad_norm": 7.257124423980713,
      "learning_rate": 4.936679330800506e-05,
      "loss": 2.0594,
      "step": 190
    },
    {
      "epoch": 0.03999200159968006,
      "grad_norm": 8.895133018493652,
      "learning_rate": 4.9333466640005333e-05,
      "loss": 1.9021,
      "step": 200
    },
    {
      "epoch": 0.041991601679664065,
      "grad_norm": 8.184698104858398,
      "learning_rate": 4.93001399720056e-05,
      "loss": 2.0463,
      "step": 210
    },
    {
      "epoch": 0.04399120175964807,
      "grad_norm": 7.4633002281188965,
      "learning_rate": 4.926681330400587e-05,
      "loss": 1.8619,
      "step": 220
    },
    {
      "epoch": 0.04599080183963208,
      "grad_norm": 10.627445220947266,
      "learning_rate": 4.923348663600613e-05,
      "loss": 1.9402,
      "step": 230
    },
    {
      "epoch": 0.04799040191961608,
      "grad_norm": 6.723014831542969,
      "learning_rate": 4.92001599680064e-05,
      "loss": 1.8906,
      "step": 240
    },
    {
      "epoch": 0.04999000199960008,
      "grad_norm": 8.850643157958984,
      "learning_rate": 4.9166833300006664e-05,
      "loss": 1.9113,
      "step": 250
    },
    {
      "epoch": 0.051989602079584085,
      "grad_norm": 7.870445251464844,
      "learning_rate": 4.9133506632006935e-05,
      "loss": 1.9412,
      "step": 260
    },
    {
      "epoch": 0.05398920215956809,
      "grad_norm": 7.138870716094971,
      "learning_rate": 4.9100179964007205e-05,
      "loss": 1.9972,
      "step": 270
    },
    {
      "epoch": 0.05598880223955209,
      "grad_norm": 10.539412498474121,
      "learning_rate": 4.906685329600747e-05,
      "loss": 1.9351,
      "step": 280
    },
    {
      "epoch": 0.05798840231953609,
      "grad_norm": 13.7241792678833,
      "learning_rate": 4.903352662800774e-05,
      "loss": 1.8579,
      "step": 290
    },
    {
      "epoch": 0.059988002399520096,
      "grad_norm": 6.764505863189697,
      "learning_rate": 4.9000199960008e-05,
      "loss": 1.7772,
      "step": 300
    },
    {
      "epoch": 0.0619876024795041,
      "grad_norm": 8.268332481384277,
      "learning_rate": 4.896687329200827e-05,
      "loss": 1.8775,
      "step": 310
    },
    {
      "epoch": 0.0639872025594881,
      "grad_norm": 9.532472610473633,
      "learning_rate": 4.8933546624008536e-05,
      "loss": 1.8566,
      "step": 320
    },
    {
      "epoch": 0.06598680263947211,
      "grad_norm": 9.410441398620605,
      "learning_rate": 4.89002199560088e-05,
      "loss": 1.7573,
      "step": 330
    },
    {
      "epoch": 0.06798640271945611,
      "grad_norm": 8.815823554992676,
      "learning_rate": 4.886689328800907e-05,
      "loss": 1.8349,
      "step": 340
    },
    {
      "epoch": 0.06998600279944012,
      "grad_norm": 10.014233589172363,
      "learning_rate": 4.8833566620009333e-05,
      "loss": 1.7115,
      "step": 350
    },
    {
      "epoch": 0.07198560287942411,
      "grad_norm": 7.97587776184082,
      "learning_rate": 4.8800239952009604e-05,
      "loss": 1.7167,
      "step": 360
    },
    {
      "epoch": 0.07398520295940812,
      "grad_norm": 10.347760200500488,
      "learning_rate": 4.876691328400987e-05,
      "loss": 1.9247,
      "step": 370
    },
    {
      "epoch": 0.07598480303939212,
      "grad_norm": 9.23169231414795,
      "learning_rate": 4.873358661601014e-05,
      "loss": 1.6842,
      "step": 380
    },
    {
      "epoch": 0.07798440311937613,
      "grad_norm": 8.449100494384766,
      "learning_rate": 4.87002599480104e-05,
      "loss": 1.6756,
      "step": 390
    },
    {
      "epoch": 0.07998400319936012,
      "grad_norm": 6.70433235168457,
      "learning_rate": 4.866693328001067e-05,
      "loss": 1.4329,
      "step": 400
    },
    {
      "epoch": 0.08198360327934413,
      "grad_norm": 9.499358177185059,
      "learning_rate": 4.8633606612010935e-05,
      "loss": 1.9025,
      "step": 410
    },
    {
      "epoch": 0.08398320335932813,
      "grad_norm": 9.831555366516113,
      "learning_rate": 4.86002799440112e-05,
      "loss": 1.8554,
      "step": 420
    },
    {
      "epoch": 0.08598280343931214,
      "grad_norm": 8.456974983215332,
      "learning_rate": 4.856695327601147e-05,
      "loss": 1.6412,
      "step": 430
    },
    {
      "epoch": 0.08798240351929613,
      "grad_norm": 6.636467933654785,
      "learning_rate": 4.853362660801173e-05,
      "loss": 1.5953,
      "step": 440
    },
    {
      "epoch": 0.08998200359928014,
      "grad_norm": 14.0305757522583,
      "learning_rate": 4.8500299940012e-05,
      "loss": 1.4538,
      "step": 450
    },
    {
      "epoch": 0.09198160367926415,
      "grad_norm": 10.056876182556152,
      "learning_rate": 4.8466973272012266e-05,
      "loss": 1.6688,
      "step": 460
    },
    {
      "epoch": 0.09398120375924815,
      "grad_norm": 11.251740455627441,
      "learning_rate": 4.8433646604012536e-05,
      "loss": 1.6104,
      "step": 470
    },
    {
      "epoch": 0.09598080383923216,
      "grad_norm": 11.618427276611328,
      "learning_rate": 4.84003199360128e-05,
      "loss": 1.6278,
      "step": 480
    },
    {
      "epoch": 0.09798040391921616,
      "grad_norm": 11.514531135559082,
      "learning_rate": 4.836699326801307e-05,
      "loss": 1.525,
      "step": 490
    },
    {
      "epoch": 0.09998000399920016,
      "grad_norm": 12.38217830657959,
      "learning_rate": 4.833366660001333e-05,
      "loss": 1.7233,
      "step": 500
    },
    {
      "epoch": 0.10197960407918416,
      "grad_norm": 9.643281936645508,
      "learning_rate": 4.83003399320136e-05,
      "loss": 1.5336,
      "step": 510
    },
    {
      "epoch": 0.10397920415916817,
      "grad_norm": 9.240829467773438,
      "learning_rate": 4.826701326401387e-05,
      "loss": 1.497,
      "step": 520
    },
    {
      "epoch": 0.10597880423915217,
      "grad_norm": 11.246994972229004,
      "learning_rate": 4.823368659601413e-05,
      "loss": 1.5234,
      "step": 530
    },
    {
      "epoch": 0.10797840431913618,
      "grad_norm": 10.207000732421875,
      "learning_rate": 4.82003599280144e-05,
      "loss": 1.9054,
      "step": 540
    },
    {
      "epoch": 0.10997800439912017,
      "grad_norm": 14.543898582458496,
      "learning_rate": 4.8167033260014664e-05,
      "loss": 1.5054,
      "step": 550
    },
    {
      "epoch": 0.11197760447910418,
      "grad_norm": 8.36032485961914,
      "learning_rate": 4.8133706592014935e-05,
      "loss": 1.5388,
      "step": 560
    },
    {
      "epoch": 0.11397720455908818,
      "grad_norm": 13.23294448852539,
      "learning_rate": 4.81003799240152e-05,
      "loss": 1.5393,
      "step": 570
    },
    {
      "epoch": 0.11597680463907219,
      "grad_norm": 9.620367050170898,
      "learning_rate": 4.806705325601547e-05,
      "loss": 1.4854,
      "step": 580
    },
    {
      "epoch": 0.11797640471905618,
      "grad_norm": 11.607989311218262,
      "learning_rate": 4.803372658801573e-05,
      "loss": 1.8466,
      "step": 590
    },
    {
      "epoch": 0.11997600479904019,
      "grad_norm": 9.428267478942871,
      "learning_rate": 4.8000399920015995e-05,
      "loss": 1.4937,
      "step": 600
    },
    {
      "epoch": 0.1219756048790242,
      "grad_norm": 9.171460151672363,
      "learning_rate": 4.7967073252016266e-05,
      "loss": 1.6304,
      "step": 610
    },
    {
      "epoch": 0.1239752049590082,
      "grad_norm": 11.798443794250488,
      "learning_rate": 4.793374658401653e-05,
      "loss": 1.4827,
      "step": 620
    },
    {
      "epoch": 0.1259748050389922,
      "grad_norm": 8.675115585327148,
      "learning_rate": 4.79004199160168e-05,
      "loss": 1.6025,
      "step": 630
    },
    {
      "epoch": 0.1279744051189762,
      "grad_norm": 18.90923500061035,
      "learning_rate": 4.786709324801706e-05,
      "loss": 1.6164,
      "step": 640
    },
    {
      "epoch": 0.1299740051989602,
      "grad_norm": 10.327335357666016,
      "learning_rate": 4.783376658001733e-05,
      "loss": 1.4738,
      "step": 650
    },
    {
      "epoch": 0.13197360527894422,
      "grad_norm": 7.653177738189697,
      "learning_rate": 4.78004399120176e-05,
      "loss": 1.5338,
      "step": 660
    },
    {
      "epoch": 0.1339732053589282,
      "grad_norm": 12.2083740234375,
      "learning_rate": 4.776711324401786e-05,
      "loss": 1.5009,
      "step": 670
    },
    {
      "epoch": 0.13597280543891221,
      "grad_norm": 9.088950157165527,
      "learning_rate": 4.773378657601813e-05,
      "loss": 1.4126,
      "step": 680
    },
    {
      "epoch": 0.13797240551889622,
      "grad_norm": 10.023622512817383,
      "learning_rate": 4.7700459908018394e-05,
      "loss": 1.8376,
      "step": 690
    },
    {
      "epoch": 0.13997200559888023,
      "grad_norm": 12.354560852050781,
      "learning_rate": 4.7667133240018664e-05,
      "loss": 1.3388,
      "step": 700
    },
    {
      "epoch": 0.14197160567886422,
      "grad_norm": 10.93286418914795,
      "learning_rate": 4.763380657201893e-05,
      "loss": 1.5739,
      "step": 710
    },
    {
      "epoch": 0.14397120575884823,
      "grad_norm": 7.3670477867126465,
      "learning_rate": 4.76004799040192e-05,
      "loss": 1.6356,
      "step": 720
    },
    {
      "epoch": 0.14597080583883223,
      "grad_norm": 11.679475784301758,
      "learning_rate": 4.756715323601946e-05,
      "loss": 1.48,
      "step": 730
    },
    {
      "epoch": 0.14797040591881624,
      "grad_norm": 10.872512817382812,
      "learning_rate": 4.753382656801973e-05,
      "loss": 1.7101,
      "step": 740
    },
    {
      "epoch": 0.14997000599880023,
      "grad_norm": 8.5926513671875,
      "learning_rate": 4.7500499900019995e-05,
      "loss": 1.4482,
      "step": 750
    },
    {
      "epoch": 0.15196960607878424,
      "grad_norm": 15.137410163879395,
      "learning_rate": 4.746717323202026e-05,
      "loss": 1.6292,
      "step": 760
    },
    {
      "epoch": 0.15396920615876825,
      "grad_norm": 7.711568832397461,
      "learning_rate": 4.743384656402053e-05,
      "loss": 1.596,
      "step": 770
    },
    {
      "epoch": 0.15596880623875226,
      "grad_norm": 8.692476272583008,
      "learning_rate": 4.74005198960208e-05,
      "loss": 1.6412,
      "step": 780
    },
    {
      "epoch": 0.15796840631873627,
      "grad_norm": 11.786295890808105,
      "learning_rate": 4.736719322802107e-05,
      "loss": 1.2785,
      "step": 790
    },
    {
      "epoch": 0.15996800639872025,
      "grad_norm": 13.041200637817383,
      "learning_rate": 4.733386656002133e-05,
      "loss": 1.3944,
      "step": 800
    },
    {
      "epoch": 0.16196760647870426,
      "grad_norm": 9.187958717346191,
      "learning_rate": 4.7300539892021604e-05,
      "loss": 1.5101,
      "step": 810
    },
    {
      "epoch": 0.16396720655868827,
      "grad_norm": 7.86534309387207,
      "learning_rate": 4.726721322402187e-05,
      "loss": 1.4288,
      "step": 820
    },
    {
      "epoch": 0.16596680663867228,
      "grad_norm": 17.428510665893555,
      "learning_rate": 4.723388655602213e-05,
      "loss": 1.3366,
      "step": 830
    },
    {
      "epoch": 0.16796640671865626,
      "grad_norm": 10.72262954711914,
      "learning_rate": 4.72005598880224e-05,
      "loss": 1.4652,
      "step": 840
    },
    {
      "epoch": 0.16996600679864027,
      "grad_norm": 9.7687349319458,
      "learning_rate": 4.7167233220022664e-05,
      "loss": 1.4758,
      "step": 850
    },
    {
      "epoch": 0.17196560687862428,
      "grad_norm": 9.781272888183594,
      "learning_rate": 4.7133906552022935e-05,
      "loss": 1.3669,
      "step": 860
    },
    {
      "epoch": 0.1739652069586083,
      "grad_norm": 13.76578426361084,
      "learning_rate": 4.71005798840232e-05,
      "loss": 1.5577,
      "step": 870
    },
    {
      "epoch": 0.17596480703859227,
      "grad_norm": 6.283199310302734,
      "learning_rate": 4.706725321602347e-05,
      "loss": 1.5136,
      "step": 880
    },
    {
      "epoch": 0.17796440711857628,
      "grad_norm": 7.023477077484131,
      "learning_rate": 4.703392654802373e-05,
      "loss": 1.3839,
      "step": 890
    },
    {
      "epoch": 0.1799640071985603,
      "grad_norm": 10.161480903625488,
      "learning_rate": 4.7000599880024e-05,
      "loss": 1.6868,
      "step": 900
    },
    {
      "epoch": 0.1819636072785443,
      "grad_norm": 12.608484268188477,
      "learning_rate": 4.6967273212024266e-05,
      "loss": 1.6066,
      "step": 910
    },
    {
      "epoch": 0.1839632073585283,
      "grad_norm": 9.216276168823242,
      "learning_rate": 4.693394654402453e-05,
      "loss": 1.7574,
      "step": 920
    },
    {
      "epoch": 0.1859628074385123,
      "grad_norm": 13.055495262145996,
      "learning_rate": 4.69006198760248e-05,
      "loss": 1.584,
      "step": 930
    },
    {
      "epoch": 0.1879624075184963,
      "grad_norm": 14.09416389465332,
      "learning_rate": 4.686729320802506e-05,
      "loss": 1.3843,
      "step": 940
    },
    {
      "epoch": 0.1899620075984803,
      "grad_norm": 7.26009464263916,
      "learning_rate": 4.683396654002533e-05,
      "loss": 1.2144,
      "step": 950
    },
    {
      "epoch": 0.19196160767846432,
      "grad_norm": 7.7341437339782715,
      "learning_rate": 4.68006398720256e-05,
      "loss": 1.3398,
      "step": 960
    },
    {
      "epoch": 0.1939612077584483,
      "grad_norm": 12.265687942504883,
      "learning_rate": 4.676731320402587e-05,
      "loss": 1.6898,
      "step": 970
    },
    {
      "epoch": 0.1959608078384323,
      "grad_norm": 11.596131324768066,
      "learning_rate": 4.673398653602613e-05,
      "loss": 1.5245,
      "step": 980
    },
    {
      "epoch": 0.19796040791841632,
      "grad_norm": 12.545600891113281,
      "learning_rate": 4.6700659868026394e-05,
      "loss": 1.4145,
      "step": 990
    },
    {
      "epoch": 0.19996000799840033,
      "grad_norm": 12.142346382141113,
      "learning_rate": 4.6667333200026664e-05,
      "loss": 1.4091,
      "step": 1000
    },
    {
      "epoch": 0.2019596080783843,
      "grad_norm": 12.169853210449219,
      "learning_rate": 4.663400653202693e-05,
      "loss": 1.5774,
      "step": 1010
    },
    {
      "epoch": 0.20395920815836832,
      "grad_norm": 11.01085376739502,
      "learning_rate": 4.66006798640272e-05,
      "loss": 1.5963,
      "step": 1020
    },
    {
      "epoch": 0.20595880823835233,
      "grad_norm": 9.94249439239502,
      "learning_rate": 4.656735319602746e-05,
      "loss": 1.7998,
      "step": 1030
    },
    {
      "epoch": 0.20795840831833634,
      "grad_norm": 13.21448040008545,
      "learning_rate": 4.653402652802773e-05,
      "loss": 1.5053,
      "step": 1040
    },
    {
      "epoch": 0.20995800839832032,
      "grad_norm": 12.69991683959961,
      "learning_rate": 4.6500699860027995e-05,
      "loss": 1.3867,
      "step": 1050
    },
    {
      "epoch": 0.21195760847830433,
      "grad_norm": 9.030729293823242,
      "learning_rate": 4.6467373192028266e-05,
      "loss": 1.3164,
      "step": 1060
    },
    {
      "epoch": 0.21395720855828834,
      "grad_norm": 9.211381912231445,
      "learning_rate": 4.643404652402853e-05,
      "loss": 1.4043,
      "step": 1070
    },
    {
      "epoch": 0.21595680863827235,
      "grad_norm": 10.702520370483398,
      "learning_rate": 4.640071985602879e-05,
      "loss": 1.3004,
      "step": 1080
    },
    {
      "epoch": 0.21795640871825636,
      "grad_norm": 11.85316276550293,
      "learning_rate": 4.636739318802906e-05,
      "loss": 1.1928,
      "step": 1090
    },
    {
      "epoch": 0.21995600879824034,
      "grad_norm": 9.451969146728516,
      "learning_rate": 4.6334066520029326e-05,
      "loss": 1.6014,
      "step": 1100
    },
    {
      "epoch": 0.22195560887822435,
      "grad_norm": 12.471943855285645,
      "learning_rate": 4.63007398520296e-05,
      "loss": 1.7064,
      "step": 1110
    },
    {
      "epoch": 0.22395520895820836,
      "grad_norm": 8.981622695922852,
      "learning_rate": 4.626741318402986e-05,
      "loss": 1.459,
      "step": 1120
    },
    {
      "epoch": 0.22595480903819237,
      "grad_norm": 8.550056457519531,
      "learning_rate": 4.623408651603013e-05,
      "loss": 1.6949,
      "step": 1130
    },
    {
      "epoch": 0.22795440911817635,
      "grad_norm": 9.892953872680664,
      "learning_rate": 4.6200759848030394e-05,
      "loss": 1.7617,
      "step": 1140
    },
    {
      "epoch": 0.22995400919816036,
      "grad_norm": 8.144174575805664,
      "learning_rate": 4.6167433180030664e-05,
      "loss": 1.4758,
      "step": 1150
    },
    {
      "epoch": 0.23195360927814437,
      "grad_norm": 8.207463264465332,
      "learning_rate": 4.613410651203093e-05,
      "loss": 1.4159,
      "step": 1160
    },
    {
      "epoch": 0.23395320935812838,
      "grad_norm": 10.39710521697998,
      "learning_rate": 4.610077984403119e-05,
      "loss": 1.2303,
      "step": 1170
    },
    {
      "epoch": 0.23595280943811237,
      "grad_norm": 12.242365837097168,
      "learning_rate": 4.606745317603146e-05,
      "loss": 1.6174,
      "step": 1180
    },
    {
      "epoch": 0.23795240951809637,
      "grad_norm": 6.95166540145874,
      "learning_rate": 4.6034126508031725e-05,
      "loss": 1.2082,
      "step": 1190
    },
    {
      "epoch": 0.23995200959808038,
      "grad_norm": 11.375865936279297,
      "learning_rate": 4.6000799840031995e-05,
      "loss": 1.4974,
      "step": 1200
    },
    {
      "epoch": 0.2419516096780644,
      "grad_norm": 11.419027328491211,
      "learning_rate": 4.596747317203226e-05,
      "loss": 1.572,
      "step": 1210
    },
    {
      "epoch": 0.2439512097580484,
      "grad_norm": 10.515029907226562,
      "learning_rate": 4.593414650403253e-05,
      "loss": 1.4316,
      "step": 1220
    },
    {
      "epoch": 0.24595080983803239,
      "grad_norm": 8.428004264831543,
      "learning_rate": 4.590081983603279e-05,
      "loss": 1.3163,
      "step": 1230
    },
    {
      "epoch": 0.2479504099180164,
      "grad_norm": 10.471663475036621,
      "learning_rate": 4.586749316803306e-05,
      "loss": 1.3908,
      "step": 1240
    },
    {
      "epoch": 0.2499500099980004,
      "grad_norm": 10.277301788330078,
      "learning_rate": 4.5834166500033326e-05,
      "loss": 1.5551,
      "step": 1250
    },
    {
      "epoch": 0.2519496100779844,
      "grad_norm": 6.981058597564697,
      "learning_rate": 4.580083983203359e-05,
      "loss": 1.4664,
      "step": 1260
    },
    {
      "epoch": 0.2539492101579684,
      "grad_norm": 12.309026718139648,
      "learning_rate": 4.576751316403386e-05,
      "loss": 1.3181,
      "step": 1270
    },
    {
      "epoch": 0.2559488102379524,
      "grad_norm": 12.076576232910156,
      "learning_rate": 4.5734186496034124e-05,
      "loss": 1.5454,
      "step": 1280
    },
    {
      "epoch": 0.2579484103179364,
      "grad_norm": 10.341789245605469,
      "learning_rate": 4.57008598280344e-05,
      "loss": 1.4374,
      "step": 1290
    },
    {
      "epoch": 0.2599480103979204,
      "grad_norm": 8.169737815856934,
      "learning_rate": 4.5667533160034664e-05,
      "loss": 1.3568,
      "step": 1300
    },
    {
      "epoch": 0.26194761047790444,
      "grad_norm": 7.443861961364746,
      "learning_rate": 4.563420649203493e-05,
      "loss": 1.3275,
      "step": 1310
    },
    {
      "epoch": 0.26394721055788845,
      "grad_norm": 10.078680038452148,
      "learning_rate": 4.56008798240352e-05,
      "loss": 1.4387,
      "step": 1320
    },
    {
      "epoch": 0.2659468106378724,
      "grad_norm": 9.901040077209473,
      "learning_rate": 4.556755315603546e-05,
      "loss": 1.0959,
      "step": 1330
    },
    {
      "epoch": 0.2679464107178564,
      "grad_norm": 9.337478637695312,
      "learning_rate": 4.553422648803573e-05,
      "loss": 1.5797,
      "step": 1340
    },
    {
      "epoch": 0.2699460107978404,
      "grad_norm": 16.274538040161133,
      "learning_rate": 4.5500899820035995e-05,
      "loss": 1.1363,
      "step": 1350
    },
    {
      "epoch": 0.27194561087782443,
      "grad_norm": 8.576570510864258,
      "learning_rate": 4.5467573152036266e-05,
      "loss": 1.2673,
      "step": 1360
    },
    {
      "epoch": 0.27394521095780844,
      "grad_norm": 14.18104362487793,
      "learning_rate": 4.543424648403653e-05,
      "loss": 1.3367,
      "step": 1370
    },
    {
      "epoch": 0.27594481103779245,
      "grad_norm": 16.912872314453125,
      "learning_rate": 4.54009198160368e-05,
      "loss": 1.75,
      "step": 1380
    },
    {
      "epoch": 0.27794441111777646,
      "grad_norm": 9.762338638305664,
      "learning_rate": 4.536759314803706e-05,
      "loss": 1.2588,
      "step": 1390
    },
    {
      "epoch": 0.27994401119776047,
      "grad_norm": 8.185384750366211,
      "learning_rate": 4.5334266480037326e-05,
      "loss": 1.4889,
      "step": 1400
    },
    {
      "epoch": 0.2819436112777445,
      "grad_norm": 12.606782913208008,
      "learning_rate": 4.53009398120376e-05,
      "loss": 1.3724,
      "step": 1410
    },
    {
      "epoch": 0.28394321135772843,
      "grad_norm": 7.541880130767822,
      "learning_rate": 4.526761314403786e-05,
      "loss": 1.3016,
      "step": 1420
    },
    {
      "epoch": 0.28594281143771244,
      "grad_norm": 7.293546676635742,
      "learning_rate": 4.523428647603813e-05,
      "loss": 1.2568,
      "step": 1430
    },
    {
      "epoch": 0.28794241151769645,
      "grad_norm": 10.203044891357422,
      "learning_rate": 4.5200959808038394e-05,
      "loss": 1.2787,
      "step": 1440
    },
    {
      "epoch": 0.28994201159768046,
      "grad_norm": 11.306634902954102,
      "learning_rate": 4.5167633140038664e-05,
      "loss": 1.3075,
      "step": 1450
    },
    {
      "epoch": 0.29194161167766447,
      "grad_norm": 11.130594253540039,
      "learning_rate": 4.513430647203893e-05,
      "loss": 1.4412,
      "step": 1460
    },
    {
      "epoch": 0.2939412117576485,
      "grad_norm": 12.61267375946045,
      "learning_rate": 4.51009798040392e-05,
      "loss": 1.4638,
      "step": 1470
    },
    {
      "epoch": 0.2959408118376325,
      "grad_norm": 11.044858932495117,
      "learning_rate": 4.506765313603946e-05,
      "loss": 1.2142,
      "step": 1480
    },
    {
      "epoch": 0.2979404119176165,
      "grad_norm": 10.749207496643066,
      "learning_rate": 4.5034326468039725e-05,
      "loss": 1.2413,
      "step": 1490
    },
    {
      "epoch": 0.29994001199760045,
      "grad_norm": 13.847024917602539,
      "learning_rate": 4.5000999800039995e-05,
      "loss": 1.3353,
      "step": 1500
    },
    {
      "epoch": 0.30193961207758446,
      "grad_norm": 7.468137741088867,
      "learning_rate": 4.496767313204026e-05,
      "loss": 1.2833,
      "step": 1510
    },
    {
      "epoch": 0.3039392121575685,
      "grad_norm": 7.363351821899414,
      "learning_rate": 4.493434646404053e-05,
      "loss": 1.2521,
      "step": 1520
    },
    {
      "epoch": 0.3059388122375525,
      "grad_norm": 11.773109436035156,
      "learning_rate": 4.490101979604079e-05,
      "loss": 1.5557,
      "step": 1530
    },
    {
      "epoch": 0.3079384123175365,
      "grad_norm": 6.45768404006958,
      "learning_rate": 4.486769312804106e-05,
      "loss": 1.1955,
      "step": 1540
    },
    {
      "epoch": 0.3099380123975205,
      "grad_norm": 10.601333618164062,
      "learning_rate": 4.4834366460041326e-05,
      "loss": 1.4657,
      "step": 1550
    },
    {
      "epoch": 0.3119376124775045,
      "grad_norm": 12.380545616149902,
      "learning_rate": 4.4801039792041597e-05,
      "loss": 1.3831,
      "step": 1560
    },
    {
      "epoch": 0.3139372125574885,
      "grad_norm": 9.64351749420166,
      "learning_rate": 4.476771312404186e-05,
      "loss": 1.4464,
      "step": 1570
    },
    {
      "epoch": 0.31593681263747253,
      "grad_norm": 10.940901756286621,
      "learning_rate": 4.4734386456042124e-05,
      "loss": 1.404,
      "step": 1580
    },
    {
      "epoch": 0.3179364127174565,
      "grad_norm": 14.800042152404785,
      "learning_rate": 4.4701059788042394e-05,
      "loss": 1.2978,
      "step": 1590
    },
    {
      "epoch": 0.3199360127974405,
      "grad_norm": 8.449132919311523,
      "learning_rate": 4.466773312004266e-05,
      "loss": 1.526,
      "step": 1600
    },
    {
      "epoch": 0.3219356128774245,
      "grad_norm": 12.05859661102295,
      "learning_rate": 4.463440645204293e-05,
      "loss": 1.314,
      "step": 1610
    },
    {
      "epoch": 0.3239352129574085,
      "grad_norm": 13.708454132080078,
      "learning_rate": 4.460107978404319e-05,
      "loss": 1.4002,
      "step": 1620
    },
    {
      "epoch": 0.3259348130373925,
      "grad_norm": 13.73060131072998,
      "learning_rate": 4.456775311604346e-05,
      "loss": 1.3451,
      "step": 1630
    },
    {
      "epoch": 0.32793441311737653,
      "grad_norm": 12.938565254211426,
      "learning_rate": 4.4534426448043725e-05,
      "loss": 1.55,
      "step": 1640
    },
    {
      "epoch": 0.32993401319736054,
      "grad_norm": 12.568390846252441,
      "learning_rate": 4.4501099780043995e-05,
      "loss": 1.447,
      "step": 1650
    },
    {
      "epoch": 0.33193361327734455,
      "grad_norm": 15.179072380065918,
      "learning_rate": 4.446777311204426e-05,
      "loss": 1.4206,
      "step": 1660
    },
    {
      "epoch": 0.3339332133573285,
      "grad_norm": 10.558350563049316,
      "learning_rate": 4.443444644404452e-05,
      "loss": 1.328,
      "step": 1670
    },
    {
      "epoch": 0.3359328134373125,
      "grad_norm": 7.207569122314453,
      "learning_rate": 4.440111977604479e-05,
      "loss": 1.4097,
      "step": 1680
    },
    {
      "epoch": 0.3379324135172965,
      "grad_norm": 13.435464859008789,
      "learning_rate": 4.4367793108045056e-05,
      "loss": 1.3629,
      "step": 1690
    },
    {
      "epoch": 0.33993201359728054,
      "grad_norm": 10.228316307067871,
      "learning_rate": 4.4334466440045326e-05,
      "loss": 1.3451,
      "step": 1700
    },
    {
      "epoch": 0.34193161367726455,
      "grad_norm": 10.333452224731445,
      "learning_rate": 4.430113977204559e-05,
      "loss": 1.4554,
      "step": 1710
    },
    {
      "epoch": 0.34393121375724856,
      "grad_norm": 7.805350303649902,
      "learning_rate": 4.426781310404586e-05,
      "loss": 1.4457,
      "step": 1720
    },
    {
      "epoch": 0.34593081383723256,
      "grad_norm": 14.084467887878418,
      "learning_rate": 4.4234486436046124e-05,
      "loss": 1.326,
      "step": 1730
    },
    {
      "epoch": 0.3479304139172166,
      "grad_norm": 6.707535266876221,
      "learning_rate": 4.4201159768046394e-05,
      "loss": 1.5048,
      "step": 1740
    },
    {
      "epoch": 0.3499300139972006,
      "grad_norm": 7.939605236053467,
      "learning_rate": 4.416783310004666e-05,
      "loss": 1.1874,
      "step": 1750
    },
    {
      "epoch": 0.35192961407718454,
      "grad_norm": 10.233711242675781,
      "learning_rate": 4.413450643204692e-05,
      "loss": 1.2484,
      "step": 1760
    },
    {
      "epoch": 0.35392921415716855,
      "grad_norm": 15.285201072692871,
      "learning_rate": 4.410117976404719e-05,
      "loss": 1.2695,
      "step": 1770
    },
    {
      "epoch": 0.35592881423715256,
      "grad_norm": 12.951933860778809,
      "learning_rate": 4.4067853096047455e-05,
      "loss": 1.4355,
      "step": 1780
    },
    {
      "epoch": 0.35792841431713657,
      "grad_norm": 15.830033302307129,
      "learning_rate": 4.4034526428047725e-05,
      "loss": 1.4417,
      "step": 1790
    },
    {
      "epoch": 0.3599280143971206,
      "grad_norm": 7.617671489715576,
      "learning_rate": 4.4001199760047995e-05,
      "loss": 1.3658,
      "step": 1800
    },
    {
      "epoch": 0.3619276144771046,
      "grad_norm": 14.878056526184082,
      "learning_rate": 4.396787309204826e-05,
      "loss": 1.2227,
      "step": 1810
    },
    {
      "epoch": 0.3639272145570886,
      "grad_norm": 13.695913314819336,
      "learning_rate": 4.393454642404853e-05,
      "loss": 1.4606,
      "step": 1820
    },
    {
      "epoch": 0.3659268146370726,
      "grad_norm": 12.099872589111328,
      "learning_rate": 4.390121975604879e-05,
      "loss": 1.2938,
      "step": 1830
    },
    {
      "epoch": 0.3679264147170566,
      "grad_norm": 10.036174774169922,
      "learning_rate": 4.386789308804906e-05,
      "loss": 1.3093,
      "step": 1840
    },
    {
      "epoch": 0.36992601479704057,
      "grad_norm": 6.709839820861816,
      "learning_rate": 4.3834566420049326e-05,
      "loss": 1.4853,
      "step": 1850
    },
    {
      "epoch": 0.3719256148770246,
      "grad_norm": 4.925860404968262,
      "learning_rate": 4.3801239752049597e-05,
      "loss": 1.4173,
      "step": 1860
    },
    {
      "epoch": 0.3739252149570086,
      "grad_norm": 11.966093063354492,
      "learning_rate": 4.376791308404986e-05,
      "loss": 1.2895,
      "step": 1870
    },
    {
      "epoch": 0.3759248150369926,
      "grad_norm": 10.420259475708008,
      "learning_rate": 4.373458641605013e-05,
      "loss": 1.3862,
      "step": 1880
    },
    {
      "epoch": 0.3779244151169766,
      "grad_norm": 15.055235862731934,
      "learning_rate": 4.3701259748050394e-05,
      "loss": 1.4851,
      "step": 1890
    },
    {
      "epoch": 0.3799240151969606,
      "grad_norm": 16.069576263427734,
      "learning_rate": 4.366793308005066e-05,
      "loss": 1.4612,
      "step": 1900
    },
    {
      "epoch": 0.38192361527694463,
      "grad_norm": 9.623614311218262,
      "learning_rate": 4.363460641205093e-05,
      "loss": 1.3047,
      "step": 1910
    },
    {
      "epoch": 0.38392321535692864,
      "grad_norm": 6.300931930541992,
      "learning_rate": 4.360127974405119e-05,
      "loss": 1.5649,
      "step": 1920
    },
    {
      "epoch": 0.3859228154369126,
      "grad_norm": 12.016615867614746,
      "learning_rate": 4.356795307605146e-05,
      "loss": 1.3232,
      "step": 1930
    },
    {
      "epoch": 0.3879224155168966,
      "grad_norm": 11.33920669555664,
      "learning_rate": 4.3534626408051725e-05,
      "loss": 1.4063,
      "step": 1940
    },
    {
      "epoch": 0.3899220155968806,
      "grad_norm": 8.669095039367676,
      "learning_rate": 4.3501299740051995e-05,
      "loss": 1.4773,
      "step": 1950
    },
    {
      "epoch": 0.3919216156768646,
      "grad_norm": 10.304688453674316,
      "learning_rate": 4.346797307205226e-05,
      "loss": 1.196,
      "step": 1960
    },
    {
      "epoch": 0.39392121575684863,
      "grad_norm": 8.115824699401855,
      "learning_rate": 4.343464640405253e-05,
      "loss": 1.1125,
      "step": 1970
    },
    {
      "epoch": 0.39592081583683264,
      "grad_norm": 7.063497066497803,
      "learning_rate": 4.340131973605279e-05,
      "loss": 1.3435,
      "step": 1980
    },
    {
      "epoch": 0.39792041591681665,
      "grad_norm": 8.129348754882812,
      "learning_rate": 4.3367993068053056e-05,
      "loss": 1.1309,
      "step": 1990
    },
    {
      "epoch": 0.39992001599680066,
      "grad_norm": 8.476826667785645,
      "learning_rate": 4.3334666400053326e-05,
      "loss": 1.4277,
      "step": 2000
    },
    {
      "epoch": 0.40191961607678467,
      "grad_norm": 14.241874694824219,
      "learning_rate": 4.330133973205359e-05,
      "loss": 1.2404,
      "step": 2010
    },
    {
      "epoch": 0.4039192161567686,
      "grad_norm": 7.7841315269470215,
      "learning_rate": 4.326801306405386e-05,
      "loss": 1.0315,
      "step": 2020
    },
    {
      "epoch": 0.40591881623675263,
      "grad_norm": 9.687084197998047,
      "learning_rate": 4.3234686396054124e-05,
      "loss": 1.3285,
      "step": 2030
    },
    {
      "epoch": 0.40791841631673664,
      "grad_norm": 11.329412460327148,
      "learning_rate": 4.3201359728054394e-05,
      "loss": 1.3635,
      "step": 2040
    },
    {
      "epoch": 0.40991801639672065,
      "grad_norm": 9.064562797546387,
      "learning_rate": 4.316803306005466e-05,
      "loss": 1.541,
      "step": 2050
    },
    {
      "epoch": 0.41191761647670466,
      "grad_norm": 7.466919422149658,
      "learning_rate": 4.313470639205493e-05,
      "loss": 1.2689,
      "step": 2060
    },
    {
      "epoch": 0.41391721655668867,
      "grad_norm": 8.256321907043457,
      "learning_rate": 4.310137972405519e-05,
      "loss": 1.6079,
      "step": 2070
    },
    {
      "epoch": 0.4159168166366727,
      "grad_norm": 8.191024780273438,
      "learning_rate": 4.3068053056055455e-05,
      "loss": 1.2105,
      "step": 2080
    },
    {
      "epoch": 0.4179164167166567,
      "grad_norm": 16.89287567138672,
      "learning_rate": 4.3034726388055725e-05,
      "loss": 1.3279,
      "step": 2090
    },
    {
      "epoch": 0.41991601679664065,
      "grad_norm": 6.810368537902832,
      "learning_rate": 4.300139972005599e-05,
      "loss": 1.3173,
      "step": 2100
    },
    {
      "epoch": 0.42191561687662466,
      "grad_norm": 6.354698657989502,
      "learning_rate": 4.296807305205626e-05,
      "loss": 1.3385,
      "step": 2110
    },
    {
      "epoch": 0.42391521695660866,
      "grad_norm": 12.645870208740234,
      "learning_rate": 4.293474638405652e-05,
      "loss": 1.4435,
      "step": 2120
    },
    {
      "epoch": 0.4259148170365927,
      "grad_norm": 9.203939437866211,
      "learning_rate": 4.290141971605679e-05,
      "loss": 1.4086,
      "step": 2130
    },
    {
      "epoch": 0.4279144171165767,
      "grad_norm": 12.003053665161133,
      "learning_rate": 4.2868093048057056e-05,
      "loss": 1.3651,
      "step": 2140
    },
    {
      "epoch": 0.4299140171965607,
      "grad_norm": 7.764573574066162,
      "learning_rate": 4.2834766380057326e-05,
      "loss": 1.4471,
      "step": 2150
    },
    {
      "epoch": 0.4319136172765447,
      "grad_norm": 9.585809707641602,
      "learning_rate": 4.280143971205759e-05,
      "loss": 1.3044,
      "step": 2160
    },
    {
      "epoch": 0.4339132173565287,
      "grad_norm": 19.769412994384766,
      "learning_rate": 4.276811304405785e-05,
      "loss": 1.2034,
      "step": 2170
    },
    {
      "epoch": 0.4359128174365127,
      "grad_norm": 10.261392593383789,
      "learning_rate": 4.2734786376058123e-05,
      "loss": 1.3423,
      "step": 2180
    },
    {
      "epoch": 0.4379124175164967,
      "grad_norm": 6.7034478187561035,
      "learning_rate": 4.270145970805839e-05,
      "loss": 1.1584,
      "step": 2190
    },
    {
      "epoch": 0.4399120175964807,
      "grad_norm": 13.299567222595215,
      "learning_rate": 4.266813304005866e-05,
      "loss": 1.5279,
      "step": 2200
    },
    {
      "epoch": 0.4419116176764647,
      "grad_norm": 8.817646980285645,
      "learning_rate": 4.263480637205892e-05,
      "loss": 1.4097,
      "step": 2210
    },
    {
      "epoch": 0.4439112177564487,
      "grad_norm": 10.895479202270508,
      "learning_rate": 4.260147970405919e-05,
      "loss": 1.1805,
      "step": 2220
    },
    {
      "epoch": 0.4459108178364327,
      "grad_norm": 7.209839344024658,
      "learning_rate": 4.2568153036059455e-05,
      "loss": 1.0239,
      "step": 2230
    },
    {
      "epoch": 0.4479104179164167,
      "grad_norm": 10.350363731384277,
      "learning_rate": 4.2534826368059725e-05,
      "loss": 1.4044,
      "step": 2240
    },
    {
      "epoch": 0.44991001799640074,
      "grad_norm": 9.756516456604004,
      "learning_rate": 4.250149970005999e-05,
      "loss": 1.1526,
      "step": 2250
    },
    {
      "epoch": 0.45190961807638474,
      "grad_norm": 11.944194793701172,
      "learning_rate": 4.246817303206025e-05,
      "loss": 1.6343,
      "step": 2260
    },
    {
      "epoch": 0.4539092181563687,
      "grad_norm": 11.264093399047852,
      "learning_rate": 4.243484636406052e-05,
      "loss": 1.2512,
      "step": 2270
    },
    {
      "epoch": 0.4559088182363527,
      "grad_norm": 9.32564926147461,
      "learning_rate": 4.2401519696060786e-05,
      "loss": 1.2642,
      "step": 2280
    },
    {
      "epoch": 0.4579084183163367,
      "grad_norm": 10.246231079101562,
      "learning_rate": 4.2368193028061056e-05,
      "loss": 1.2339,
      "step": 2290
    },
    {
      "epoch": 0.45990801839632073,
      "grad_norm": 7.710769176483154,
      "learning_rate": 4.233486636006132e-05,
      "loss": 1.1466,
      "step": 2300
    },
    {
      "epoch": 0.46190761847630474,
      "grad_norm": 10.273658752441406,
      "learning_rate": 4.230153969206159e-05,
      "loss": 1.3991,
      "step": 2310
    },
    {
      "epoch": 0.46390721855628875,
      "grad_norm": 9.619648933410645,
      "learning_rate": 4.226821302406186e-05,
      "loss": 1.2619,
      "step": 2320
    },
    {
      "epoch": 0.46590681863627276,
      "grad_norm": 5.6224493980407715,
      "learning_rate": 4.2234886356062123e-05,
      "loss": 1.1529,
      "step": 2330
    },
    {
      "epoch": 0.46790641871625677,
      "grad_norm": 6.693117618560791,
      "learning_rate": 4.2201559688062394e-05,
      "loss": 1.0262,
      "step": 2340
    },
    {
      "epoch": 0.4699060187962408,
      "grad_norm": 12.3672513961792,
      "learning_rate": 4.216823302006266e-05,
      "loss": 1.2952,
      "step": 2350
    },
    {
      "epoch": 0.47190561887622473,
      "grad_norm": 14.722169876098633,
      "learning_rate": 4.213490635206293e-05,
      "loss": 1.1985,
      "step": 2360
    },
    {
      "epoch": 0.47390521895620874,
      "grad_norm": 8.191874504089355,
      "learning_rate": 4.210157968406319e-05,
      "loss": 1.5322,
      "step": 2370
    },
    {
      "epoch": 0.47590481903619275,
      "grad_norm": 6.439329624176025,
      "learning_rate": 4.206825301606346e-05,
      "loss": 1.3296,
      "step": 2380
    },
    {
      "epoch": 0.47790441911617676,
      "grad_norm": 5.884023189544678,
      "learning_rate": 4.2034926348063725e-05,
      "loss": 1.2206,
      "step": 2390
    },
    {
      "epoch": 0.47990401919616077,
      "grad_norm": 11.148348808288574,
      "learning_rate": 4.200159968006399e-05,
      "loss": 1.4249,
      "step": 2400
    },
    {
      "epoch": 0.4819036192761448,
      "grad_norm": 10.005032539367676,
      "learning_rate": 4.196827301206426e-05,
      "loss": 1.0862,
      "step": 2410
    },
    {
      "epoch": 0.4839032193561288,
      "grad_norm": 6.979532241821289,
      "learning_rate": 4.193494634406452e-05,
      "loss": 1.2102,
      "step": 2420
    },
    {
      "epoch": 0.4859028194361128,
      "grad_norm": 10.417128562927246,
      "learning_rate": 4.190161967606479e-05,
      "loss": 1.3869,
      "step": 2430
    },
    {
      "epoch": 0.4879024195160968,
      "grad_norm": 9.855016708374023,
      "learning_rate": 4.1868293008065056e-05,
      "loss": 1.4188,
      "step": 2440
    },
    {
      "epoch": 0.48990201959608076,
      "grad_norm": 8.409106254577637,
      "learning_rate": 4.1834966340065326e-05,
      "loss": 1.3839,
      "step": 2450
    },
    {
      "epoch": 0.49190161967606477,
      "grad_norm": 9.7792329788208,
      "learning_rate": 4.180163967206559e-05,
      "loss": 1.3037,
      "step": 2460
    },
    {
      "epoch": 0.4939012197560488,
      "grad_norm": 10.044366836547852,
      "learning_rate": 4.176831300406586e-05,
      "loss": 1.2748,
      "step": 2470
    },
    {
      "epoch": 0.4959008198360328,
      "grad_norm": 9.707967758178711,
      "learning_rate": 4.1734986336066123e-05,
      "loss": 1.2917,
      "step": 2480
    },
    {
      "epoch": 0.4979004199160168,
      "grad_norm": 10.713278770446777,
      "learning_rate": 4.170165966806639e-05,
      "loss": 1.2926,
      "step": 2490
    },
    {
      "epoch": 0.4999000199960008,
      "grad_norm": 8.194883346557617,
      "learning_rate": 4.166833300006666e-05,
      "loss": 1.3953,
      "step": 2500
    },
    {
      "epoch": 0.5018996200759848,
      "grad_norm": 12.108803749084473,
      "learning_rate": 4.163500633206692e-05,
      "loss": 1.2468,
      "step": 2510
    },
    {
      "epoch": 0.5038992201559688,
      "grad_norm": 8.096705436706543,
      "learning_rate": 4.160167966406719e-05,
      "loss": 1.5883,
      "step": 2520
    },
    {
      "epoch": 0.5058988202359528,
      "grad_norm": 6.243859767913818,
      "learning_rate": 4.1568352996067454e-05,
      "loss": 1.5474,
      "step": 2530
    },
    {
      "epoch": 0.5078984203159368,
      "grad_norm": 7.659424304962158,
      "learning_rate": 4.1535026328067725e-05,
      "loss": 1.5919,
      "step": 2540
    },
    {
      "epoch": 0.5098980203959208,
      "grad_norm": 8.2278470993042,
      "learning_rate": 4.150169966006799e-05,
      "loss": 1.1875,
      "step": 2550
    },
    {
      "epoch": 0.5118976204759048,
      "grad_norm": 10.82964038848877,
      "learning_rate": 4.146837299206825e-05,
      "loss": 1.5497,
      "step": 2560
    },
    {
      "epoch": 0.5138972205558888,
      "grad_norm": 8.458332061767578,
      "learning_rate": 4.143504632406852e-05,
      "loss": 1.2138,
      "step": 2570
    },
    {
      "epoch": 0.5158968206358728,
      "grad_norm": 10.303706169128418,
      "learning_rate": 4.1401719656068786e-05,
      "loss": 1.167,
      "step": 2580
    },
    {
      "epoch": 0.5178964207158568,
      "grad_norm": 8.409741401672363,
      "learning_rate": 4.1368392988069056e-05,
      "loss": 1.3977,
      "step": 2590
    },
    {
      "epoch": 0.5198960207958409,
      "grad_norm": 9.65929126739502,
      "learning_rate": 4.133506632006932e-05,
      "loss": 1.4502,
      "step": 2600
    },
    {
      "epoch": 0.5218956208758249,
      "grad_norm": 9.120354652404785,
      "learning_rate": 4.130173965206959e-05,
      "loss": 1.2112,
      "step": 2610
    },
    {
      "epoch": 0.5238952209558089,
      "grad_norm": 11.970780372619629,
      "learning_rate": 4.126841298406985e-05,
      "loss": 1.2607,
      "step": 2620
    },
    {
      "epoch": 0.5258948210357929,
      "grad_norm": 11.290714263916016,
      "learning_rate": 4.123508631607012e-05,
      "loss": 1.3214,
      "step": 2630
    },
    {
      "epoch": 0.5278944211157769,
      "grad_norm": 8.56667709350586,
      "learning_rate": 4.120175964807039e-05,
      "loss": 1.2839,
      "step": 2640
    },
    {
      "epoch": 0.5298940211957609,
      "grad_norm": 14.916085243225098,
      "learning_rate": 4.116843298007065e-05,
      "loss": 1.644,
      "step": 2650
    },
    {
      "epoch": 0.5318936212757448,
      "grad_norm": 19.220802307128906,
      "learning_rate": 4.113510631207092e-05,
      "loss": 1.3359,
      "step": 2660
    },
    {
      "epoch": 0.5338932213557288,
      "grad_norm": 10.924009323120117,
      "learning_rate": 4.1101779644071184e-05,
      "loss": 1.222,
      "step": 2670
    },
    {
      "epoch": 0.5358928214357128,
      "grad_norm": 7.355231761932373,
      "learning_rate": 4.1068452976071454e-05,
      "loss": 1.3494,
      "step": 2680
    },
    {
      "epoch": 0.5378924215156968,
      "grad_norm": 11.765384674072266,
      "learning_rate": 4.103512630807172e-05,
      "loss": 1.2495,
      "step": 2690
    },
    {
      "epoch": 0.5398920215956808,
      "grad_norm": 17.74324607849121,
      "learning_rate": 4.100179964007199e-05,
      "loss": 1.3723,
      "step": 2700
    },
    {
      "epoch": 0.5418916216756648,
      "grad_norm": 8.6763334274292,
      "learning_rate": 4.096847297207225e-05,
      "loss": 1.2759,
      "step": 2710
    },
    {
      "epoch": 0.5438912217556489,
      "grad_norm": 11.438884735107422,
      "learning_rate": 4.093514630407252e-05,
      "loss": 1.2887,
      "step": 2720
    },
    {
      "epoch": 0.5458908218356329,
      "grad_norm": 13.431958198547363,
      "learning_rate": 4.0901819636072785e-05,
      "loss": 1.1635,
      "step": 2730
    },
    {
      "epoch": 0.5478904219156169,
      "grad_norm": 13.024909973144531,
      "learning_rate": 4.086849296807305e-05,
      "loss": 1.3041,
      "step": 2740
    },
    {
      "epoch": 0.5498900219956009,
      "grad_norm": 12.283001899719238,
      "learning_rate": 4.083516630007332e-05,
      "loss": 1.3644,
      "step": 2750
    },
    {
      "epoch": 0.5518896220755849,
      "grad_norm": 7.722893238067627,
      "learning_rate": 4.080183963207358e-05,
      "loss": 1.2789,
      "step": 2760
    },
    {
      "epoch": 0.5538892221555689,
      "grad_norm": 9.883880615234375,
      "learning_rate": 4.076851296407385e-05,
      "loss": 1.1748,
      "step": 2770
    },
    {
      "epoch": 0.5558888222355529,
      "grad_norm": 6.70943021774292,
      "learning_rate": 4.0735186296074117e-05,
      "loss": 1.5044,
      "step": 2780
    },
    {
      "epoch": 0.5578884223155369,
      "grad_norm": 12.161844253540039,
      "learning_rate": 4.070185962807439e-05,
      "loss": 1.2099,
      "step": 2790
    },
    {
      "epoch": 0.5598880223955209,
      "grad_norm": 10.237136840820312,
      "learning_rate": 4.066853296007465e-05,
      "loss": 1.2317,
      "step": 2800
    },
    {
      "epoch": 0.5618876224755049,
      "grad_norm": 15.287521362304688,
      "learning_rate": 4.063520629207492e-05,
      "loss": 1.2061,
      "step": 2810
    },
    {
      "epoch": 0.563887222555489,
      "grad_norm": 14.262770652770996,
      "learning_rate": 4.0601879624075184e-05,
      "loss": 1.1119,
      "step": 2820
    },
    {
      "epoch": 0.5658868226354729,
      "grad_norm": 6.919824600219727,
      "learning_rate": 4.0568552956075454e-05,
      "loss": 1.3667,
      "step": 2830
    },
    {
      "epoch": 0.5678864227154569,
      "grad_norm": 14.183902740478516,
      "learning_rate": 4.0535226288075725e-05,
      "loss": 1.3654,
      "step": 2840
    },
    {
      "epoch": 0.5698860227954409,
      "grad_norm": 9.950738906860352,
      "learning_rate": 4.050189962007599e-05,
      "loss": 1.5347,
      "step": 2850
    },
    {
      "epoch": 0.5718856228754249,
      "grad_norm": 13.368810653686523,
      "learning_rate": 4.046857295207626e-05,
      "loss": 1.049,
      "step": 2860
    },
    {
      "epoch": 0.5738852229554089,
      "grad_norm": 8.22337532043457,
      "learning_rate": 4.043524628407652e-05,
      "loss": 1.3517,
      "step": 2870
    },
    {
      "epoch": 0.5758848230353929,
      "grad_norm": 7.839128017425537,
      "learning_rate": 4.0401919616076785e-05,
      "loss": 1.1669,
      "step": 2880
    },
    {
      "epoch": 0.5778844231153769,
      "grad_norm": 7.4061970710754395,
      "learning_rate": 4.0368592948077056e-05,
      "loss": 1.0605,
      "step": 2890
    },
    {
      "epoch": 0.5798840231953609,
      "grad_norm": 9.438356399536133,
      "learning_rate": 4.033526628007732e-05,
      "loss": 1.6649,
      "step": 2900
    },
    {
      "epoch": 0.5818836232753449,
      "grad_norm": 11.02318286895752,
      "learning_rate": 4.030193961207759e-05,
      "loss": 1.1835,
      "step": 2910
    },
    {
      "epoch": 0.5838832233553289,
      "grad_norm": 9.416038513183594,
      "learning_rate": 4.026861294407785e-05,
      "loss": 1.3389,
      "step": 2920
    },
    {
      "epoch": 0.585882823435313,
      "grad_norm": 8.444393157958984,
      "learning_rate": 4.023528627607812e-05,
      "loss": 1.3769,
      "step": 2930
    },
    {
      "epoch": 0.587882423515297,
      "grad_norm": 6.090178966522217,
      "learning_rate": 4.020195960807839e-05,
      "loss": 1.2626,
      "step": 2940
    },
    {
      "epoch": 0.589882023595281,
      "grad_norm": 14.910218238830566,
      "learning_rate": 4.016863294007866e-05,
      "loss": 1.176,
      "step": 2950
    },
    {
      "epoch": 0.591881623675265,
      "grad_norm": 16.014402389526367,
      "learning_rate": 4.013530627207892e-05,
      "loss": 1.1762,
      "step": 2960
    },
    {
      "epoch": 0.593881223755249,
      "grad_norm": 6.923309326171875,
      "learning_rate": 4.0101979604079184e-05,
      "loss": 1.0968,
      "step": 2970
    },
    {
      "epoch": 0.595880823835233,
      "grad_norm": 17.23565101623535,
      "learning_rate": 4.0068652936079454e-05,
      "loss": 1.199,
      "step": 2980
    },
    {
      "epoch": 0.597880423915217,
      "grad_norm": 12.468880653381348,
      "learning_rate": 4.003532626807972e-05,
      "loss": 1.3443,
      "step": 2990
    },
    {
      "epoch": 0.5998800239952009,
      "grad_norm": 10.071290969848633,
      "learning_rate": 4.000199960007999e-05,
      "loss": 1.1263,
      "step": 3000
    },
    {
      "epoch": 0.6018796240751849,
      "grad_norm": 10.354352951049805,
      "learning_rate": 3.996867293208025e-05,
      "loss": 1.3717,
      "step": 3010
    },
    {
      "epoch": 0.6038792241551689,
      "grad_norm": 11.601980209350586,
      "learning_rate": 3.993534626408052e-05,
      "loss": 1.1071,
      "step": 3020
    },
    {
      "epoch": 0.6058788242351529,
      "grad_norm": 18.797748565673828,
      "learning_rate": 3.9902019596080785e-05,
      "loss": 1.3429,
      "step": 3030
    },
    {
      "epoch": 0.607878424315137,
      "grad_norm": 9.523914337158203,
      "learning_rate": 3.9868692928081056e-05,
      "loss": 1.3549,
      "step": 3040
    },
    {
      "epoch": 0.609878024395121,
      "grad_norm": 20.726600646972656,
      "learning_rate": 3.983536626008132e-05,
      "loss": 1.219,
      "step": 3050
    },
    {
      "epoch": 0.611877624475105,
      "grad_norm": 13.368647575378418,
      "learning_rate": 3.980203959208158e-05,
      "loss": 1.3893,
      "step": 3060
    },
    {
      "epoch": 0.613877224555089,
      "grad_norm": 10.923848152160645,
      "learning_rate": 3.976871292408185e-05,
      "loss": 1.2772,
      "step": 3070
    },
    {
      "epoch": 0.615876824635073,
      "grad_norm": 7.875560760498047,
      "learning_rate": 3.9735386256082116e-05,
      "loss": 1.1633,
      "step": 3080
    },
    {
      "epoch": 0.617876424715057,
      "grad_norm": 11.409017562866211,
      "learning_rate": 3.970205958808239e-05,
      "loss": 1.0883,
      "step": 3090
    },
    {
      "epoch": 0.619876024795041,
      "grad_norm": 7.789727687835693,
      "learning_rate": 3.966873292008265e-05,
      "loss": 1.3719,
      "step": 3100
    },
    {
      "epoch": 0.621875624875025,
      "grad_norm": 8.09294319152832,
      "learning_rate": 3.963540625208292e-05,
      "loss": 1.1676,
      "step": 3110
    },
    {
      "epoch": 0.623875224955009,
      "grad_norm": 9.983613014221191,
      "learning_rate": 3.9602079584083184e-05,
      "loss": 1.2901,
      "step": 3120
    },
    {
      "epoch": 0.625874825034993,
      "grad_norm": 16.45145034790039,
      "learning_rate": 3.9568752916083454e-05,
      "loss": 1.4913,
      "step": 3130
    },
    {
      "epoch": 0.627874425114977,
      "grad_norm": 13.140570640563965,
      "learning_rate": 3.953542624808372e-05,
      "loss": 1.3606,
      "step": 3140
    },
    {
      "epoch": 0.629874025194961,
      "grad_norm": 9.14824104309082,
      "learning_rate": 3.950209958008398e-05,
      "loss": 0.9801,
      "step": 3150
    },
    {
      "epoch": 0.6318736252749451,
      "grad_norm": 10.571456909179688,
      "learning_rate": 3.946877291208425e-05,
      "loss": 1.3986,
      "step": 3160
    },
    {
      "epoch": 0.633873225354929,
      "grad_norm": 9.13987922668457,
      "learning_rate": 3.9435446244084515e-05,
      "loss": 1.2015,
      "step": 3170
    },
    {
      "epoch": 0.635872825434913,
      "grad_norm": 19.51978874206543,
      "learning_rate": 3.9402119576084785e-05,
      "loss": 1.3393,
      "step": 3180
    },
    {
      "epoch": 0.637872425514897,
      "grad_norm": 7.830604553222656,
      "learning_rate": 3.936879290808505e-05,
      "loss": 1.4424,
      "step": 3190
    },
    {
      "epoch": 0.639872025594881,
      "grad_norm": 13.370462417602539,
      "learning_rate": 3.933546624008532e-05,
      "loss": 1.2709,
      "step": 3200
    },
    {
      "epoch": 0.641871625674865,
      "grad_norm": 10.603973388671875,
      "learning_rate": 3.930213957208558e-05,
      "loss": 1.4042,
      "step": 3210
    },
    {
      "epoch": 0.643871225754849,
      "grad_norm": 10.764043807983398,
      "learning_rate": 3.926881290408585e-05,
      "loss": 1.1157,
      "step": 3220
    },
    {
      "epoch": 0.645870825834833,
      "grad_norm": 13.663467407226562,
      "learning_rate": 3.9235486236086116e-05,
      "loss": 1.1413,
      "step": 3230
    },
    {
      "epoch": 0.647870425914817,
      "grad_norm": 9.756425857543945,
      "learning_rate": 3.920215956808638e-05,
      "loss": 1.2439,
      "step": 3240
    },
    {
      "epoch": 0.649870025994801,
      "grad_norm": 7.393795490264893,
      "learning_rate": 3.916883290008665e-05,
      "loss": 0.9973,
      "step": 3250
    },
    {
      "epoch": 0.651869626074785,
      "grad_norm": 11.991584777832031,
      "learning_rate": 3.9135506232086914e-05,
      "loss": 1.1343,
      "step": 3260
    },
    {
      "epoch": 0.6538692261547691,
      "grad_norm": 7.072484493255615,
      "learning_rate": 3.9102179564087184e-05,
      "loss": 1.1824,
      "step": 3270
    },
    {
      "epoch": 0.6558688262347531,
      "grad_norm": 14.368646621704102,
      "learning_rate": 3.906885289608745e-05,
      "loss": 1.3315,
      "step": 3280
    },
    {
      "epoch": 0.6578684263147371,
      "grad_norm": 12.82465934753418,
      "learning_rate": 3.903552622808772e-05,
      "loss": 1.2157,
      "step": 3290
    },
    {
      "epoch": 0.6598680263947211,
      "grad_norm": 13.874706268310547,
      "learning_rate": 3.900219956008798e-05,
      "loss": 1.1154,
      "step": 3300
    },
    {
      "epoch": 0.6618676264747051,
      "grad_norm": 11.70471477508545,
      "learning_rate": 3.896887289208825e-05,
      "loss": 1.4185,
      "step": 3310
    },
    {
      "epoch": 0.6638672265546891,
      "grad_norm": 14.185334205627441,
      "learning_rate": 3.8935546224088515e-05,
      "loss": 1.3941,
      "step": 3320
    },
    {
      "epoch": 0.6658668266346731,
      "grad_norm": 11.32946491241455,
      "learning_rate": 3.890221955608878e-05,
      "loss": 1.4729,
      "step": 3330
    },
    {
      "epoch": 0.667866426714657,
      "grad_norm": 8.55795669555664,
      "learning_rate": 3.8868892888089056e-05,
      "loss": 1.2327,
      "step": 3340
    },
    {
      "epoch": 0.669866026794641,
      "grad_norm": 18.567283630371094,
      "learning_rate": 3.883556622008932e-05,
      "loss": 1.3003,
      "step": 3350
    },
    {
      "epoch": 0.671865626874625,
      "grad_norm": 14.00277328491211,
      "learning_rate": 3.880223955208959e-05,
      "loss": 1.3131,
      "step": 3360
    },
    {
      "epoch": 0.673865226954609,
      "grad_norm": 9.731492042541504,
      "learning_rate": 3.876891288408985e-05,
      "loss": 1.3762,
      "step": 3370
    },
    {
      "epoch": 0.675864827034593,
      "grad_norm": 11.062589645385742,
      "learning_rate": 3.8735586216090116e-05,
      "loss": 1.1645,
      "step": 3380
    },
    {
      "epoch": 0.6778644271145771,
      "grad_norm": 12.758499145507812,
      "learning_rate": 3.870225954809039e-05,
      "loss": 1.2923,
      "step": 3390
    },
    {
      "epoch": 0.6798640271945611,
      "grad_norm": 11.518110275268555,
      "learning_rate": 3.866893288009065e-05,
      "loss": 1.2215,
      "step": 3400
    },
    {
      "epoch": 0.6818636272745451,
      "grad_norm": 8.667983055114746,
      "learning_rate": 3.863560621209092e-05,
      "loss": 1.1419,
      "step": 3410
    },
    {
      "epoch": 0.6838632273545291,
      "grad_norm": 10.714900970458984,
      "learning_rate": 3.8602279544091184e-05,
      "loss": 1.4783,
      "step": 3420
    },
    {
      "epoch": 0.6858628274345131,
      "grad_norm": 5.994640827178955,
      "learning_rate": 3.8568952876091454e-05,
      "loss": 1.2491,
      "step": 3430
    },
    {
      "epoch": 0.6878624275144971,
      "grad_norm": 7.661020755767822,
      "learning_rate": 3.853562620809172e-05,
      "loss": 1.2888,
      "step": 3440
    },
    {
      "epoch": 0.6898620275944811,
      "grad_norm": 8.590173721313477,
      "learning_rate": 3.850229954009199e-05,
      "loss": 1.2561,
      "step": 3450
    },
    {
      "epoch": 0.6918616276744651,
      "grad_norm": 6.554612636566162,
      "learning_rate": 3.846897287209225e-05,
      "loss": 1.3874,
      "step": 3460
    },
    {
      "epoch": 0.6938612277544491,
      "grad_norm": 7.30633544921875,
      "learning_rate": 3.8435646204092515e-05,
      "loss": 1.0389,
      "step": 3470
    },
    {
      "epoch": 0.6958608278344331,
      "grad_norm": 9.767383575439453,
      "learning_rate": 3.8402319536092785e-05,
      "loss": 1.1564,
      "step": 3480
    },
    {
      "epoch": 0.6978604279144172,
      "grad_norm": 17.199951171875,
      "learning_rate": 3.836899286809305e-05,
      "loss": 1.3786,
      "step": 3490
    },
    {
      "epoch": 0.6998600279944012,
      "grad_norm": 12.99984073638916,
      "learning_rate": 3.833566620009332e-05,
      "loss": 1.0717,
      "step": 3500
    },
    {
      "epoch": 0.7018596280743852,
      "grad_norm": 10.522557258605957,
      "learning_rate": 3.830233953209358e-05,
      "loss": 1.3302,
      "step": 3510
    },
    {
      "epoch": 0.7038592281543691,
      "grad_norm": 6.328661918640137,
      "learning_rate": 3.826901286409385e-05,
      "loss": 1.0549,
      "step": 3520
    },
    {
      "epoch": 0.7058588282343531,
      "grad_norm": 6.699892997741699,
      "learning_rate": 3.8235686196094116e-05,
      "loss": 1.0214,
      "step": 3530
    },
    {
      "epoch": 0.7078584283143371,
      "grad_norm": 9.971946716308594,
      "learning_rate": 3.820235952809439e-05,
      "loss": 1.4373,
      "step": 3540
    },
    {
      "epoch": 0.7098580283943211,
      "grad_norm": 9.8873291015625,
      "learning_rate": 3.816903286009465e-05,
      "loss": 1.0573,
      "step": 3550
    },
    {
      "epoch": 0.7118576284743051,
      "grad_norm": 11.81954288482666,
      "learning_rate": 3.8135706192094914e-05,
      "loss": 1.1712,
      "step": 3560
    },
    {
      "epoch": 0.7138572285542891,
      "grad_norm": 8.340261459350586,
      "learning_rate": 3.8102379524095184e-05,
      "loss": 1.324,
      "step": 3570
    },
    {
      "epoch": 0.7158568286342731,
      "grad_norm": 9.43123722076416,
      "learning_rate": 3.806905285609545e-05,
      "loss": 1.1287,
      "step": 3580
    },
    {
      "epoch": 0.7178564287142571,
      "grad_norm": 14.495919227600098,
      "learning_rate": 3.803572618809572e-05,
      "loss": 1.4844,
      "step": 3590
    },
    {
      "epoch": 0.7198560287942412,
      "grad_norm": 8.78662395477295,
      "learning_rate": 3.800239952009598e-05,
      "loss": 1.3817,
      "step": 3600
    },
    {
      "epoch": 0.7218556288742252,
      "grad_norm": 9.767730712890625,
      "learning_rate": 3.796907285209625e-05,
      "loss": 0.9198,
      "step": 3610
    },
    {
      "epoch": 0.7238552289542092,
      "grad_norm": 12.570734024047852,
      "learning_rate": 3.7935746184096515e-05,
      "loss": 1.1038,
      "step": 3620
    },
    {
      "epoch": 0.7258548290341932,
      "grad_norm": 21.11334228515625,
      "learning_rate": 3.7902419516096785e-05,
      "loss": 1.1425,
      "step": 3630
    },
    {
      "epoch": 0.7278544291141772,
      "grad_norm": 7.805469512939453,
      "learning_rate": 3.786909284809705e-05,
      "loss": 1.0135,
      "step": 3640
    },
    {
      "epoch": 0.7298540291941612,
      "grad_norm": 8.64013671875,
      "learning_rate": 3.783576618009731e-05,
      "loss": 1.1986,
      "step": 3650
    },
    {
      "epoch": 0.7318536292741452,
      "grad_norm": 14.696758270263672,
      "learning_rate": 3.780243951209758e-05,
      "loss": 1.3546,
      "step": 3660
    },
    {
      "epoch": 0.7338532293541292,
      "grad_norm": 11.276936531066895,
      "learning_rate": 3.7769112844097846e-05,
      "loss": 1.3429,
      "step": 3670
    },
    {
      "epoch": 0.7358528294341132,
      "grad_norm": 17.966609954833984,
      "learning_rate": 3.7735786176098116e-05,
      "loss": 1.0624,
      "step": 3680
    },
    {
      "epoch": 0.7378524295140971,
      "grad_norm": 13.770709037780762,
      "learning_rate": 3.770245950809838e-05,
      "loss": 1.2056,
      "step": 3690
    },
    {
      "epoch": 0.7398520295940811,
      "grad_norm": 6.752033710479736,
      "learning_rate": 3.766913284009865e-05,
      "loss": 1.1658,
      "step": 3700
    },
    {
      "epoch": 0.7418516296740651,
      "grad_norm": 11.799617767333984,
      "learning_rate": 3.7635806172098914e-05,
      "loss": 1.1856,
      "step": 3710
    },
    {
      "epoch": 0.7438512297540492,
      "grad_norm": 17.172138214111328,
      "learning_rate": 3.7602479504099184e-05,
      "loss": 1.2781,
      "step": 3720
    },
    {
      "epoch": 0.7458508298340332,
      "grad_norm": 15.196943283081055,
      "learning_rate": 3.756915283609945e-05,
      "loss": 1.2169,
      "step": 3730
    },
    {
      "epoch": 0.7478504299140172,
      "grad_norm": 4.767385482788086,
      "learning_rate": 3.753582616809971e-05,
      "loss": 1.2243,
      "step": 3740
    },
    {
      "epoch": 0.7498500299940012,
      "grad_norm": 13.462945938110352,
      "learning_rate": 3.750249950009998e-05,
      "loss": 1.4067,
      "step": 3750
    },
    {
      "epoch": 0.7518496300739852,
      "grad_norm": 7.2137370109558105,
      "learning_rate": 3.7469172832100245e-05,
      "loss": 1.2452,
      "step": 3760
    },
    {
      "epoch": 0.7538492301539692,
      "grad_norm": 6.6230058670043945,
      "learning_rate": 3.7435846164100515e-05,
      "loss": 1.0704,
      "step": 3770
    },
    {
      "epoch": 0.7558488302339532,
      "grad_norm": 11.077132225036621,
      "learning_rate": 3.740251949610078e-05,
      "loss": 1.4128,
      "step": 3780
    },
    {
      "epoch": 0.7578484303139372,
      "grad_norm": 15.635919570922852,
      "learning_rate": 3.736919282810105e-05,
      "loss": 1.2471,
      "step": 3790
    },
    {
      "epoch": 0.7598480303939212,
      "grad_norm": 25.102005004882812,
      "learning_rate": 3.733586616010131e-05,
      "loss": 1.2006,
      "step": 3800
    },
    {
      "epoch": 0.7618476304739052,
      "grad_norm": 4.534594535827637,
      "learning_rate": 3.730253949210158e-05,
      "loss": 1.2177,
      "step": 3810
    },
    {
      "epoch": 0.7638472305538893,
      "grad_norm": 8.330376625061035,
      "learning_rate": 3.7269212824101846e-05,
      "loss": 1.2563,
      "step": 3820
    },
    {
      "epoch": 0.7658468306338733,
      "grad_norm": 5.789188861846924,
      "learning_rate": 3.723588615610211e-05,
      "loss": 1.1185,
      "step": 3830
    },
    {
      "epoch": 0.7678464307138573,
      "grad_norm": 11.026089668273926,
      "learning_rate": 3.720255948810238e-05,
      "loss": 1.0352,
      "step": 3840
    },
    {
      "epoch": 0.7698460307938413,
      "grad_norm": 7.858421802520752,
      "learning_rate": 3.716923282010264e-05,
      "loss": 1.1111,
      "step": 3850
    },
    {
      "epoch": 0.7718456308738252,
      "grad_norm": 12.779881477355957,
      "learning_rate": 3.713590615210292e-05,
      "loss": 1.113,
      "step": 3860
    },
    {
      "epoch": 0.7738452309538092,
      "grad_norm": 2.9998302459716797,
      "learning_rate": 3.7102579484103184e-05,
      "loss": 1.2392,
      "step": 3870
    },
    {
      "epoch": 0.7758448310337932,
      "grad_norm": 9.57577133178711,
      "learning_rate": 3.706925281610345e-05,
      "loss": 1.1163,
      "step": 3880
    },
    {
      "epoch": 0.7778444311137772,
      "grad_norm": 9.906031608581543,
      "learning_rate": 3.703592614810372e-05,
      "loss": 1.2483,
      "step": 3890
    },
    {
      "epoch": 0.7798440311937612,
      "grad_norm": 18.57105827331543,
      "learning_rate": 3.700259948010398e-05,
      "loss": 1.2075,
      "step": 3900
    },
    {
      "epoch": 0.7818436312737452,
      "grad_norm": 26.144001007080078,
      "learning_rate": 3.696927281210425e-05,
      "loss": 1.3552,
      "step": 3910
    },
    {
      "epoch": 0.7838432313537292,
      "grad_norm": 11.23480224609375,
      "learning_rate": 3.6935946144104515e-05,
      "loss": 1.2501,
      "step": 3920
    },
    {
      "epoch": 0.7858428314337133,
      "grad_norm": 8.739659309387207,
      "learning_rate": 3.6902619476104785e-05,
      "loss": 0.9989,
      "step": 3930
    },
    {
      "epoch": 0.7878424315136973,
      "grad_norm": 8.682856559753418,
      "learning_rate": 3.686929280810505e-05,
      "loss": 1.0536,
      "step": 3940
    },
    {
      "epoch": 0.7898420315936813,
      "grad_norm": 17.458547592163086,
      "learning_rate": 3.683596614010532e-05,
      "loss": 1.2393,
      "step": 3950
    },
    {
      "epoch": 0.7918416316736653,
      "grad_norm": 11.164270401000977,
      "learning_rate": 3.680263947210558e-05,
      "loss": 1.4945,
      "step": 3960
    },
    {
      "epoch": 0.7938412317536493,
      "grad_norm": 16.356037139892578,
      "learning_rate": 3.6769312804105846e-05,
      "loss": 1.2136,
      "step": 3970
    },
    {
      "epoch": 0.7958408318336333,
      "grad_norm": 8.346263885498047,
      "learning_rate": 3.6735986136106116e-05,
      "loss": 1.113,
      "step": 3980
    },
    {
      "epoch": 0.7978404319136173,
      "grad_norm": 14.635550498962402,
      "learning_rate": 3.670265946810638e-05,
      "loss": 1.3943,
      "step": 3990
    },
    {
      "epoch": 0.7998400319936013,
      "grad_norm": 14.845978736877441,
      "learning_rate": 3.666933280010665e-05,
      "loss": 0.8147,
      "step": 4000
    },
    {
      "epoch": 0.8018396320735853,
      "grad_norm": 14.854844093322754,
      "learning_rate": 3.6636006132106914e-05,
      "loss": 1.1508,
      "step": 4010
    },
    {
      "epoch": 0.8038392321535693,
      "grad_norm": 12.797466278076172,
      "learning_rate": 3.6602679464107184e-05,
      "loss": 1.3471,
      "step": 4020
    },
    {
      "epoch": 0.8058388322335532,
      "grad_norm": 9.929220199584961,
      "learning_rate": 3.656935279610745e-05,
      "loss": 1.273,
      "step": 4030
    },
    {
      "epoch": 0.8078384323135372,
      "grad_norm": 12.258395195007324,
      "learning_rate": 3.653602612810772e-05,
      "loss": 1.3169,
      "step": 4040
    },
    {
      "epoch": 0.8098380323935213,
      "grad_norm": 4.256494045257568,
      "learning_rate": 3.650269946010798e-05,
      "loss": 1.1675,
      "step": 4050
    },
    {
      "epoch": 0.8118376324735053,
      "grad_norm": 11.364853858947754,
      "learning_rate": 3.6469372792108245e-05,
      "loss": 1.276,
      "step": 4060
    },
    {
      "epoch": 0.8138372325534893,
      "grad_norm": 14.28345775604248,
      "learning_rate": 3.6436046124108515e-05,
      "loss": 0.9736,
      "step": 4070
    },
    {
      "epoch": 0.8158368326334733,
      "grad_norm": 13.391210556030273,
      "learning_rate": 3.640271945610878e-05,
      "loss": 1.276,
      "step": 4080
    },
    {
      "epoch": 0.8178364327134573,
      "grad_norm": 7.524712562561035,
      "learning_rate": 3.636939278810905e-05,
      "loss": 1.1983,
      "step": 4090
    },
    {
      "epoch": 0.8198360327934413,
      "grad_norm": 8.591636657714844,
      "learning_rate": 3.633606612010931e-05,
      "loss": 1.0727,
      "step": 4100
    },
    {
      "epoch": 0.8218356328734253,
      "grad_norm": 9.45606517791748,
      "learning_rate": 3.630273945210958e-05,
      "loss": 1.1102,
      "step": 4110
    },
    {
      "epoch": 0.8238352329534093,
      "grad_norm": 16.31220245361328,
      "learning_rate": 3.6269412784109846e-05,
      "loss": 1.1206,
      "step": 4120
    },
    {
      "epoch": 0.8258348330333933,
      "grad_norm": 8.74605655670166,
      "learning_rate": 3.623608611611011e-05,
      "loss": 1.2116,
      "step": 4130
    },
    {
      "epoch": 0.8278344331133773,
      "grad_norm": 12.346248626708984,
      "learning_rate": 3.620275944811038e-05,
      "loss": 1.0673,
      "step": 4140
    },
    {
      "epoch": 0.8298340331933614,
      "grad_norm": 12.661551475524902,
      "learning_rate": 3.616943278011064e-05,
      "loss": 1.3105,
      "step": 4150
    },
    {
      "epoch": 0.8318336332733454,
      "grad_norm": 11.918707847595215,
      "learning_rate": 3.6136106112110914e-05,
      "loss": 1.1331,
      "step": 4160
    },
    {
      "epoch": 0.8338332333533294,
      "grad_norm": 17.380149841308594,
      "learning_rate": 3.610277944411118e-05,
      "loss": 1.3181,
      "step": 4170
    },
    {
      "epoch": 0.8358328334333134,
      "grad_norm": 10.09432315826416,
      "learning_rate": 3.606945277611145e-05,
      "loss": 1.2067,
      "step": 4180
    },
    {
      "epoch": 0.8378324335132974,
      "grad_norm": 8.803640365600586,
      "learning_rate": 3.603612610811171e-05,
      "loss": 1.1309,
      "step": 4190
    },
    {
      "epoch": 0.8398320335932813,
      "grad_norm": 13.035538673400879,
      "learning_rate": 3.600279944011198e-05,
      "loss": 1.1715,
      "step": 4200
    },
    {
      "epoch": 0.8418316336732653,
      "grad_norm": 11.911402702331543,
      "learning_rate": 3.5969472772112245e-05,
      "loss": 1.5066,
      "step": 4210
    },
    {
      "epoch": 0.8438312337532493,
      "grad_norm": 16.935985565185547,
      "learning_rate": 3.593614610411251e-05,
      "loss": 1.2687,
      "step": 4220
    },
    {
      "epoch": 0.8458308338332333,
      "grad_norm": 15.074732780456543,
      "learning_rate": 3.590281943611278e-05,
      "loss": 1.168,
      "step": 4230
    },
    {
      "epoch": 0.8478304339132173,
      "grad_norm": 6.399120807647705,
      "learning_rate": 3.586949276811304e-05,
      "loss": 1.2842,
      "step": 4240
    },
    {
      "epoch": 0.8498300339932013,
      "grad_norm": 14.004207611083984,
      "learning_rate": 3.583616610011331e-05,
      "loss": 1.3401,
      "step": 4250
    },
    {
      "epoch": 0.8518296340731853,
      "grad_norm": 14.454651832580566,
      "learning_rate": 3.5802839432113576e-05,
      "loss": 1.1477,
      "step": 4260
    },
    {
      "epoch": 0.8538292341531694,
      "grad_norm": 16.35297203063965,
      "learning_rate": 3.5769512764113846e-05,
      "loss": 1.2992,
      "step": 4270
    },
    {
      "epoch": 0.8558288342331534,
      "grad_norm": 9.263208389282227,
      "learning_rate": 3.573618609611411e-05,
      "loss": 1.0686,
      "step": 4280
    },
    {
      "epoch": 0.8578284343131374,
      "grad_norm": 8.38028621673584,
      "learning_rate": 3.570285942811438e-05,
      "loss": 0.8009,
      "step": 4290
    },
    {
      "epoch": 0.8598280343931214,
      "grad_norm": 4.291645526885986,
      "learning_rate": 3.566953276011464e-05,
      "loss": 0.8402,
      "step": 4300
    },
    {
      "epoch": 0.8618276344731054,
      "grad_norm": 15.904095649719238,
      "learning_rate": 3.563620609211491e-05,
      "loss": 1.0968,
      "step": 4310
    },
    {
      "epoch": 0.8638272345530894,
      "grad_norm": 6.9572625160217285,
      "learning_rate": 3.560287942411518e-05,
      "loss": 1.2145,
      "step": 4320
    },
    {
      "epoch": 0.8658268346330734,
      "grad_norm": 12.459647178649902,
      "learning_rate": 3.556955275611544e-05,
      "loss": 0.9938,
      "step": 4330
    },
    {
      "epoch": 0.8678264347130574,
      "grad_norm": 11.682072639465332,
      "learning_rate": 3.553622608811571e-05,
      "loss": 1.0895,
      "step": 4340
    },
    {
      "epoch": 0.8698260347930414,
      "grad_norm": 14.969725608825684,
      "learning_rate": 3.5502899420115974e-05,
      "loss": 1.284,
      "step": 4350
    },
    {
      "epoch": 0.8718256348730254,
      "grad_norm": 9.82462215423584,
      "learning_rate": 3.5469572752116245e-05,
      "loss": 1.2934,
      "step": 4360
    },
    {
      "epoch": 0.8738252349530093,
      "grad_norm": 9.050975799560547,
      "learning_rate": 3.5436246084116515e-05,
      "loss": 1.1432,
      "step": 4370
    },
    {
      "epoch": 0.8758248350329934,
      "grad_norm": 10.72243881225586,
      "learning_rate": 3.540291941611678e-05,
      "loss": 1.0824,
      "step": 4380
    },
    {
      "epoch": 0.8778244351129774,
      "grad_norm": 39.329166412353516,
      "learning_rate": 3.536959274811705e-05,
      "loss": 1.3842,
      "step": 4390
    },
    {
      "epoch": 0.8798240351929614,
      "grad_norm": 8.453580856323242,
      "learning_rate": 3.533626608011731e-05,
      "loss": 1.309,
      "step": 4400
    },
    {
      "epoch": 0.8818236352729454,
      "grad_norm": 14.04312801361084,
      "learning_rate": 3.530293941211758e-05,
      "loss": 1.2582,
      "step": 4410
    },
    {
      "epoch": 0.8838232353529294,
      "grad_norm": 8.614508628845215,
      "learning_rate": 3.5269612744117846e-05,
      "loss": 1.1154,
      "step": 4420
    },
    {
      "epoch": 0.8858228354329134,
      "grad_norm": 12.570950508117676,
      "learning_rate": 3.5236286076118116e-05,
      "loss": 1.2485,
      "step": 4430
    },
    {
      "epoch": 0.8878224355128974,
      "grad_norm": 6.513674736022949,
      "learning_rate": 3.520295940811838e-05,
      "loss": 1.1752,
      "step": 4440
    },
    {
      "epoch": 0.8898220355928814,
      "grad_norm": 9.635892868041992,
      "learning_rate": 3.516963274011864e-05,
      "loss": 1.3091,
      "step": 4450
    },
    {
      "epoch": 0.8918216356728654,
      "grad_norm": 14.788000106811523,
      "learning_rate": 3.5136306072118913e-05,
      "loss": 1.3712,
      "step": 4460
    },
    {
      "epoch": 0.8938212357528494,
      "grad_norm": 14.451257705688477,
      "learning_rate": 3.510297940411918e-05,
      "loss": 1.0481,
      "step": 4470
    },
    {
      "epoch": 0.8958208358328335,
      "grad_norm": 7.513576984405518,
      "learning_rate": 3.506965273611945e-05,
      "loss": 1.2806,
      "step": 4480
    },
    {
      "epoch": 0.8978204359128175,
      "grad_norm": 9.49386978149414,
      "learning_rate": 3.503632606811971e-05,
      "loss": 1.1525,
      "step": 4490
    },
    {
      "epoch": 0.8998200359928015,
      "grad_norm": 10.454849243164062,
      "learning_rate": 3.500299940011998e-05,
      "loss": 1.0675,
      "step": 4500
    },
    {
      "epoch": 0.9018196360727855,
      "grad_norm": 13.658658981323242,
      "learning_rate": 3.4969672732120245e-05,
      "loss": 0.9555,
      "step": 4510
    },
    {
      "epoch": 0.9038192361527695,
      "grad_norm": 10.609099388122559,
      "learning_rate": 3.4936346064120515e-05,
      "loss": 1.3651,
      "step": 4520
    },
    {
      "epoch": 0.9058188362327535,
      "grad_norm": 10.870566368103027,
      "learning_rate": 3.490301939612078e-05,
      "loss": 0.9185,
      "step": 4530
    },
    {
      "epoch": 0.9078184363127374,
      "grad_norm": 11.157939910888672,
      "learning_rate": 3.486969272812104e-05,
      "loss": 1.2257,
      "step": 4540
    },
    {
      "epoch": 0.9098180363927214,
      "grad_norm": 11.493034362792969,
      "learning_rate": 3.483636606012131e-05,
      "loss": 1.3589,
      "step": 4550
    },
    {
      "epoch": 0.9118176364727054,
      "grad_norm": 13.887752532958984,
      "learning_rate": 3.4803039392121576e-05,
      "loss": 1.3894,
      "step": 4560
    },
    {
      "epoch": 0.9138172365526894,
      "grad_norm": 15.50894832611084,
      "learning_rate": 3.4769712724121846e-05,
      "loss": 1.07,
      "step": 4570
    },
    {
      "epoch": 0.9158168366326734,
      "grad_norm": 8.387073516845703,
      "learning_rate": 3.473638605612211e-05,
      "loss": 1.1459,
      "step": 4580
    },
    {
      "epoch": 0.9178164367126574,
      "grad_norm": 11.760666847229004,
      "learning_rate": 3.470305938812238e-05,
      "loss": 1.1632,
      "step": 4590
    },
    {
      "epoch": 0.9198160367926415,
      "grad_norm": 8.760969161987305,
      "learning_rate": 3.466973272012264e-05,
      "loss": 1.1117,
      "step": 4600
    },
    {
      "epoch": 0.9218156368726255,
      "grad_norm": 6.207510471343994,
      "learning_rate": 3.4636406052122913e-05,
      "loss": 1.3161,
      "step": 4610
    },
    {
      "epoch": 0.9238152369526095,
      "grad_norm": 7.408812999725342,
      "learning_rate": 3.460307938412318e-05,
      "loss": 1.0473,
      "step": 4620
    },
    {
      "epoch": 0.9258148370325935,
      "grad_norm": 4.378427505493164,
      "learning_rate": 3.456975271612344e-05,
      "loss": 1.1607,
      "step": 4630
    },
    {
      "epoch": 0.9278144371125775,
      "grad_norm": 10.167492866516113,
      "learning_rate": 3.453642604812371e-05,
      "loss": 1.2536,
      "step": 4640
    },
    {
      "epoch": 0.9298140371925615,
      "grad_norm": 14.910898208618164,
      "learning_rate": 3.4503099380123974e-05,
      "loss": 1.2916,
      "step": 4650
    },
    {
      "epoch": 0.9318136372725455,
      "grad_norm": 10.41291618347168,
      "learning_rate": 3.4469772712124244e-05,
      "loss": 1.247,
      "step": 4660
    },
    {
      "epoch": 0.9338132373525295,
      "grad_norm": 11.666461944580078,
      "learning_rate": 3.443644604412451e-05,
      "loss": 1.4462,
      "step": 4670
    },
    {
      "epoch": 0.9358128374325135,
      "grad_norm": 10.939391136169434,
      "learning_rate": 3.440311937612478e-05,
      "loss": 1.1648,
      "step": 4680
    },
    {
      "epoch": 0.9378124375124975,
      "grad_norm": 9.107660293579102,
      "learning_rate": 3.436979270812504e-05,
      "loss": 1.2819,
      "step": 4690
    },
    {
      "epoch": 0.9398120375924816,
      "grad_norm": 11.61727523803711,
      "learning_rate": 3.433646604012531e-05,
      "loss": 1.1475,
      "step": 4700
    },
    {
      "epoch": 0.9418116376724655,
      "grad_norm": 17.280546188354492,
      "learning_rate": 3.4303139372125576e-05,
      "loss": 1.2758,
      "step": 4710
    },
    {
      "epoch": 0.9438112377524495,
      "grad_norm": 12.218339920043945,
      "learning_rate": 3.426981270412584e-05,
      "loss": 1.3179,
      "step": 4720
    },
    {
      "epoch": 0.9458108378324335,
      "grad_norm": 7.429111003875732,
      "learning_rate": 3.423648603612611e-05,
      "loss": 1.1278,
      "step": 4730
    },
    {
      "epoch": 0.9478104379124175,
      "grad_norm": 6.1706624031066895,
      "learning_rate": 3.420315936812637e-05,
      "loss": 1.2153,
      "step": 4740
    },
    {
      "epoch": 0.9498100379924015,
      "grad_norm": 9.295451164245605,
      "learning_rate": 3.416983270012664e-05,
      "loss": 1.1627,
      "step": 4750
    },
    {
      "epoch": 0.9518096380723855,
      "grad_norm": 9.740483283996582,
      "learning_rate": 3.4136506032126907e-05,
      "loss": 1.3606,
      "step": 4760
    },
    {
      "epoch": 0.9538092381523695,
      "grad_norm": 6.441413879394531,
      "learning_rate": 3.410317936412718e-05,
      "loss": 1.002,
      "step": 4770
    },
    {
      "epoch": 0.9558088382323535,
      "grad_norm": 8.092058181762695,
      "learning_rate": 3.406985269612744e-05,
      "loss": 1.3924,
      "step": 4780
    },
    {
      "epoch": 0.9578084383123375,
      "grad_norm": 7.319852828979492,
      "learning_rate": 3.403652602812771e-05,
      "loss": 1.2565,
      "step": 4790
    },
    {
      "epoch": 0.9598080383923215,
      "grad_norm": 10.23288345336914,
      "learning_rate": 3.4003199360127974e-05,
      "loss": 1.3499,
      "step": 4800
    },
    {
      "epoch": 0.9618076384723055,
      "grad_norm": 20.769672393798828,
      "learning_rate": 3.396987269212824e-05,
      "loss": 1.3413,
      "step": 4810
    },
    {
      "epoch": 0.9638072385522896,
      "grad_norm": 7.018844127655029,
      "learning_rate": 3.393654602412851e-05,
      "loss": 1.2197,
      "step": 4820
    },
    {
      "epoch": 0.9658068386322736,
      "grad_norm": 7.582221508026123,
      "learning_rate": 3.390321935612877e-05,
      "loss": 1.1589,
      "step": 4830
    },
    {
      "epoch": 0.9678064387122576,
      "grad_norm": 8.07785701751709,
      "learning_rate": 3.386989268812904e-05,
      "loss": 1.0716,
      "step": 4840
    },
    {
      "epoch": 0.9698060387922416,
      "grad_norm": 9.573235511779785,
      "learning_rate": 3.3836566020129305e-05,
      "loss": 0.9411,
      "step": 4850
    },
    {
      "epoch": 0.9718056388722256,
      "grad_norm": 13.495512008666992,
      "learning_rate": 3.3803239352129576e-05,
      "loss": 1.2313,
      "step": 4860
    },
    {
      "epoch": 0.9738052389522096,
      "grad_norm": 6.523849964141846,
      "learning_rate": 3.376991268412984e-05,
      "loss": 0.9583,
      "step": 4870
    },
    {
      "epoch": 0.9758048390321936,
      "grad_norm": 8.444738388061523,
      "learning_rate": 3.373658601613011e-05,
      "loss": 1.0424,
      "step": 4880
    },
    {
      "epoch": 0.9778044391121775,
      "grad_norm": 9.013163566589355,
      "learning_rate": 3.370325934813038e-05,
      "loss": 1.1927,
      "step": 4890
    },
    {
      "epoch": 0.9798040391921615,
      "grad_norm": 14.2869234085083,
      "learning_rate": 3.366993268013064e-05,
      "loss": 1.6121,
      "step": 4900
    },
    {
      "epoch": 0.9818036392721455,
      "grad_norm": 7.120988368988037,
      "learning_rate": 3.363660601213091e-05,
      "loss": 1.3173,
      "step": 4910
    },
    {
      "epoch": 0.9838032393521295,
      "grad_norm": 17.530702590942383,
      "learning_rate": 3.360327934413118e-05,
      "loss": 1.1298,
      "step": 4920
    },
    {
      "epoch": 0.9858028394321136,
      "grad_norm": 7.3383073806762695,
      "learning_rate": 3.356995267613145e-05,
      "loss": 1.2524,
      "step": 4930
    },
    {
      "epoch": 0.9878024395120976,
      "grad_norm": 9.781767845153809,
      "learning_rate": 3.353662600813171e-05,
      "loss": 1.0125,
      "step": 4940
    },
    {
      "epoch": 0.9898020395920816,
      "grad_norm": 13.11892318725586,
      "learning_rate": 3.3503299340131974e-05,
      "loss": 0.9793,
      "step": 4950
    },
    {
      "epoch": 0.9918016396720656,
      "grad_norm": 11.650012969970703,
      "learning_rate": 3.3469972672132244e-05,
      "loss": 1.2953,
      "step": 4960
    },
    {
      "epoch": 0.9938012397520496,
      "grad_norm": 14.943472862243652,
      "learning_rate": 3.343664600413251e-05,
      "loss": 1.1491,
      "step": 4970
    },
    {
      "epoch": 0.9958008398320336,
      "grad_norm": 13.538097381591797,
      "learning_rate": 3.340331933613278e-05,
      "loss": 1.1344,
      "step": 4980
    },
    {
      "epoch": 0.9978004399120176,
      "grad_norm": 6.88840913772583,
      "learning_rate": 3.336999266813304e-05,
      "loss": 1.2435,
      "step": 4990
    },
    {
      "epoch": 0.9998000399920016,
      "grad_norm": 24.508262634277344,
      "learning_rate": 3.333666600013331e-05,
      "loss": 1.1372,
      "step": 5000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6237752449510098,
      "eval_loss": 1.1715487241744995,
      "eval_runtime": 42.8644,
      "eval_samples_per_second": 233.34,
      "eval_steps_per_second": 29.185,
      "step": 5001
    },
    {
      "epoch": 1.0017996400719855,
      "grad_norm": 10.80722427368164,
      "learning_rate": 3.3303339332133575e-05,
      "loss": 1.163,
      "step": 5010
    },
    {
      "epoch": 1.0037992401519695,
      "grad_norm": 8.493399620056152,
      "learning_rate": 3.3270012664133846e-05,
      "loss": 0.9369,
      "step": 5020
    },
    {
      "epoch": 1.0057988402319535,
      "grad_norm": 15.178129196166992,
      "learning_rate": 3.323668599613411e-05,
      "loss": 0.9431,
      "step": 5030
    },
    {
      "epoch": 1.0077984403119375,
      "grad_norm": 6.269545078277588,
      "learning_rate": 3.320335932813437e-05,
      "loss": 0.9414,
      "step": 5040
    },
    {
      "epoch": 1.0097980403919216,
      "grad_norm": 6.547365665435791,
      "learning_rate": 3.317003266013464e-05,
      "loss": 0.9277,
      "step": 5050
    },
    {
      "epoch": 1.0117976404719056,
      "grad_norm": 7.8420491218566895,
      "learning_rate": 3.3136705992134907e-05,
      "loss": 1.1022,
      "step": 5060
    },
    {
      "epoch": 1.0137972405518896,
      "grad_norm": 6.245758056640625,
      "learning_rate": 3.310337932413518e-05,
      "loss": 0.7611,
      "step": 5070
    },
    {
      "epoch": 1.0157968406318736,
      "grad_norm": 9.121499061584473,
      "learning_rate": 3.307005265613544e-05,
      "loss": 0.9662,
      "step": 5080
    },
    {
      "epoch": 1.0177964407118576,
      "grad_norm": 9.858642578125,
      "learning_rate": 3.303672598813571e-05,
      "loss": 1.0268,
      "step": 5090
    },
    {
      "epoch": 1.0197960407918416,
      "grad_norm": 11.590520858764648,
      "learning_rate": 3.3003399320135974e-05,
      "loss": 0.8286,
      "step": 5100
    },
    {
      "epoch": 1.0217956408718256,
      "grad_norm": 8.963308334350586,
      "learning_rate": 3.2970072652136244e-05,
      "loss": 0.9525,
      "step": 5110
    },
    {
      "epoch": 1.0237952409518096,
      "grad_norm": 12.631857872009277,
      "learning_rate": 3.293674598413651e-05,
      "loss": 1.0707,
      "step": 5120
    },
    {
      "epoch": 1.0257948410317936,
      "grad_norm": 13.74509048461914,
      "learning_rate": 3.290341931613677e-05,
      "loss": 0.9984,
      "step": 5130
    },
    {
      "epoch": 1.0277944411117776,
      "grad_norm": 8.911447525024414,
      "learning_rate": 3.287009264813704e-05,
      "loss": 1.1386,
      "step": 5140
    },
    {
      "epoch": 1.0297940411917617,
      "grad_norm": 10.373127937316895,
      "learning_rate": 3.2836765980137305e-05,
      "loss": 0.8283,
      "step": 5150
    },
    {
      "epoch": 1.0317936412717457,
      "grad_norm": 13.13809871673584,
      "learning_rate": 3.2803439312137575e-05,
      "loss": 0.9403,
      "step": 5160
    },
    {
      "epoch": 1.0337932413517297,
      "grad_norm": 14.591425895690918,
      "learning_rate": 3.277011264413784e-05,
      "loss": 1.065,
      "step": 5170
    },
    {
      "epoch": 1.0357928414317137,
      "grad_norm": 7.198284149169922,
      "learning_rate": 3.273678597613811e-05,
      "loss": 0.8203,
      "step": 5180
    },
    {
      "epoch": 1.0377924415116977,
      "grad_norm": 11.379169464111328,
      "learning_rate": 3.270345930813837e-05,
      "loss": 1.1749,
      "step": 5190
    },
    {
      "epoch": 1.0397920415916817,
      "grad_norm": 13.003775596618652,
      "learning_rate": 3.267013264013864e-05,
      "loss": 0.9096,
      "step": 5200
    },
    {
      "epoch": 1.0417916416716657,
      "grad_norm": 17.678098678588867,
      "learning_rate": 3.2636805972138906e-05,
      "loss": 0.9329,
      "step": 5210
    },
    {
      "epoch": 1.0437912417516497,
      "grad_norm": 5.709446907043457,
      "learning_rate": 3.260347930413917e-05,
      "loss": 0.9144,
      "step": 5220
    },
    {
      "epoch": 1.0457908418316337,
      "grad_norm": 14.666826248168945,
      "learning_rate": 3.257015263613944e-05,
      "loss": 0.7632,
      "step": 5230
    },
    {
      "epoch": 1.0477904419116177,
      "grad_norm": 6.8980231285095215,
      "learning_rate": 3.2536825968139704e-05,
      "loss": 0.6891,
      "step": 5240
    },
    {
      "epoch": 1.0497900419916018,
      "grad_norm": 13.20803451538086,
      "learning_rate": 3.2503499300139974e-05,
      "loss": 0.937,
      "step": 5250
    },
    {
      "epoch": 1.0517896420715858,
      "grad_norm": 12.968145370483398,
      "learning_rate": 3.247017263214024e-05,
      "loss": 0.7012,
      "step": 5260
    },
    {
      "epoch": 1.0537892421515698,
      "grad_norm": 16.991886138916016,
      "learning_rate": 3.243684596414051e-05,
      "loss": 1.2341,
      "step": 5270
    },
    {
      "epoch": 1.0557888422315538,
      "grad_norm": 21.708141326904297,
      "learning_rate": 3.240351929614077e-05,
      "loss": 1.2953,
      "step": 5280
    },
    {
      "epoch": 1.0577884423115378,
      "grad_norm": 11.243534088134766,
      "learning_rate": 3.237019262814104e-05,
      "loss": 1.1495,
      "step": 5290
    },
    {
      "epoch": 1.0597880423915218,
      "grad_norm": 7.377511978149414,
      "learning_rate": 3.2336865960141305e-05,
      "loss": 0.7342,
      "step": 5300
    },
    {
      "epoch": 1.0617876424715056,
      "grad_norm": 8.163166999816895,
      "learning_rate": 3.230353929214157e-05,
      "loss": 0.954,
      "step": 5310
    },
    {
      "epoch": 1.0637872425514896,
      "grad_norm": 7.441366672515869,
      "learning_rate": 3.227021262414184e-05,
      "loss": 0.9379,
      "step": 5320
    },
    {
      "epoch": 1.0657868426314736,
      "grad_norm": 12.603632926940918,
      "learning_rate": 3.22368859561421e-05,
      "loss": 0.8223,
      "step": 5330
    },
    {
      "epoch": 1.0677864427114576,
      "grad_norm": 8.971607208251953,
      "learning_rate": 3.220355928814237e-05,
      "loss": 0.845,
      "step": 5340
    },
    {
      "epoch": 1.0697860427914416,
      "grad_norm": 3.259829521179199,
      "learning_rate": 3.2170232620142636e-05,
      "loss": 1.0544,
      "step": 5350
    },
    {
      "epoch": 1.0717856428714256,
      "grad_norm": 6.0408782958984375,
      "learning_rate": 3.2136905952142906e-05,
      "loss": 0.9631,
      "step": 5360
    },
    {
      "epoch": 1.0737852429514096,
      "grad_norm": 7.043534278869629,
      "learning_rate": 3.210357928414317e-05,
      "loss": 0.5751,
      "step": 5370
    },
    {
      "epoch": 1.0757848430313937,
      "grad_norm": 9.236739158630371,
      "learning_rate": 3.207025261614344e-05,
      "loss": 1.0121,
      "step": 5380
    },
    {
      "epoch": 1.0777844431113777,
      "grad_norm": 6.585311412811279,
      "learning_rate": 3.2036925948143704e-05,
      "loss": 0.8082,
      "step": 5390
    },
    {
      "epoch": 1.0797840431913617,
      "grad_norm": 11.481274604797363,
      "learning_rate": 3.2003599280143974e-05,
      "loss": 0.6042,
      "step": 5400
    },
    {
      "epoch": 1.0817836432713457,
      "grad_norm": 8.64932632446289,
      "learning_rate": 3.1970272612144244e-05,
      "loss": 1.0637,
      "step": 5410
    },
    {
      "epoch": 1.0837832433513297,
      "grad_norm": 9.197028160095215,
      "learning_rate": 3.193694594414451e-05,
      "loss": 0.7352,
      "step": 5420
    },
    {
      "epoch": 1.0857828434313137,
      "grad_norm": 18.155794143676758,
      "learning_rate": 3.190361927614478e-05,
      "loss": 0.8185,
      "step": 5430
    },
    {
      "epoch": 1.0877824435112977,
      "grad_norm": 16.271743774414062,
      "learning_rate": 3.187029260814504e-05,
      "loss": 1.1765,
      "step": 5440
    },
    {
      "epoch": 1.0897820435912817,
      "grad_norm": 13.116707801818848,
      "learning_rate": 3.1836965940145305e-05,
      "loss": 0.8313,
      "step": 5450
    },
    {
      "epoch": 1.0917816436712657,
      "grad_norm": 6.3304762840271,
      "learning_rate": 3.1803639272145575e-05,
      "loss": 0.8482,
      "step": 5460
    },
    {
      "epoch": 1.0937812437512497,
      "grad_norm": 14.8366117477417,
      "learning_rate": 3.177031260414584e-05,
      "loss": 1.2845,
      "step": 5470
    },
    {
      "epoch": 1.0957808438312338,
      "grad_norm": 16.442058563232422,
      "learning_rate": 3.173698593614611e-05,
      "loss": 0.824,
      "step": 5480
    },
    {
      "epoch": 1.0977804439112178,
      "grad_norm": 7.993218898773193,
      "learning_rate": 3.170365926814637e-05,
      "loss": 1.1316,
      "step": 5490
    },
    {
      "epoch": 1.0997800439912018,
      "grad_norm": 11.515085220336914,
      "learning_rate": 3.167033260014664e-05,
      "loss": 0.9085,
      "step": 5500
    },
    {
      "epoch": 1.1017796440711858,
      "grad_norm": 7.664937973022461,
      "learning_rate": 3.1637005932146906e-05,
      "loss": 0.776,
      "step": 5510
    },
    {
      "epoch": 1.1037792441511698,
      "grad_norm": 9.206944465637207,
      "learning_rate": 3.160367926414718e-05,
      "loss": 1.0517,
      "step": 5520
    },
    {
      "epoch": 1.1057788442311538,
      "grad_norm": 13.307101249694824,
      "learning_rate": 3.157035259614744e-05,
      "loss": 0.8342,
      "step": 5530
    },
    {
      "epoch": 1.1077784443111378,
      "grad_norm": 12.264434814453125,
      "learning_rate": 3.1537025928147704e-05,
      "loss": 1.0454,
      "step": 5540
    },
    {
      "epoch": 1.1097780443911218,
      "grad_norm": 5.8035569190979,
      "learning_rate": 3.1503699260147974e-05,
      "loss": 0.6741,
      "step": 5550
    },
    {
      "epoch": 1.1117776444711058,
      "grad_norm": 16.37992286682129,
      "learning_rate": 3.147037259214824e-05,
      "loss": 1.066,
      "step": 5560
    },
    {
      "epoch": 1.1137772445510898,
      "grad_norm": 17.01441192626953,
      "learning_rate": 3.143704592414851e-05,
      "loss": 0.8589,
      "step": 5570
    },
    {
      "epoch": 1.1157768446310738,
      "grad_norm": 14.23218822479248,
      "learning_rate": 3.140371925614877e-05,
      "loss": 0.9207,
      "step": 5580
    },
    {
      "epoch": 1.1177764447110579,
      "grad_norm": 11.696382522583008,
      "learning_rate": 3.137039258814904e-05,
      "loss": 1.1679,
      "step": 5590
    },
    {
      "epoch": 1.1197760447910419,
      "grad_norm": 8.917357444763184,
      "learning_rate": 3.1337065920149305e-05,
      "loss": 0.6825,
      "step": 5600
    },
    {
      "epoch": 1.1217756448710259,
      "grad_norm": 12.26638126373291,
      "learning_rate": 3.1303739252149575e-05,
      "loss": 0.8601,
      "step": 5610
    },
    {
      "epoch": 1.1237752449510099,
      "grad_norm": 11.834938049316406,
      "learning_rate": 3.127041258414984e-05,
      "loss": 0.8281,
      "step": 5620
    },
    {
      "epoch": 1.125774845030994,
      "grad_norm": 9.974050521850586,
      "learning_rate": 3.12370859161501e-05,
      "loss": 0.9397,
      "step": 5630
    },
    {
      "epoch": 1.127774445110978,
      "grad_norm": 11.04778003692627,
      "learning_rate": 3.120375924815037e-05,
      "loss": 1.1122,
      "step": 5640
    },
    {
      "epoch": 1.129774045190962,
      "grad_norm": 9.386514663696289,
      "learning_rate": 3.1170432580150636e-05,
      "loss": 0.9728,
      "step": 5650
    },
    {
      "epoch": 1.131773645270946,
      "grad_norm": 17.45366668701172,
      "learning_rate": 3.1137105912150906e-05,
      "loss": 1.0274,
      "step": 5660
    },
    {
      "epoch": 1.13377324535093,
      "grad_norm": 15.180498123168945,
      "learning_rate": 3.110377924415117e-05,
      "loss": 1.1871,
      "step": 5670
    },
    {
      "epoch": 1.1357728454309137,
      "grad_norm": 7.698610305786133,
      "learning_rate": 3.107045257615144e-05,
      "loss": 0.8722,
      "step": 5680
    },
    {
      "epoch": 1.1377724455108977,
      "grad_norm": 3.4791507720947266,
      "learning_rate": 3.1037125908151704e-05,
      "loss": 0.7908,
      "step": 5690
    },
    {
      "epoch": 1.1397720455908817,
      "grad_norm": 9.352248191833496,
      "learning_rate": 3.1003799240151974e-05,
      "loss": 0.8986,
      "step": 5700
    },
    {
      "epoch": 1.1417716456708658,
      "grad_norm": 8.973991394042969,
      "learning_rate": 3.097047257215224e-05,
      "loss": 0.8611,
      "step": 5710
    },
    {
      "epoch": 1.1437712457508498,
      "grad_norm": 17.87181854248047,
      "learning_rate": 3.09371459041525e-05,
      "loss": 1.0444,
      "step": 5720
    },
    {
      "epoch": 1.1457708458308338,
      "grad_norm": 7.875071048736572,
      "learning_rate": 3.090381923615277e-05,
      "loss": 0.7987,
      "step": 5730
    },
    {
      "epoch": 1.1477704459108178,
      "grad_norm": 18.167049407958984,
      "learning_rate": 3.0870492568153035e-05,
      "loss": 1.1651,
      "step": 5740
    },
    {
      "epoch": 1.1497700459908018,
      "grad_norm": 10.68212890625,
      "learning_rate": 3.0837165900153305e-05,
      "loss": 0.6871,
      "step": 5750
    },
    {
      "epoch": 1.1517696460707858,
      "grad_norm": 12.9796142578125,
      "learning_rate": 3.080383923215357e-05,
      "loss": 0.8647,
      "step": 5760
    },
    {
      "epoch": 1.1537692461507698,
      "grad_norm": 14.928367614746094,
      "learning_rate": 3.077051256415384e-05,
      "loss": 0.8908,
      "step": 5770
    },
    {
      "epoch": 1.1557688462307538,
      "grad_norm": 14.036500930786133,
      "learning_rate": 3.07371858961541e-05,
      "loss": 1.0174,
      "step": 5780
    },
    {
      "epoch": 1.1577684463107378,
      "grad_norm": 16.86418914794922,
      "learning_rate": 3.0703859228154366e-05,
      "loss": 0.8741,
      "step": 5790
    },
    {
      "epoch": 1.1597680463907218,
      "grad_norm": 9.419305801391602,
      "learning_rate": 3.0670532560154636e-05,
      "loss": 0.9169,
      "step": 5800
    },
    {
      "epoch": 1.1617676464707059,
      "grad_norm": 19.17961883544922,
      "learning_rate": 3.06372058921549e-05,
      "loss": 0.8474,
      "step": 5810
    },
    {
      "epoch": 1.1637672465506899,
      "grad_norm": 16.903907775878906,
      "learning_rate": 3.060387922415517e-05,
      "loss": 0.9598,
      "step": 5820
    },
    {
      "epoch": 1.1657668466306739,
      "grad_norm": 10.185314178466797,
      "learning_rate": 3.057055255615543e-05,
      "loss": 1.1211,
      "step": 5830
    },
    {
      "epoch": 1.1677664467106579,
      "grad_norm": 11.333818435668945,
      "learning_rate": 3.0537225888155704e-05,
      "loss": 1.1428,
      "step": 5840
    },
    {
      "epoch": 1.169766046790642,
      "grad_norm": 12.338004112243652,
      "learning_rate": 3.0503899220155967e-05,
      "loss": 0.69,
      "step": 5850
    },
    {
      "epoch": 1.171765646870626,
      "grad_norm": 14.337776184082031,
      "learning_rate": 3.0470572552156234e-05,
      "loss": 1.1756,
      "step": 5860
    },
    {
      "epoch": 1.17376524695061,
      "grad_norm": 11.719893455505371,
      "learning_rate": 3.04372458841565e-05,
      "loss": 0.9811,
      "step": 5870
    },
    {
      "epoch": 1.175764847030594,
      "grad_norm": 21.65361785888672,
      "learning_rate": 3.0403919216156768e-05,
      "loss": 1.1197,
      "step": 5880
    },
    {
      "epoch": 1.177764447110578,
      "grad_norm": 18.096418380737305,
      "learning_rate": 3.0370592548157035e-05,
      "loss": 1.1154,
      "step": 5890
    },
    {
      "epoch": 1.179764047190562,
      "grad_norm": 20.325082778930664,
      "learning_rate": 3.03372658801573e-05,
      "loss": 0.8448,
      "step": 5900
    },
    {
      "epoch": 1.181763647270546,
      "grad_norm": 6.630752086639404,
      "learning_rate": 3.0303939212157572e-05,
      "loss": 0.9076,
      "step": 5910
    },
    {
      "epoch": 1.18376324735053,
      "grad_norm": 7.704328536987305,
      "learning_rate": 3.027061254415784e-05,
      "loss": 1.0016,
      "step": 5920
    },
    {
      "epoch": 1.185762847430514,
      "grad_norm": 9.372870445251465,
      "learning_rate": 3.0237285876158106e-05,
      "loss": 1.2018,
      "step": 5930
    },
    {
      "epoch": 1.187762447510498,
      "grad_norm": 22.715303421020508,
      "learning_rate": 3.0203959208158373e-05,
      "loss": 1.1119,
      "step": 5940
    },
    {
      "epoch": 1.189762047590482,
      "grad_norm": 8.567610740661621,
      "learning_rate": 3.017063254015864e-05,
      "loss": 0.9934,
      "step": 5950
    },
    {
      "epoch": 1.191761647670466,
      "grad_norm": 18.01203155517578,
      "learning_rate": 3.0137305872158906e-05,
      "loss": 1.2177,
      "step": 5960
    },
    {
      "epoch": 1.19376124775045,
      "grad_norm": 10.317336082458496,
      "learning_rate": 3.0103979204159173e-05,
      "loss": 0.6794,
      "step": 5970
    },
    {
      "epoch": 1.1957608478304338,
      "grad_norm": 5.600399494171143,
      "learning_rate": 3.0070652536159437e-05,
      "loss": 0.8211,
      "step": 5980
    },
    {
      "epoch": 1.1977604479104178,
      "grad_norm": 14.516963005065918,
      "learning_rate": 3.0037325868159704e-05,
      "loss": 0.813,
      "step": 5990
    },
    {
      "epoch": 1.1997600479904018,
      "grad_norm": 17.314960479736328,
      "learning_rate": 3.000399920015997e-05,
      "loss": 0.925,
      "step": 6000
    },
    {
      "epoch": 1.2017596480703858,
      "grad_norm": 13.642000198364258,
      "learning_rate": 2.9970672532160237e-05,
      "loss": 0.8871,
      "step": 6010
    },
    {
      "epoch": 1.2037592481503698,
      "grad_norm": 7.610594272613525,
      "learning_rate": 2.9937345864160504e-05,
      "loss": 0.9739,
      "step": 6020
    },
    {
      "epoch": 1.2057588482303538,
      "grad_norm": 1.6586260795593262,
      "learning_rate": 2.990401919616077e-05,
      "loss": 0.7223,
      "step": 6030
    },
    {
      "epoch": 1.2077584483103379,
      "grad_norm": 16.722536087036133,
      "learning_rate": 2.9870692528161038e-05,
      "loss": 0.914,
      "step": 6040
    },
    {
      "epoch": 1.2097580483903219,
      "grad_norm": 15.140969276428223,
      "learning_rate": 2.9837365860161305e-05,
      "loss": 0.7815,
      "step": 6050
    },
    {
      "epoch": 1.2117576484703059,
      "grad_norm": 10.601222038269043,
      "learning_rate": 2.9804039192161572e-05,
      "loss": 0.9558,
      "step": 6060
    },
    {
      "epoch": 1.2137572485502899,
      "grad_norm": 10.353321075439453,
      "learning_rate": 2.9770712524161835e-05,
      "loss": 0.9428,
      "step": 6070
    },
    {
      "epoch": 1.215756848630274,
      "grad_norm": 11.745906829833984,
      "learning_rate": 2.9737385856162102e-05,
      "loss": 1.24,
      "step": 6080
    },
    {
      "epoch": 1.217756448710258,
      "grad_norm": 13.115339279174805,
      "learning_rate": 2.970405918816237e-05,
      "loss": 0.9802,
      "step": 6090
    },
    {
      "epoch": 1.219756048790242,
      "grad_norm": 8.960882186889648,
      "learning_rate": 2.9670732520162636e-05,
      "loss": 0.9058,
      "step": 6100
    },
    {
      "epoch": 1.221755648870226,
      "grad_norm": 12.755616188049316,
      "learning_rate": 2.9637405852162903e-05,
      "loss": 0.7299,
      "step": 6110
    },
    {
      "epoch": 1.22375524895021,
      "grad_norm": 11.39201831817627,
      "learning_rate": 2.960407918416317e-05,
      "loss": 0.6987,
      "step": 6120
    },
    {
      "epoch": 1.225754849030194,
      "grad_norm": 7.725442409515381,
      "learning_rate": 2.9570752516163437e-05,
      "loss": 1.0189,
      "step": 6130
    },
    {
      "epoch": 1.227754449110178,
      "grad_norm": 7.407413005828857,
      "learning_rate": 2.9537425848163704e-05,
      "loss": 0.816,
      "step": 6140
    },
    {
      "epoch": 1.229754049190162,
      "grad_norm": 17.09162712097168,
      "learning_rate": 2.950409918016397e-05,
      "loss": 1.1845,
      "step": 6150
    },
    {
      "epoch": 1.231753649270146,
      "grad_norm": 5.858207702636719,
      "learning_rate": 2.9470772512164234e-05,
      "loss": 0.8728,
      "step": 6160
    },
    {
      "epoch": 1.23375324935013,
      "grad_norm": 21.666532516479492,
      "learning_rate": 2.94374458441645e-05,
      "loss": 0.9011,
      "step": 6170
    },
    {
      "epoch": 1.235752849430114,
      "grad_norm": 23.256362915039062,
      "learning_rate": 2.9404119176164768e-05,
      "loss": 0.8465,
      "step": 6180
    },
    {
      "epoch": 1.237752449510098,
      "grad_norm": 17.853059768676758,
      "learning_rate": 2.9370792508165035e-05,
      "loss": 0.9146,
      "step": 6190
    },
    {
      "epoch": 1.239752049590082,
      "grad_norm": 8.557899475097656,
      "learning_rate": 2.93374658401653e-05,
      "loss": 1.2533,
      "step": 6200
    },
    {
      "epoch": 1.241751649670066,
      "grad_norm": 11.35753059387207,
      "learning_rate": 2.930413917216557e-05,
      "loss": 1.0858,
      "step": 6210
    },
    {
      "epoch": 1.24375124975005,
      "grad_norm": 19.554758071899414,
      "learning_rate": 2.9270812504165835e-05,
      "loss": 0.916,
      "step": 6220
    },
    {
      "epoch": 1.245750849830034,
      "grad_norm": 9.69373607635498,
      "learning_rate": 2.9237485836166102e-05,
      "loss": 0.8962,
      "step": 6230
    },
    {
      "epoch": 1.247750449910018,
      "grad_norm": 18.72163200378418,
      "learning_rate": 2.9204159168166366e-05,
      "loss": 0.854,
      "step": 6240
    },
    {
      "epoch": 1.249750049990002,
      "grad_norm": 8.972244262695312,
      "learning_rate": 2.9170832500166633e-05,
      "loss": 0.6995,
      "step": 6250
    },
    {
      "epoch": 1.251749650069986,
      "grad_norm": 14.772855758666992,
      "learning_rate": 2.91375058321669e-05,
      "loss": 0.9528,
      "step": 6260
    },
    {
      "epoch": 1.25374925014997,
      "grad_norm": 4.612979888916016,
      "learning_rate": 2.9104179164167166e-05,
      "loss": 0.8021,
      "step": 6270
    },
    {
      "epoch": 1.255748850229954,
      "grad_norm": 10.883307456970215,
      "learning_rate": 2.9070852496167433e-05,
      "loss": 1.0495,
      "step": 6280
    },
    {
      "epoch": 1.257748450309938,
      "grad_norm": 8.313063621520996,
      "learning_rate": 2.90375258281677e-05,
      "loss": 0.8974,
      "step": 6290
    },
    {
      "epoch": 1.259748050389922,
      "grad_norm": 13.614985466003418,
      "learning_rate": 2.9004199160167967e-05,
      "loss": 0.7921,
      "step": 6300
    },
    {
      "epoch": 1.2617476504699061,
      "grad_norm": 8.801847457885742,
      "learning_rate": 2.8970872492168234e-05,
      "loss": 0.7225,
      "step": 6310
    },
    {
      "epoch": 1.2637472505498901,
      "grad_norm": 11.777344703674316,
      "learning_rate": 2.89375458241685e-05,
      "loss": 1.1624,
      "step": 6320
    },
    {
      "epoch": 1.2657468506298741,
      "grad_norm": 24.87760353088379,
      "learning_rate": 2.8904219156168764e-05,
      "loss": 1.2754,
      "step": 6330
    },
    {
      "epoch": 1.2677464507098581,
      "grad_norm": 8.853668212890625,
      "learning_rate": 2.887089248816903e-05,
      "loss": 0.8719,
      "step": 6340
    },
    {
      "epoch": 1.2697460507898422,
      "grad_norm": 13.559314727783203,
      "learning_rate": 2.8837565820169298e-05,
      "loss": 1.0044,
      "step": 6350
    },
    {
      "epoch": 1.2717456508698262,
      "grad_norm": 13.709931373596191,
      "learning_rate": 2.8804239152169565e-05,
      "loss": 0.9355,
      "step": 6360
    },
    {
      "epoch": 1.2737452509498102,
      "grad_norm": 9.021919250488281,
      "learning_rate": 2.8770912484169832e-05,
      "loss": 0.8504,
      "step": 6370
    },
    {
      "epoch": 1.275744851029794,
      "grad_norm": 12.384428024291992,
      "learning_rate": 2.87375858161701e-05,
      "loss": 1.1855,
      "step": 6380
    },
    {
      "epoch": 1.277744451109778,
      "grad_norm": 7.291859149932861,
      "learning_rate": 2.8704259148170366e-05,
      "loss": 0.9032,
      "step": 6390
    },
    {
      "epoch": 1.279744051189762,
      "grad_norm": 12.984981536865234,
      "learning_rate": 2.8670932480170633e-05,
      "loss": 0.9998,
      "step": 6400
    },
    {
      "epoch": 1.281743651269746,
      "grad_norm": 7.080036163330078,
      "learning_rate": 2.86376058121709e-05,
      "loss": 0.9465,
      "step": 6410
    },
    {
      "epoch": 1.28374325134973,
      "grad_norm": 11.403557777404785,
      "learning_rate": 2.8604279144171163e-05,
      "loss": 0.8742,
      "step": 6420
    },
    {
      "epoch": 1.285742851429714,
      "grad_norm": 10.634895324707031,
      "learning_rate": 2.8570952476171437e-05,
      "loss": 1.0217,
      "step": 6430
    },
    {
      "epoch": 1.287742451509698,
      "grad_norm": 14.626508712768555,
      "learning_rate": 2.8537625808171703e-05,
      "loss": 0.7267,
      "step": 6440
    },
    {
      "epoch": 1.289742051589682,
      "grad_norm": 15.354958534240723,
      "learning_rate": 2.850429914017197e-05,
      "loss": 0.9299,
      "step": 6450
    },
    {
      "epoch": 1.291741651669666,
      "grad_norm": 10.222518920898438,
      "learning_rate": 2.8470972472172237e-05,
      "loss": 1.0182,
      "step": 6460
    },
    {
      "epoch": 1.29374125174965,
      "grad_norm": 8.366691589355469,
      "learning_rate": 2.84376458041725e-05,
      "loss": 0.8228,
      "step": 6470
    },
    {
      "epoch": 1.295740851829634,
      "grad_norm": 14.903400421142578,
      "learning_rate": 2.8404319136172768e-05,
      "loss": 0.9604,
      "step": 6480
    },
    {
      "epoch": 1.297740451909618,
      "grad_norm": 15.889300346374512,
      "learning_rate": 2.8370992468173035e-05,
      "loss": 0.8788,
      "step": 6490
    },
    {
      "epoch": 1.299740051989602,
      "grad_norm": 12.399665832519531,
      "learning_rate": 2.83376658001733e-05,
      "loss": 1.0812,
      "step": 6500
    },
    {
      "epoch": 1.301739652069586,
      "grad_norm": 6.555566787719727,
      "learning_rate": 2.830433913217357e-05,
      "loss": 0.8717,
      "step": 6510
    },
    {
      "epoch": 1.30373925214957,
      "grad_norm": 9.936058044433594,
      "learning_rate": 2.8271012464173835e-05,
      "loss": 0.778,
      "step": 6520
    },
    {
      "epoch": 1.305738852229554,
      "grad_norm": 14.35396957397461,
      "learning_rate": 2.8237685796174102e-05,
      "loss": 1.0525,
      "step": 6530
    },
    {
      "epoch": 1.3077384523095381,
      "grad_norm": 23.216873168945312,
      "learning_rate": 2.820435912817437e-05,
      "loss": 1.0334,
      "step": 6540
    },
    {
      "epoch": 1.3097380523895221,
      "grad_norm": 13.739562034606934,
      "learning_rate": 2.8171032460174636e-05,
      "loss": 0.8104,
      "step": 6550
    },
    {
      "epoch": 1.3117376524695061,
      "grad_norm": 10.515190124511719,
      "learning_rate": 2.81377057921749e-05,
      "loss": 0.9828,
      "step": 6560
    },
    {
      "epoch": 1.3137372525494901,
      "grad_norm": 11.02917766571045,
      "learning_rate": 2.8104379124175166e-05,
      "loss": 0.8839,
      "step": 6570
    },
    {
      "epoch": 1.3157368526294742,
      "grad_norm": 11.638957977294922,
      "learning_rate": 2.8071052456175433e-05,
      "loss": 1.0147,
      "step": 6580
    },
    {
      "epoch": 1.3177364527094582,
      "grad_norm": 11.463875770568848,
      "learning_rate": 2.80377257881757e-05,
      "loss": 0.786,
      "step": 6590
    },
    {
      "epoch": 1.3197360527894422,
      "grad_norm": 14.522214889526367,
      "learning_rate": 2.8004399120175967e-05,
      "loss": 0.7976,
      "step": 6600
    },
    {
      "epoch": 1.3217356528694262,
      "grad_norm": 17.6164493560791,
      "learning_rate": 2.7971072452176234e-05,
      "loss": 1.1838,
      "step": 6610
    },
    {
      "epoch": 1.3237352529494102,
      "grad_norm": 13.60181713104248,
      "learning_rate": 2.79377457841765e-05,
      "loss": 1.0541,
      "step": 6620
    },
    {
      "epoch": 1.3257348530293942,
      "grad_norm": 8.429830551147461,
      "learning_rate": 2.7904419116176768e-05,
      "loss": 1.1562,
      "step": 6630
    },
    {
      "epoch": 1.327734453109378,
      "grad_norm": 17.81913948059082,
      "learning_rate": 2.7871092448177035e-05,
      "loss": 1.1951,
      "step": 6640
    },
    {
      "epoch": 1.329734053189362,
      "grad_norm": 9.632257461547852,
      "learning_rate": 2.7837765780177298e-05,
      "loss": 0.8749,
      "step": 6650
    },
    {
      "epoch": 1.331733653269346,
      "grad_norm": 10.270376205444336,
      "learning_rate": 2.7804439112177565e-05,
      "loss": 0.8676,
      "step": 6660
    },
    {
      "epoch": 1.33373325334933,
      "grad_norm": 10.948896408081055,
      "learning_rate": 2.7771112444177832e-05,
      "loss": 0.7519,
      "step": 6670
    },
    {
      "epoch": 1.335732853429314,
      "grad_norm": 5.359228134155273,
      "learning_rate": 2.77377857761781e-05,
      "loss": 0.8179,
      "step": 6680
    },
    {
      "epoch": 1.337732453509298,
      "grad_norm": 8.488261222839355,
      "learning_rate": 2.7704459108178366e-05,
      "loss": 0.9285,
      "step": 6690
    },
    {
      "epoch": 1.339732053589282,
      "grad_norm": 19.76957893371582,
      "learning_rate": 2.7671132440178632e-05,
      "loss": 1.2708,
      "step": 6700
    },
    {
      "epoch": 1.341731653669266,
      "grad_norm": 12.818047523498535,
      "learning_rate": 2.76378057721789e-05,
      "loss": 0.6396,
      "step": 6710
    },
    {
      "epoch": 1.34373125374925,
      "grad_norm": 17.417322158813477,
      "learning_rate": 2.7604479104179166e-05,
      "loss": 1.1852,
      "step": 6720
    },
    {
      "epoch": 1.345730853829234,
      "grad_norm": 9.09740924835205,
      "learning_rate": 2.7571152436179433e-05,
      "loss": 0.7137,
      "step": 6730
    },
    {
      "epoch": 1.347730453909218,
      "grad_norm": 19.589433670043945,
      "learning_rate": 2.7537825768179697e-05,
      "loss": 0.8319,
      "step": 6740
    },
    {
      "epoch": 1.349730053989202,
      "grad_norm": 15.73856258392334,
      "learning_rate": 2.7504499100179964e-05,
      "loss": 1.0773,
      "step": 6750
    },
    {
      "epoch": 1.351729654069186,
      "grad_norm": 11.582460403442383,
      "learning_rate": 2.747117243218023e-05,
      "loss": 0.9006,
      "step": 6760
    },
    {
      "epoch": 1.3537292541491701,
      "grad_norm": 20.336475372314453,
      "learning_rate": 2.7437845764180497e-05,
      "loss": 1.1132,
      "step": 6770
    },
    {
      "epoch": 1.3557288542291541,
      "grad_norm": 12.115336418151855,
      "learning_rate": 2.7404519096180764e-05,
      "loss": 1.1474,
      "step": 6780
    },
    {
      "epoch": 1.3577284543091381,
      "grad_norm": 11.624287605285645,
      "learning_rate": 2.737119242818103e-05,
      "loss": 0.9837,
      "step": 6790
    },
    {
      "epoch": 1.3597280543891221,
      "grad_norm": 6.931879043579102,
      "learning_rate": 2.7337865760181298e-05,
      "loss": 0.8315,
      "step": 6800
    },
    {
      "epoch": 1.3617276544691062,
      "grad_norm": 12.117997169494629,
      "learning_rate": 2.7304539092181565e-05,
      "loss": 0.9889,
      "step": 6810
    },
    {
      "epoch": 1.3637272545490902,
      "grad_norm": 5.22815465927124,
      "learning_rate": 2.7271212424181832e-05,
      "loss": 0.8471,
      "step": 6820
    },
    {
      "epoch": 1.3657268546290742,
      "grad_norm": 8.787679672241211,
      "learning_rate": 2.7237885756182095e-05,
      "loss": 0.9312,
      "step": 6830
    },
    {
      "epoch": 1.3677264547090582,
      "grad_norm": 6.934794902801514,
      "learning_rate": 2.7204559088182362e-05,
      "loss": 0.9513,
      "step": 6840
    },
    {
      "epoch": 1.3697260547890422,
      "grad_norm": 17.58083724975586,
      "learning_rate": 2.717123242018263e-05,
      "loss": 1.1248,
      "step": 6850
    },
    {
      "epoch": 1.3717256548690262,
      "grad_norm": 8.527792930603027,
      "learning_rate": 2.7137905752182896e-05,
      "loss": 0.9333,
      "step": 6860
    },
    {
      "epoch": 1.3737252549490102,
      "grad_norm": 10.242974281311035,
      "learning_rate": 2.7104579084183163e-05,
      "loss": 0.9426,
      "step": 6870
    },
    {
      "epoch": 1.3757248550289942,
      "grad_norm": 22.47539710998535,
      "learning_rate": 2.707125241618343e-05,
      "loss": 0.7765,
      "step": 6880
    },
    {
      "epoch": 1.3777244551089782,
      "grad_norm": 12.483972549438477,
      "learning_rate": 2.7037925748183697e-05,
      "loss": 1.1841,
      "step": 6890
    },
    {
      "epoch": 1.3797240551889622,
      "grad_norm": 11.942330360412598,
      "learning_rate": 2.7004599080183964e-05,
      "loss": 0.8036,
      "step": 6900
    },
    {
      "epoch": 1.3817236552689462,
      "grad_norm": 13.240253448486328,
      "learning_rate": 2.6971272412184227e-05,
      "loss": 0.924,
      "step": 6910
    },
    {
      "epoch": 1.3837232553489303,
      "grad_norm": 13.964170455932617,
      "learning_rate": 2.6937945744184494e-05,
      "loss": 0.7811,
      "step": 6920
    },
    {
      "epoch": 1.3857228554289143,
      "grad_norm": 8.87219524383545,
      "learning_rate": 2.690461907618476e-05,
      "loss": 0.8682,
      "step": 6930
    },
    {
      "epoch": 1.3877224555088983,
      "grad_norm": 12.940777778625488,
      "learning_rate": 2.6871292408185034e-05,
      "loss": 1.2073,
      "step": 6940
    },
    {
      "epoch": 1.3897220555888823,
      "grad_norm": 9.404668807983398,
      "learning_rate": 2.68379657401853e-05,
      "loss": 0.831,
      "step": 6950
    },
    {
      "epoch": 1.3917216556688663,
      "grad_norm": 6.532227516174316,
      "learning_rate": 2.6804639072185568e-05,
      "loss": 0.8743,
      "step": 6960
    },
    {
      "epoch": 1.3937212557488503,
      "grad_norm": 13.367127418518066,
      "learning_rate": 2.6771312404185832e-05,
      "loss": 1.0597,
      "step": 6970
    },
    {
      "epoch": 1.3957208558288343,
      "grad_norm": 7.954504489898682,
      "learning_rate": 2.67379857361861e-05,
      "loss": 1.1373,
      "step": 6980
    },
    {
      "epoch": 1.3977204559088183,
      "grad_norm": 8.636181831359863,
      "learning_rate": 2.6704659068186366e-05,
      "loss": 0.9004,
      "step": 6990
    },
    {
      "epoch": 1.3997200559888023,
      "grad_norm": 12.443596839904785,
      "learning_rate": 2.6671332400186632e-05,
      "loss": 0.8751,
      "step": 7000
    },
    {
      "epoch": 1.4017196560687863,
      "grad_norm": 13.689605712890625,
      "learning_rate": 2.66380057321869e-05,
      "loss": 0.9799,
      "step": 7010
    },
    {
      "epoch": 1.4037192561487704,
      "grad_norm": 15.823539733886719,
      "learning_rate": 2.6604679064187166e-05,
      "loss": 1.1044,
      "step": 7020
    },
    {
      "epoch": 1.4057188562287544,
      "grad_norm": 15.663901329040527,
      "learning_rate": 2.6571352396187433e-05,
      "loss": 0.9731,
      "step": 7030
    },
    {
      "epoch": 1.4077184563087384,
      "grad_norm": 6.513339996337891,
      "learning_rate": 2.65380257281877e-05,
      "loss": 0.804,
      "step": 7040
    },
    {
      "epoch": 1.4097180563887224,
      "grad_norm": 14.790840148925781,
      "learning_rate": 2.6504699060187967e-05,
      "loss": 0.9067,
      "step": 7050
    },
    {
      "epoch": 1.4117176564687064,
      "grad_norm": 20.572933197021484,
      "learning_rate": 2.647137239218823e-05,
      "loss": 0.9788,
      "step": 7060
    },
    {
      "epoch": 1.4137172565486902,
      "grad_norm": 20.656469345092773,
      "learning_rate": 2.6438045724188497e-05,
      "loss": 1.1067,
      "step": 7070
    },
    {
      "epoch": 1.4157168566286742,
      "grad_norm": 6.411489009857178,
      "learning_rate": 2.6404719056188764e-05,
      "loss": 0.9607,
      "step": 7080
    },
    {
      "epoch": 1.4177164567086582,
      "grad_norm": 14.918583869934082,
      "learning_rate": 2.637139238818903e-05,
      "loss": 0.9686,
      "step": 7090
    },
    {
      "epoch": 1.4197160567886422,
      "grad_norm": 9.188017845153809,
      "learning_rate": 2.6338065720189298e-05,
      "loss": 0.7416,
      "step": 7100
    },
    {
      "epoch": 1.4217156568686262,
      "grad_norm": 18.110563278198242,
      "learning_rate": 2.6304739052189565e-05,
      "loss": 0.8623,
      "step": 7110
    },
    {
      "epoch": 1.4237152569486102,
      "grad_norm": 18.458988189697266,
      "learning_rate": 2.6271412384189832e-05,
      "loss": 0.8589,
      "step": 7120
    },
    {
      "epoch": 1.4257148570285942,
      "grad_norm": 12.38065242767334,
      "learning_rate": 2.62380857161901e-05,
      "loss": 0.9043,
      "step": 7130
    },
    {
      "epoch": 1.4277144571085782,
      "grad_norm": 9.07278823852539,
      "learning_rate": 2.6204759048190362e-05,
      "loss": 0.9625,
      "step": 7140
    },
    {
      "epoch": 1.4297140571885623,
      "grad_norm": 15.144899368286133,
      "learning_rate": 2.617143238019063e-05,
      "loss": 1.0104,
      "step": 7150
    },
    {
      "epoch": 1.4317136572685463,
      "grad_norm": 14.707409858703613,
      "learning_rate": 2.6138105712190896e-05,
      "loss": 1.0069,
      "step": 7160
    },
    {
      "epoch": 1.4337132573485303,
      "grad_norm": 8.884513854980469,
      "learning_rate": 2.6104779044191163e-05,
      "loss": 1.0113,
      "step": 7170
    },
    {
      "epoch": 1.4357128574285143,
      "grad_norm": 11.681099891662598,
      "learning_rate": 2.607145237619143e-05,
      "loss": 1.0447,
      "step": 7180
    },
    {
      "epoch": 1.4377124575084983,
      "grad_norm": 13.35605239868164,
      "learning_rate": 2.6038125708191697e-05,
      "loss": 0.975,
      "step": 7190
    },
    {
      "epoch": 1.4397120575884823,
      "grad_norm": 9.162775993347168,
      "learning_rate": 2.6004799040191963e-05,
      "loss": 0.7162,
      "step": 7200
    },
    {
      "epoch": 1.4417116576684663,
      "grad_norm": 7.408576488494873,
      "learning_rate": 2.597147237219223e-05,
      "loss": 0.6877,
      "step": 7210
    },
    {
      "epoch": 1.4437112577484503,
      "grad_norm": 13.404364585876465,
      "learning_rate": 2.5938145704192497e-05,
      "loss": 0.873,
      "step": 7220
    },
    {
      "epoch": 1.4457108578284343,
      "grad_norm": 22.734363555908203,
      "learning_rate": 2.590481903619276e-05,
      "loss": 1.05,
      "step": 7230
    },
    {
      "epoch": 1.4477104579084183,
      "grad_norm": 21.75366973876953,
      "learning_rate": 2.5871492368193028e-05,
      "loss": 0.7489,
      "step": 7240
    },
    {
      "epoch": 1.4497100579884024,
      "grad_norm": 11.867043495178223,
      "learning_rate": 2.5838165700193295e-05,
      "loss": 1.085,
      "step": 7250
    },
    {
      "epoch": 1.4517096580683864,
      "grad_norm": 10.844705581665039,
      "learning_rate": 2.580483903219356e-05,
      "loss": 1.0174,
      "step": 7260
    },
    {
      "epoch": 1.4537092581483704,
      "grad_norm": 11.816149711608887,
      "learning_rate": 2.5771512364193828e-05,
      "loss": 1.2057,
      "step": 7270
    },
    {
      "epoch": 1.4557088582283544,
      "grad_norm": 14.048501014709473,
      "learning_rate": 2.5738185696194095e-05,
      "loss": 0.7638,
      "step": 7280
    },
    {
      "epoch": 1.4577084583083384,
      "grad_norm": 11.649151802062988,
      "learning_rate": 2.5704859028194362e-05,
      "loss": 0.6783,
      "step": 7290
    },
    {
      "epoch": 1.4597080583883224,
      "grad_norm": 6.235522270202637,
      "learning_rate": 2.567153236019463e-05,
      "loss": 0.9782,
      "step": 7300
    },
    {
      "epoch": 1.4617076584683064,
      "grad_norm": 9.081123352050781,
      "learning_rate": 2.5638205692194896e-05,
      "loss": 1.0608,
      "step": 7310
    },
    {
      "epoch": 1.4637072585482904,
      "grad_norm": 5.738706588745117,
      "learning_rate": 2.560487902419516e-05,
      "loss": 0.8616,
      "step": 7320
    },
    {
      "epoch": 1.4657068586282742,
      "grad_norm": 13.131057739257812,
      "learning_rate": 2.5571552356195426e-05,
      "loss": 1.2056,
      "step": 7330
    },
    {
      "epoch": 1.4677064587082582,
      "grad_norm": 7.340749740600586,
      "learning_rate": 2.5538225688195693e-05,
      "loss": 0.9675,
      "step": 7340
    },
    {
      "epoch": 1.4697060587882422,
      "grad_norm": 6.818540573120117,
      "learning_rate": 2.550489902019596e-05,
      "loss": 0.8906,
      "step": 7350
    },
    {
      "epoch": 1.4717056588682262,
      "grad_norm": 17.247026443481445,
      "learning_rate": 2.5471572352196227e-05,
      "loss": 1.0204,
      "step": 7360
    },
    {
      "epoch": 1.4737052589482103,
      "grad_norm": 18.791730880737305,
      "learning_rate": 2.5438245684196494e-05,
      "loss": 0.7862,
      "step": 7370
    },
    {
      "epoch": 1.4757048590281943,
      "grad_norm": 7.952294826507568,
      "learning_rate": 2.540491901619676e-05,
      "loss": 0.7909,
      "step": 7380
    },
    {
      "epoch": 1.4777044591081783,
      "grad_norm": 14.551902770996094,
      "learning_rate": 2.5371592348197028e-05,
      "loss": 1.0117,
      "step": 7390
    },
    {
      "epoch": 1.4797040591881623,
      "grad_norm": 11.587174415588379,
      "learning_rate": 2.5338265680197294e-05,
      "loss": 0.8765,
      "step": 7400
    },
    {
      "epoch": 1.4817036592681463,
      "grad_norm": 12.649856567382812,
      "learning_rate": 2.5304939012197558e-05,
      "loss": 1.0564,
      "step": 7410
    },
    {
      "epoch": 1.4837032593481303,
      "grad_norm": 12.81641674041748,
      "learning_rate": 2.5271612344197825e-05,
      "loss": 0.7648,
      "step": 7420
    },
    {
      "epoch": 1.4857028594281143,
      "grad_norm": 13.240670204162598,
      "learning_rate": 2.5238285676198092e-05,
      "loss": 0.9227,
      "step": 7430
    },
    {
      "epoch": 1.4877024595080983,
      "grad_norm": 11.492237091064453,
      "learning_rate": 2.520495900819836e-05,
      "loss": 1.1133,
      "step": 7440
    },
    {
      "epoch": 1.4897020595880823,
      "grad_norm": 7.153125762939453,
      "learning_rate": 2.5171632340198626e-05,
      "loss": 0.6128,
      "step": 7450
    },
    {
      "epoch": 1.4917016596680663,
      "grad_norm": 17.558624267578125,
      "learning_rate": 2.5138305672198896e-05,
      "loss": 0.9597,
      "step": 7460
    },
    {
      "epoch": 1.4937012597480503,
      "grad_norm": 9.896039009094238,
      "learning_rate": 2.5104979004199163e-05,
      "loss": 0.9112,
      "step": 7470
    },
    {
      "epoch": 1.4957008598280344,
      "grad_norm": 8.17568588256836,
      "learning_rate": 2.507165233619943e-05,
      "loss": 1.0399,
      "step": 7480
    },
    {
      "epoch": 1.4977004599080184,
      "grad_norm": 22.580636978149414,
      "learning_rate": 2.5038325668199696e-05,
      "loss": 0.9348,
      "step": 7490
    },
    {
      "epoch": 1.4997000599880024,
      "grad_norm": 10.156831741333008,
      "learning_rate": 2.5004999000199963e-05,
      "loss": 0.8761,
      "step": 7500
    },
    {
      "epoch": 1.5016996600679864,
      "grad_norm": 14.45241928100586,
      "learning_rate": 2.4971672332200227e-05,
      "loss": 0.9312,
      "step": 7510
    },
    {
      "epoch": 1.5036992601479704,
      "grad_norm": 9.735308647155762,
      "learning_rate": 2.4938345664200494e-05,
      "loss": 0.8087,
      "step": 7520
    },
    {
      "epoch": 1.5056988602279544,
      "grad_norm": 18.337432861328125,
      "learning_rate": 2.490501899620076e-05,
      "loss": 0.9242,
      "step": 7530
    },
    {
      "epoch": 1.5076984603079384,
      "grad_norm": 9.010652542114258,
      "learning_rate": 2.4871692328201028e-05,
      "loss": 1.1598,
      "step": 7540
    },
    {
      "epoch": 1.5096980603879224,
      "grad_norm": 33.32761764526367,
      "learning_rate": 2.4838365660201294e-05,
      "loss": 0.8301,
      "step": 7550
    },
    {
      "epoch": 1.5116976604679064,
      "grad_norm": 13.128631591796875,
      "learning_rate": 2.480503899220156e-05,
      "loss": 0.7415,
      "step": 7560
    },
    {
      "epoch": 1.5136972605478904,
      "grad_norm": 20.426292419433594,
      "learning_rate": 2.4771712324201825e-05,
      "loss": 0.7812,
      "step": 7570
    },
    {
      "epoch": 1.5156968606278745,
      "grad_norm": 13.392491340637207,
      "learning_rate": 2.4738385656202095e-05,
      "loss": 1.1417,
      "step": 7580
    },
    {
      "epoch": 1.5176964607078585,
      "grad_norm": 16.634199142456055,
      "learning_rate": 2.4705058988202362e-05,
      "loss": 0.9964,
      "step": 7590
    },
    {
      "epoch": 1.5196960607878425,
      "grad_norm": 12.163681983947754,
      "learning_rate": 2.467173232020263e-05,
      "loss": 0.6733,
      "step": 7600
    },
    {
      "epoch": 1.5216956608678265,
      "grad_norm": 9.320735931396484,
      "learning_rate": 2.4638405652202896e-05,
      "loss": 0.9446,
      "step": 7610
    },
    {
      "epoch": 1.5236952609478105,
      "grad_norm": 10.349356651306152,
      "learning_rate": 2.4605078984203163e-05,
      "loss": 0.9627,
      "step": 7620
    },
    {
      "epoch": 1.5256948610277945,
      "grad_norm": 10.908788681030273,
      "learning_rate": 2.457175231620343e-05,
      "loss": 1.0342,
      "step": 7630
    },
    {
      "epoch": 1.5276944611077785,
      "grad_norm": 20.068336486816406,
      "learning_rate": 2.4538425648203693e-05,
      "loss": 1.0513,
      "step": 7640
    },
    {
      "epoch": 1.5296940611877625,
      "grad_norm": 10.737165451049805,
      "learning_rate": 2.450509898020396e-05,
      "loss": 0.9646,
      "step": 7650
    },
    {
      "epoch": 1.5316936612677465,
      "grad_norm": 13.725641250610352,
      "learning_rate": 2.4471772312204227e-05,
      "loss": 0.898,
      "step": 7660
    },
    {
      "epoch": 1.5336932613477305,
      "grad_norm": 12.5100679397583,
      "learning_rate": 2.4438445644204494e-05,
      "loss": 0.8647,
      "step": 7670
    },
    {
      "epoch": 1.5356928614277146,
      "grad_norm": 10.116689682006836,
      "learning_rate": 2.440511897620476e-05,
      "loss": 1.0739,
      "step": 7680
    },
    {
      "epoch": 1.5376924615076986,
      "grad_norm": 18.53221893310547,
      "learning_rate": 2.4371792308205028e-05,
      "loss": 1.0724,
      "step": 7690
    },
    {
      "epoch": 1.5396920615876826,
      "grad_norm": 12.735157012939453,
      "learning_rate": 2.4338465640205294e-05,
      "loss": 1.0413,
      "step": 7700
    },
    {
      "epoch": 1.5416916616676666,
      "grad_norm": 11.698009490966797,
      "learning_rate": 2.430513897220556e-05,
      "loss": 0.874,
      "step": 7710
    },
    {
      "epoch": 1.5436912617476506,
      "grad_norm": 14.04918384552002,
      "learning_rate": 2.4271812304205828e-05,
      "loss": 0.8528,
      "step": 7720
    },
    {
      "epoch": 1.5456908618276346,
      "grad_norm": 7.109793663024902,
      "learning_rate": 2.423848563620609e-05,
      "loss": 0.7201,
      "step": 7730
    },
    {
      "epoch": 1.5476904619076186,
      "grad_norm": 6.678958892822266,
      "learning_rate": 2.420515896820636e-05,
      "loss": 0.968,
      "step": 7740
    },
    {
      "epoch": 1.5496900619876026,
      "grad_norm": 14.670111656188965,
      "learning_rate": 2.4171832300206625e-05,
      "loss": 0.901,
      "step": 7750
    },
    {
      "epoch": 1.5516896620675866,
      "grad_norm": 3.9040656089782715,
      "learning_rate": 2.4138505632206892e-05,
      "loss": 0.9316,
      "step": 7760
    },
    {
      "epoch": 1.5536892621475706,
      "grad_norm": 19.196956634521484,
      "learning_rate": 2.410517896420716e-05,
      "loss": 1.1955,
      "step": 7770
    },
    {
      "epoch": 1.5556888622275546,
      "grad_norm": 18.603960037231445,
      "learning_rate": 2.4071852296207426e-05,
      "loss": 0.9198,
      "step": 7780
    },
    {
      "epoch": 1.5576884623075387,
      "grad_norm": 15.166921615600586,
      "learning_rate": 2.4038525628207693e-05,
      "loss": 0.8943,
      "step": 7790
    },
    {
      "epoch": 1.5596880623875224,
      "grad_norm": 17.24521827697754,
      "learning_rate": 2.400519896020796e-05,
      "loss": 0.9201,
      "step": 7800
    },
    {
      "epoch": 1.5616876624675065,
      "grad_norm": 7.711451053619385,
      "learning_rate": 2.3971872292208223e-05,
      "loss": 0.7306,
      "step": 7810
    },
    {
      "epoch": 1.5636872625474905,
      "grad_norm": 9.45329475402832,
      "learning_rate": 2.393854562420849e-05,
      "loss": 0.5992,
      "step": 7820
    },
    {
      "epoch": 1.5656868626274745,
      "grad_norm": 10.916348457336426,
      "learning_rate": 2.3905218956208757e-05,
      "loss": 0.9465,
      "step": 7830
    },
    {
      "epoch": 1.5676864627074585,
      "grad_norm": 25.377071380615234,
      "learning_rate": 2.3871892288209027e-05,
      "loss": 0.895,
      "step": 7840
    },
    {
      "epoch": 1.5696860627874425,
      "grad_norm": 27.933053970336914,
      "learning_rate": 2.3838565620209294e-05,
      "loss": 0.9828,
      "step": 7850
    },
    {
      "epoch": 1.5716856628674265,
      "grad_norm": 10.246896743774414,
      "learning_rate": 2.380523895220956e-05,
      "loss": 0.8133,
      "step": 7860
    },
    {
      "epoch": 1.5736852629474105,
      "grad_norm": 13.863579750061035,
      "learning_rate": 2.3771912284209828e-05,
      "loss": 0.9457,
      "step": 7870
    },
    {
      "epoch": 1.5756848630273945,
      "grad_norm": 8.989386558532715,
      "learning_rate": 2.3738585616210095e-05,
      "loss": 0.8622,
      "step": 7880
    },
    {
      "epoch": 1.5776844631073785,
      "grad_norm": 13.264229774475098,
      "learning_rate": 2.370525894821036e-05,
      "loss": 1.1234,
      "step": 7890
    },
    {
      "epoch": 1.5796840631873625,
      "grad_norm": 10.056224822998047,
      "learning_rate": 2.3671932280210625e-05,
      "loss": 0.9853,
      "step": 7900
    },
    {
      "epoch": 1.5816836632673466,
      "grad_norm": 12.802730560302734,
      "learning_rate": 2.3638605612210892e-05,
      "loss": 0.8224,
      "step": 7910
    },
    {
      "epoch": 1.5836832633473306,
      "grad_norm": 16.58254623413086,
      "learning_rate": 2.360527894421116e-05,
      "loss": 0.9976,
      "step": 7920
    },
    {
      "epoch": 1.5856828634273146,
      "grad_norm": 11.844390869140625,
      "learning_rate": 2.3571952276211426e-05,
      "loss": 0.888,
      "step": 7930
    },
    {
      "epoch": 1.5876824635072986,
      "grad_norm": 5.788026332855225,
      "learning_rate": 2.3538625608211693e-05,
      "loss": 0.6935,
      "step": 7940
    },
    {
      "epoch": 1.5896820635872826,
      "grad_norm": 9.781177520751953,
      "learning_rate": 2.350529894021196e-05,
      "loss": 0.6938,
      "step": 7950
    },
    {
      "epoch": 1.5916816636672666,
      "grad_norm": 21.769424438476562,
      "learning_rate": 2.3471972272212227e-05,
      "loss": 1.0358,
      "step": 7960
    },
    {
      "epoch": 1.5936812637472504,
      "grad_norm": 11.132895469665527,
      "learning_rate": 2.343864560421249e-05,
      "loss": 0.8865,
      "step": 7970
    },
    {
      "epoch": 1.5956808638272344,
      "grad_norm": 27.089752197265625,
      "learning_rate": 2.3405318936212757e-05,
      "loss": 0.8815,
      "step": 7980
    },
    {
      "epoch": 1.5976804639072184,
      "grad_norm": 8.712761878967285,
      "learning_rate": 2.3371992268213024e-05,
      "loss": 1.0101,
      "step": 7990
    },
    {
      "epoch": 1.5996800639872024,
      "grad_norm": 5.68848180770874,
      "learning_rate": 2.333866560021329e-05,
      "loss": 0.9092,
      "step": 8000
    },
    {
      "epoch": 1.6016796640671864,
      "grad_norm": 6.3105950355529785,
      "learning_rate": 2.3305338932213558e-05,
      "loss": 0.8517,
      "step": 8010
    },
    {
      "epoch": 1.6036792641471704,
      "grad_norm": 2.6986207962036133,
      "learning_rate": 2.3272012264213825e-05,
      "loss": 1.0083,
      "step": 8020
    },
    {
      "epoch": 1.6056788642271544,
      "grad_norm": 19.223224639892578,
      "learning_rate": 2.323868559621409e-05,
      "loss": 0.8173,
      "step": 8030
    },
    {
      "epoch": 1.6076784643071385,
      "grad_norm": 28.298616409301758,
      "learning_rate": 2.320535892821436e-05,
      "loss": 0.7637,
      "step": 8040
    },
    {
      "epoch": 1.6096780643871225,
      "grad_norm": 18.474109649658203,
      "learning_rate": 2.3172032260214625e-05,
      "loss": 0.9525,
      "step": 8050
    },
    {
      "epoch": 1.6116776644671065,
      "grad_norm": 10.524460792541504,
      "learning_rate": 2.313870559221489e-05,
      "loss": 1.0375,
      "step": 8060
    },
    {
      "epoch": 1.6136772645470905,
      "grad_norm": 10.27405071258545,
      "learning_rate": 2.3105378924215156e-05,
      "loss": 0.9828,
      "step": 8070
    },
    {
      "epoch": 1.6156768646270745,
      "grad_norm": 6.650126934051514,
      "learning_rate": 2.3072052256215423e-05,
      "loss": 0.7584,
      "step": 8080
    },
    {
      "epoch": 1.6176764647070585,
      "grad_norm": 15.64673900604248,
      "learning_rate": 2.303872558821569e-05,
      "loss": 1.0182,
      "step": 8090
    },
    {
      "epoch": 1.6196760647870425,
      "grad_norm": 20.677635192871094,
      "learning_rate": 2.300539892021596e-05,
      "loss": 1.0128,
      "step": 8100
    },
    {
      "epoch": 1.6216756648670265,
      "grad_norm": 16.926610946655273,
      "learning_rate": 2.2972072252216227e-05,
      "loss": 1.1034,
      "step": 8110
    },
    {
      "epoch": 1.6236752649470105,
      "grad_norm": 14.85013198852539,
      "learning_rate": 2.2938745584216494e-05,
      "loss": 0.7246,
      "step": 8120
    },
    {
      "epoch": 1.6256748650269945,
      "grad_norm": 7.353440761566162,
      "learning_rate": 2.2905418916216757e-05,
      "loss": 0.7547,
      "step": 8130
    },
    {
      "epoch": 1.6276744651069786,
      "grad_norm": 8.178986549377441,
      "learning_rate": 2.2872092248217024e-05,
      "loss": 0.8136,
      "step": 8140
    },
    {
      "epoch": 1.6296740651869626,
      "grad_norm": 14.500388145446777,
      "learning_rate": 2.283876558021729e-05,
      "loss": 0.7895,
      "step": 8150
    },
    {
      "epoch": 1.6316736652669466,
      "grad_norm": 15.035855293273926,
      "learning_rate": 2.2805438912217558e-05,
      "loss": 1.2386,
      "step": 8160
    },
    {
      "epoch": 1.6336732653469306,
      "grad_norm": 21.421194076538086,
      "learning_rate": 2.2772112244217825e-05,
      "loss": 1.1336,
      "step": 8170
    },
    {
      "epoch": 1.6356728654269146,
      "grad_norm": 9.28275203704834,
      "learning_rate": 2.273878557621809e-05,
      "loss": 0.858,
      "step": 8180
    },
    {
      "epoch": 1.6376724655068986,
      "grad_norm": 12.54520320892334,
      "learning_rate": 2.270545890821836e-05,
      "loss": 0.8215,
      "step": 8190
    },
    {
      "epoch": 1.6396720655868826,
      "grad_norm": 16.689228057861328,
      "learning_rate": 2.2672132240218625e-05,
      "loss": 0.8317,
      "step": 8200
    },
    {
      "epoch": 1.6416716656668666,
      "grad_norm": 8.203595161437988,
      "learning_rate": 2.2638805572218892e-05,
      "loss": 1.0769,
      "step": 8210
    },
    {
      "epoch": 1.6436712657468506,
      "grad_norm": 7.193220615386963,
      "learning_rate": 2.2605478904219156e-05,
      "loss": 1.1434,
      "step": 8220
    },
    {
      "epoch": 1.6456708658268346,
      "grad_norm": 5.279370307922363,
      "learning_rate": 2.2572152236219423e-05,
      "loss": 1.0614,
      "step": 8230
    },
    {
      "epoch": 1.6476704659068186,
      "grad_norm": 8.350167274475098,
      "learning_rate": 2.253882556821969e-05,
      "loss": 0.963,
      "step": 8240
    },
    {
      "epoch": 1.6496700659868027,
      "grad_norm": 4.027693748474121,
      "learning_rate": 2.2505498900219956e-05,
      "loss": 0.9206,
      "step": 8250
    },
    {
      "epoch": 1.6516696660667867,
      "grad_norm": 23.948497772216797,
      "learning_rate": 2.2472172232220223e-05,
      "loss": 0.8297,
      "step": 8260
    },
    {
      "epoch": 1.6536692661467707,
      "grad_norm": 19.319717407226562,
      "learning_rate": 2.243884556422049e-05,
      "loss": 0.8948,
      "step": 8270
    },
    {
      "epoch": 1.6556688662267547,
      "grad_norm": 22.865127563476562,
      "learning_rate": 2.2405518896220757e-05,
      "loss": 0.9914,
      "step": 8280
    },
    {
      "epoch": 1.6576684663067387,
      "grad_norm": 10.844398498535156,
      "learning_rate": 2.2372192228221024e-05,
      "loss": 0.8465,
      "step": 8290
    },
    {
      "epoch": 1.6596680663867227,
      "grad_norm": 8.119958877563477,
      "learning_rate": 2.233886556022129e-05,
      "loss": 0.6153,
      "step": 8300
    },
    {
      "epoch": 1.6616676664667067,
      "grad_norm": 10.448104858398438,
      "learning_rate": 2.2305538892221554e-05,
      "loss": 0.9316,
      "step": 8310
    },
    {
      "epoch": 1.6636672665466907,
      "grad_norm": 10.083983421325684,
      "learning_rate": 2.227221222422182e-05,
      "loss": 0.9181,
      "step": 8320
    },
    {
      "epoch": 1.6656668666266747,
      "grad_norm": 6.744692802429199,
      "learning_rate": 2.2238885556222088e-05,
      "loss": 0.9771,
      "step": 8330
    },
    {
      "epoch": 1.6676664667066587,
      "grad_norm": 10.539505958557129,
      "learning_rate": 2.2205558888222355e-05,
      "loss": 1.0089,
      "step": 8340
    },
    {
      "epoch": 1.6696660667866428,
      "grad_norm": 15.05086612701416,
      "learning_rate": 2.2172232220222625e-05,
      "loss": 0.822,
      "step": 8350
    },
    {
      "epoch": 1.6716656668666268,
      "grad_norm": 12.63435173034668,
      "learning_rate": 2.2138905552222892e-05,
      "loss": 1.0752,
      "step": 8360
    },
    {
      "epoch": 1.6736652669466108,
      "grad_norm": 15.257124900817871,
      "learning_rate": 2.210557888422316e-05,
      "loss": 0.8581,
      "step": 8370
    },
    {
      "epoch": 1.6756648670265948,
      "grad_norm": 7.550647258758545,
      "learning_rate": 2.2072252216223423e-05,
      "loss": 0.8571,
      "step": 8380
    },
    {
      "epoch": 1.6776644671065788,
      "grad_norm": 12.365416526794434,
      "learning_rate": 2.203892554822369e-05,
      "loss": 0.6975,
      "step": 8390
    },
    {
      "epoch": 1.6796640671865628,
      "grad_norm": 10.107141494750977,
      "learning_rate": 2.2005598880223956e-05,
      "loss": 0.9625,
      "step": 8400
    },
    {
      "epoch": 1.6816636672665468,
      "grad_norm": 6.639669895172119,
      "learning_rate": 2.1972272212224223e-05,
      "loss": 0.8825,
      "step": 8410
    },
    {
      "epoch": 1.6836632673465308,
      "grad_norm": 12.808358192443848,
      "learning_rate": 2.193894554422449e-05,
      "loss": 0.8331,
      "step": 8420
    },
    {
      "epoch": 1.6856628674265148,
      "grad_norm": 9.29654312133789,
      "learning_rate": 2.1905618876224757e-05,
      "loss": 0.8141,
      "step": 8430
    },
    {
      "epoch": 1.6876624675064988,
      "grad_norm": 11.545607566833496,
      "learning_rate": 2.1872292208225024e-05,
      "loss": 0.8114,
      "step": 8440
    },
    {
      "epoch": 1.6896620675864829,
      "grad_norm": 10.409873962402344,
      "learning_rate": 2.183896554022529e-05,
      "loss": 0.7724,
      "step": 8450
    },
    {
      "epoch": 1.6916616676664669,
      "grad_norm": 3.2900426387786865,
      "learning_rate": 2.1805638872225558e-05,
      "loss": 0.7539,
      "step": 8460
    },
    {
      "epoch": 1.6936612677464509,
      "grad_norm": 13.953104972839355,
      "learning_rate": 2.177231220422582e-05,
      "loss": 1.1126,
      "step": 8470
    },
    {
      "epoch": 1.6956608678264347,
      "grad_norm": 12.057395935058594,
      "learning_rate": 2.1738985536226088e-05,
      "loss": 0.7607,
      "step": 8480
    },
    {
      "epoch": 1.6976604679064187,
      "grad_norm": 13.12625789642334,
      "learning_rate": 2.1705658868226355e-05,
      "loss": 1.0034,
      "step": 8490
    },
    {
      "epoch": 1.6996600679864027,
      "grad_norm": 10.311214447021484,
      "learning_rate": 2.1672332200226622e-05,
      "loss": 0.8478,
      "step": 8500
    },
    {
      "epoch": 1.7016596680663867,
      "grad_norm": 17.286720275878906,
      "learning_rate": 2.163900553222689e-05,
      "loss": 0.7595,
      "step": 8510
    },
    {
      "epoch": 1.7036592681463707,
      "grad_norm": 15.64364242553711,
      "learning_rate": 2.1605678864227156e-05,
      "loss": 0.9551,
      "step": 8520
    },
    {
      "epoch": 1.7056588682263547,
      "grad_norm": 19.75090789794922,
      "learning_rate": 2.1572352196227423e-05,
      "loss": 1.0511,
      "step": 8530
    },
    {
      "epoch": 1.7076584683063387,
      "grad_norm": 5.147335529327393,
      "learning_rate": 2.153902552822769e-05,
      "loss": 0.8334,
      "step": 8540
    },
    {
      "epoch": 1.7096580683863227,
      "grad_norm": 7.920611381530762,
      "learning_rate": 2.1505698860227956e-05,
      "loss": 1.0475,
      "step": 8550
    },
    {
      "epoch": 1.7116576684663067,
      "grad_norm": 10.065372467041016,
      "learning_rate": 2.147237219222822e-05,
      "loss": 0.9338,
      "step": 8560
    },
    {
      "epoch": 1.7136572685462907,
      "grad_norm": 10.696304321289062,
      "learning_rate": 2.1439045524228487e-05,
      "loss": 0.844,
      "step": 8570
    },
    {
      "epoch": 1.7156568686262748,
      "grad_norm": 13.630880355834961,
      "learning_rate": 2.1405718856228754e-05,
      "loss": 1.0843,
      "step": 8580
    },
    {
      "epoch": 1.7176564687062588,
      "grad_norm": 10.213717460632324,
      "learning_rate": 2.137239218822902e-05,
      "loss": 0.7853,
      "step": 8590
    },
    {
      "epoch": 1.7196560687862428,
      "grad_norm": 10.085070610046387,
      "learning_rate": 2.1339065520229287e-05,
      "loss": 0.6855,
      "step": 8600
    },
    {
      "epoch": 1.7216556688662268,
      "grad_norm": 12.72898006439209,
      "learning_rate": 2.1305738852229558e-05,
      "loss": 0.9861,
      "step": 8610
    },
    {
      "epoch": 1.7236552689462108,
      "grad_norm": 16.236072540283203,
      "learning_rate": 2.1272412184229825e-05,
      "loss": 1.0747,
      "step": 8620
    },
    {
      "epoch": 1.7256548690261948,
      "grad_norm": 12.683197021484375,
      "learning_rate": 2.1239085516230088e-05,
      "loss": 0.8788,
      "step": 8630
    },
    {
      "epoch": 1.7276544691061788,
      "grad_norm": 12.480019569396973,
      "learning_rate": 2.1205758848230355e-05,
      "loss": 0.8819,
      "step": 8640
    },
    {
      "epoch": 1.7296540691861626,
      "grad_norm": 14.541876792907715,
      "learning_rate": 2.1172432180230622e-05,
      "loss": 0.8669,
      "step": 8650
    },
    {
      "epoch": 1.7316536692661466,
      "grad_norm": 3.758991003036499,
      "learning_rate": 2.113910551223089e-05,
      "loss": 0.8818,
      "step": 8660
    },
    {
      "epoch": 1.7336532693461306,
      "grad_norm": 12.65374755859375,
      "learning_rate": 2.1105778844231156e-05,
      "loss": 1.065,
      "step": 8670
    },
    {
      "epoch": 1.7356528694261146,
      "grad_norm": 11.451781272888184,
      "learning_rate": 2.1072452176231423e-05,
      "loss": 1.1107,
      "step": 8680
    },
    {
      "epoch": 1.7376524695060986,
      "grad_norm": 13.226959228515625,
      "learning_rate": 2.103912550823169e-05,
      "loss": 1.1821,
      "step": 8690
    },
    {
      "epoch": 1.7396520695860826,
      "grad_norm": 19.20694923400879,
      "learning_rate": 2.1005798840231956e-05,
      "loss": 0.8511,
      "step": 8700
    },
    {
      "epoch": 1.7416516696660667,
      "grad_norm": 15.77137565612793,
      "learning_rate": 2.097247217223222e-05,
      "loss": 0.9531,
      "step": 8710
    },
    {
      "epoch": 1.7436512697460507,
      "grad_norm": 17.072975158691406,
      "learning_rate": 2.0939145504232487e-05,
      "loss": 0.8385,
      "step": 8720
    },
    {
      "epoch": 1.7456508698260347,
      "grad_norm": 10.083065032958984,
      "learning_rate": 2.0905818836232754e-05,
      "loss": 1.0256,
      "step": 8730
    },
    {
      "epoch": 1.7476504699060187,
      "grad_norm": 9.85015869140625,
      "learning_rate": 2.087249216823302e-05,
      "loss": 1.0439,
      "step": 8740
    },
    {
      "epoch": 1.7496500699860027,
      "grad_norm": 16.12037467956543,
      "learning_rate": 2.0839165500233287e-05,
      "loss": 1.0087,
      "step": 8750
    },
    {
      "epoch": 1.7516496700659867,
      "grad_norm": 10.927610397338867,
      "learning_rate": 2.0805838832233554e-05,
      "loss": 1.038,
      "step": 8760
    },
    {
      "epoch": 1.7536492701459707,
      "grad_norm": 11.44180679321289,
      "learning_rate": 2.077251216423382e-05,
      "loss": 0.9177,
      "step": 8770
    },
    {
      "epoch": 1.7556488702259547,
      "grad_norm": 11.606925010681152,
      "learning_rate": 2.0739185496234088e-05,
      "loss": 1.0563,
      "step": 8780
    },
    {
      "epoch": 1.7576484703059387,
      "grad_norm": 9.021439552307129,
      "learning_rate": 2.0705858828234355e-05,
      "loss": 0.6828,
      "step": 8790
    },
    {
      "epoch": 1.7596480703859227,
      "grad_norm": 11.329367637634277,
      "learning_rate": 2.067253216023462e-05,
      "loss": 0.8777,
      "step": 8800
    },
    {
      "epoch": 1.7616476704659068,
      "grad_norm": 10.107690811157227,
      "learning_rate": 2.0639205492234885e-05,
      "loss": 0.8922,
      "step": 8810
    },
    {
      "epoch": 1.7636472705458908,
      "grad_norm": 13.615962982177734,
      "learning_rate": 2.0605878824235152e-05,
      "loss": 0.9824,
      "step": 8820
    },
    {
      "epoch": 1.7656468706258748,
      "grad_norm": 17.762266159057617,
      "learning_rate": 2.057255215623542e-05,
      "loss": 1.1563,
      "step": 8830
    },
    {
      "epoch": 1.7676464707058588,
      "grad_norm": 12.241039276123047,
      "learning_rate": 2.0539225488235686e-05,
      "loss": 0.7723,
      "step": 8840
    },
    {
      "epoch": 1.7696460707858428,
      "grad_norm": 9.065059661865234,
      "learning_rate": 2.0505898820235953e-05,
      "loss": 0.8828,
      "step": 8850
    },
    {
      "epoch": 1.7716456708658268,
      "grad_norm": 20.97224998474121,
      "learning_rate": 2.047257215223622e-05,
      "loss": 0.814,
      "step": 8860
    },
    {
      "epoch": 1.7736452709458108,
      "grad_norm": 15.479120254516602,
      "learning_rate": 2.0439245484236487e-05,
      "loss": 0.9221,
      "step": 8870
    },
    {
      "epoch": 1.7756448710257948,
      "grad_norm": 10.539931297302246,
      "learning_rate": 2.0405918816236754e-05,
      "loss": 1.1889,
      "step": 8880
    },
    {
      "epoch": 1.7776444711057788,
      "grad_norm": 9.529964447021484,
      "learning_rate": 2.037259214823702e-05,
      "loss": 0.9212,
      "step": 8890
    },
    {
      "epoch": 1.7796440711857628,
      "grad_norm": 15.20002269744873,
      "learning_rate": 2.0339265480237287e-05,
      "loss": 0.9207,
      "step": 8900
    },
    {
      "epoch": 1.7816436712657469,
      "grad_norm": 16.73724365234375,
      "learning_rate": 2.0305938812237554e-05,
      "loss": 1.0364,
      "step": 8910
    },
    {
      "epoch": 1.7836432713457309,
      "grad_norm": 10.436765670776367,
      "learning_rate": 2.027261214423782e-05,
      "loss": 1.0879,
      "step": 8920
    },
    {
      "epoch": 1.7856428714257149,
      "grad_norm": 12.81459903717041,
      "learning_rate": 2.0239285476238088e-05,
      "loss": 0.7415,
      "step": 8930
    },
    {
      "epoch": 1.7876424715056989,
      "grad_norm": 19.77081298828125,
      "learning_rate": 2.0205958808238355e-05,
      "loss": 1.0254,
      "step": 8940
    },
    {
      "epoch": 1.789642071585683,
      "grad_norm": 10.381092071533203,
      "learning_rate": 2.0172632140238622e-05,
      "loss": 0.9298,
      "step": 8950
    },
    {
      "epoch": 1.791641671665667,
      "grad_norm": 10.86275863647461,
      "learning_rate": 2.0139305472238885e-05,
      "loss": 1.0363,
      "step": 8960
    },
    {
      "epoch": 1.793641271745651,
      "grad_norm": 13.695610046386719,
      "learning_rate": 2.0105978804239152e-05,
      "loss": 0.9721,
      "step": 8970
    },
    {
      "epoch": 1.795640871825635,
      "grad_norm": 15.264116287231445,
      "learning_rate": 2.007265213623942e-05,
      "loss": 0.9164,
      "step": 8980
    },
    {
      "epoch": 1.797640471905619,
      "grad_norm": 14.342418670654297,
      "learning_rate": 2.0039325468239686e-05,
      "loss": 0.8597,
      "step": 8990
    },
    {
      "epoch": 1.799640071985603,
      "grad_norm": 17.930665969848633,
      "learning_rate": 2.0005998800239953e-05,
      "loss": 0.9225,
      "step": 9000
    },
    {
      "epoch": 1.801639672065587,
      "grad_norm": 15.836463928222656,
      "learning_rate": 1.997267213224022e-05,
      "loss": 0.7023,
      "step": 9010
    },
    {
      "epoch": 1.803639272145571,
      "grad_norm": 12.789152145385742,
      "learning_rate": 1.9939345464240487e-05,
      "loss": 0.8973,
      "step": 9020
    },
    {
      "epoch": 1.805638872225555,
      "grad_norm": 12.876086235046387,
      "learning_rate": 1.9906018796240754e-05,
      "loss": 0.81,
      "step": 9030
    },
    {
      "epoch": 1.807638472305539,
      "grad_norm": 7.880180358886719,
      "learning_rate": 1.987269212824102e-05,
      "loss": 0.8356,
      "step": 9040
    },
    {
      "epoch": 1.809638072385523,
      "grad_norm": 13.32410717010498,
      "learning_rate": 1.9839365460241284e-05,
      "loss": 1.1132,
      "step": 9050
    },
    {
      "epoch": 1.811637672465507,
      "grad_norm": 27.95636558532715,
      "learning_rate": 1.980603879224155e-05,
      "loss": 1.0856,
      "step": 9060
    },
    {
      "epoch": 1.813637272545491,
      "grad_norm": 7.560559272766113,
      "learning_rate": 1.9772712124241818e-05,
      "loss": 0.7827,
      "step": 9070
    },
    {
      "epoch": 1.815636872625475,
      "grad_norm": 15.392495155334473,
      "learning_rate": 1.9739385456242085e-05,
      "loss": 0.8442,
      "step": 9080
    },
    {
      "epoch": 1.817636472705459,
      "grad_norm": 17.45170021057129,
      "learning_rate": 1.970605878824235e-05,
      "loss": 0.792,
      "step": 9090
    },
    {
      "epoch": 1.819636072785443,
      "grad_norm": 15.709832191467285,
      "learning_rate": 1.967273212024262e-05,
      "loss": 0.7872,
      "step": 9100
    },
    {
      "epoch": 1.821635672865427,
      "grad_norm": 13.550786972045898,
      "learning_rate": 1.9639405452242885e-05,
      "loss": 0.9873,
      "step": 9110
    },
    {
      "epoch": 1.823635272945411,
      "grad_norm": 15.451420783996582,
      "learning_rate": 1.9606078784243152e-05,
      "loss": 0.7994,
      "step": 9120
    },
    {
      "epoch": 1.825634873025395,
      "grad_norm": 15.823291778564453,
      "learning_rate": 1.957275211624342e-05,
      "loss": 1.0405,
      "step": 9130
    },
    {
      "epoch": 1.827634473105379,
      "grad_norm": 10.39190673828125,
      "learning_rate": 1.9539425448243686e-05,
      "loss": 0.8549,
      "step": 9140
    },
    {
      "epoch": 1.829634073185363,
      "grad_norm": 15.095505714416504,
      "learning_rate": 1.9506098780243953e-05,
      "loss": 0.9611,
      "step": 9150
    },
    {
      "epoch": 1.831633673265347,
      "grad_norm": 16.204410552978516,
      "learning_rate": 1.947277211224422e-05,
      "loss": 0.9971,
      "step": 9160
    },
    {
      "epoch": 1.8336332733453309,
      "grad_norm": 6.4726362228393555,
      "learning_rate": 1.9439445444244487e-05,
      "loss": 0.8253,
      "step": 9170
    },
    {
      "epoch": 1.835632873425315,
      "grad_norm": 11.180807113647461,
      "learning_rate": 1.9406118776244753e-05,
      "loss": 0.8543,
      "step": 9180
    },
    {
      "epoch": 1.837632473505299,
      "grad_norm": 21.38705062866211,
      "learning_rate": 1.937279210824502e-05,
      "loss": 0.9153,
      "step": 9190
    },
    {
      "epoch": 1.839632073585283,
      "grad_norm": 18.402969360351562,
      "learning_rate": 1.9339465440245287e-05,
      "loss": 0.9414,
      "step": 9200
    },
    {
      "epoch": 1.841631673665267,
      "grad_norm": 6.903501033782959,
      "learning_rate": 1.930613877224555e-05,
      "loss": 0.881,
      "step": 9210
    },
    {
      "epoch": 1.843631273745251,
      "grad_norm": 7.905416011810303,
      "learning_rate": 1.9272812104245818e-05,
      "loss": 0.9907,
      "step": 9220
    },
    {
      "epoch": 1.845630873825235,
      "grad_norm": 10.56607723236084,
      "learning_rate": 1.9239485436246085e-05,
      "loss": 0.8483,
      "step": 9230
    },
    {
      "epoch": 1.847630473905219,
      "grad_norm": 14.055285453796387,
      "learning_rate": 1.920615876824635e-05,
      "loss": 1.022,
      "step": 9240
    },
    {
      "epoch": 1.849630073985203,
      "grad_norm": 11.782842636108398,
      "learning_rate": 1.917283210024662e-05,
      "loss": 1.0266,
      "step": 9250
    },
    {
      "epoch": 1.851629674065187,
      "grad_norm": 10.175871849060059,
      "learning_rate": 1.9139505432246885e-05,
      "loss": 0.8575,
      "step": 9260
    },
    {
      "epoch": 1.853629274145171,
      "grad_norm": 9.090228080749512,
      "learning_rate": 1.9106178764247152e-05,
      "loss": 0.9012,
      "step": 9270
    },
    {
      "epoch": 1.855628874225155,
      "grad_norm": 7.652507781982422,
      "learning_rate": 1.907285209624742e-05,
      "loss": 0.8924,
      "step": 9280
    },
    {
      "epoch": 1.857628474305139,
      "grad_norm": 15.255035400390625,
      "learning_rate": 1.9039525428247686e-05,
      "loss": 0.9179,
      "step": 9290
    },
    {
      "epoch": 1.859628074385123,
      "grad_norm": 10.98122787475586,
      "learning_rate": 1.900619876024795e-05,
      "loss": 1.0818,
      "step": 9300
    },
    {
      "epoch": 1.861627674465107,
      "grad_norm": 15.384321212768555,
      "learning_rate": 1.8972872092248216e-05,
      "loss": 0.9836,
      "step": 9310
    },
    {
      "epoch": 1.863627274545091,
      "grad_norm": 26.9388427734375,
      "learning_rate": 1.8939545424248483e-05,
      "loss": 0.8974,
      "step": 9320
    },
    {
      "epoch": 1.865626874625075,
      "grad_norm": 12.916791915893555,
      "learning_rate": 1.890621875624875e-05,
      "loss": 0.7557,
      "step": 9330
    },
    {
      "epoch": 1.8676264747050588,
      "grad_norm": 13.508138656616211,
      "learning_rate": 1.8872892088249017e-05,
      "loss": 0.7722,
      "step": 9340
    },
    {
      "epoch": 1.8696260747850428,
      "grad_norm": 14.396242141723633,
      "learning_rate": 1.8839565420249284e-05,
      "loss": 1.3622,
      "step": 9350
    },
    {
      "epoch": 1.8716256748650268,
      "grad_norm": 10.766502380371094,
      "learning_rate": 1.880623875224955e-05,
      "loss": 0.7723,
      "step": 9360
    },
    {
      "epoch": 1.8736252749450109,
      "grad_norm": 10.913481712341309,
      "learning_rate": 1.8772912084249818e-05,
      "loss": 1.0079,
      "step": 9370
    },
    {
      "epoch": 1.8756248750249949,
      "grad_norm": 20.435184478759766,
      "learning_rate": 1.8739585416250085e-05,
      "loss": 0.6538,
      "step": 9380
    },
    {
      "epoch": 1.8776244751049789,
      "grad_norm": 17.866680145263672,
      "learning_rate": 1.870625874825035e-05,
      "loss": 0.8944,
      "step": 9390
    },
    {
      "epoch": 1.8796240751849629,
      "grad_norm": 16.603431701660156,
      "learning_rate": 1.8672932080250618e-05,
      "loss": 0.8338,
      "step": 9400
    },
    {
      "epoch": 1.881623675264947,
      "grad_norm": 13.20230770111084,
      "learning_rate": 1.8639605412250885e-05,
      "loss": 0.8271,
      "step": 9410
    },
    {
      "epoch": 1.883623275344931,
      "grad_norm": 7.094874858856201,
      "learning_rate": 1.8606278744251152e-05,
      "loss": 1.0168,
      "step": 9420
    },
    {
      "epoch": 1.885622875424915,
      "grad_norm": 10.963252067565918,
      "learning_rate": 1.857295207625142e-05,
      "loss": 0.8206,
      "step": 9430
    },
    {
      "epoch": 1.887622475504899,
      "grad_norm": 16.15831756591797,
      "learning_rate": 1.8539625408251686e-05,
      "loss": 0.7223,
      "step": 9440
    },
    {
      "epoch": 1.889622075584883,
      "grad_norm": 13.24299144744873,
      "learning_rate": 1.8506298740251953e-05,
      "loss": 0.7259,
      "step": 9450
    },
    {
      "epoch": 1.891621675664867,
      "grad_norm": 7.246231555938721,
      "learning_rate": 1.8472972072252216e-05,
      "loss": 0.994,
      "step": 9460
    },
    {
      "epoch": 1.893621275744851,
      "grad_norm": 9.332191467285156,
      "learning_rate": 1.8439645404252483e-05,
      "loss": 0.7954,
      "step": 9470
    },
    {
      "epoch": 1.895620875824835,
      "grad_norm": 17.472156524658203,
      "learning_rate": 1.840631873625275e-05,
      "loss": 1.0061,
      "step": 9480
    },
    {
      "epoch": 1.897620475904819,
      "grad_norm": 11.535503387451172,
      "learning_rate": 1.8372992068253017e-05,
      "loss": 1.0386,
      "step": 9490
    },
    {
      "epoch": 1.899620075984803,
      "grad_norm": 15.05003547668457,
      "learning_rate": 1.8339665400253284e-05,
      "loss": 0.8138,
      "step": 9500
    },
    {
      "epoch": 1.901619676064787,
      "grad_norm": 13.669083595275879,
      "learning_rate": 1.830633873225355e-05,
      "loss": 0.7975,
      "step": 9510
    },
    {
      "epoch": 1.903619276144771,
      "grad_norm": 13.927993774414062,
      "learning_rate": 1.8273012064253818e-05,
      "loss": 1.1258,
      "step": 9520
    },
    {
      "epoch": 1.905618876224755,
      "grad_norm": 1.2419037818908691,
      "learning_rate": 1.8239685396254084e-05,
      "loss": 0.7873,
      "step": 9530
    },
    {
      "epoch": 1.907618476304739,
      "grad_norm": 14.448333740234375,
      "learning_rate": 1.8206358728254348e-05,
      "loss": 0.7384,
      "step": 9540
    },
    {
      "epoch": 1.909618076384723,
      "grad_norm": 13.133004188537598,
      "learning_rate": 1.8173032060254615e-05,
      "loss": 0.6067,
      "step": 9550
    },
    {
      "epoch": 1.911617676464707,
      "grad_norm": 5.566751956939697,
      "learning_rate": 1.8139705392254882e-05,
      "loss": 0.6626,
      "step": 9560
    },
    {
      "epoch": 1.913617276544691,
      "grad_norm": 17.446382522583008,
      "learning_rate": 1.810637872425515e-05,
      "loss": 1.0282,
      "step": 9570
    },
    {
      "epoch": 1.915616876624675,
      "grad_norm": 20.57236671447754,
      "learning_rate": 1.8073052056255416e-05,
      "loss": 0.9497,
      "step": 9580
    },
    {
      "epoch": 1.917616476704659,
      "grad_norm": 14.766932487487793,
      "learning_rate": 1.8039725388255682e-05,
      "loss": 0.8677,
      "step": 9590
    },
    {
      "epoch": 1.919616076784643,
      "grad_norm": 5.919003009796143,
      "learning_rate": 1.800639872025595e-05,
      "loss": 1.1654,
      "step": 9600
    },
    {
      "epoch": 1.921615676864627,
      "grad_norm": 12.143542289733887,
      "learning_rate": 1.7973072052256216e-05,
      "loss": 0.9579,
      "step": 9610
    },
    {
      "epoch": 1.923615276944611,
      "grad_norm": 9.730494499206543,
      "learning_rate": 1.7939745384256483e-05,
      "loss": 0.8149,
      "step": 9620
    },
    {
      "epoch": 1.925614877024595,
      "grad_norm": 16.596120834350586,
      "learning_rate": 1.7906418716256747e-05,
      "loss": 0.9119,
      "step": 9630
    },
    {
      "epoch": 1.9276144771045791,
      "grad_norm": 10.097875595092773,
      "learning_rate": 1.7873092048257017e-05,
      "loss": 1.1861,
      "step": 9640
    },
    {
      "epoch": 1.9296140771845631,
      "grad_norm": 8.297917366027832,
      "learning_rate": 1.7839765380257284e-05,
      "loss": 0.9679,
      "step": 9650
    },
    {
      "epoch": 1.9316136772645471,
      "grad_norm": 7.627737522125244,
      "learning_rate": 1.780643871225755e-05,
      "loss": 0.803,
      "step": 9660
    },
    {
      "epoch": 1.9336132773445311,
      "grad_norm": 8.948898315429688,
      "learning_rate": 1.7773112044257818e-05,
      "loss": 0.9855,
      "step": 9670
    },
    {
      "epoch": 1.9356128774245152,
      "grad_norm": 12.643610000610352,
      "learning_rate": 1.7739785376258084e-05,
      "loss": 1.002,
      "step": 9680
    },
    {
      "epoch": 1.9376124775044992,
      "grad_norm": 11.639910697937012,
      "learning_rate": 1.770645870825835e-05,
      "loss": 0.8557,
      "step": 9690
    },
    {
      "epoch": 1.9396120775844832,
      "grad_norm": 14.669049263000488,
      "learning_rate": 1.7673132040258615e-05,
      "loss": 0.7671,
      "step": 9700
    },
    {
      "epoch": 1.9416116776644672,
      "grad_norm": 16.027828216552734,
      "learning_rate": 1.7639805372258882e-05,
      "loss": 0.7013,
      "step": 9710
    },
    {
      "epoch": 1.9436112777444512,
      "grad_norm": 10.484170913696289,
      "learning_rate": 1.760647870425915e-05,
      "loss": 1.0175,
      "step": 9720
    },
    {
      "epoch": 1.9456108778244352,
      "grad_norm": 14.227584838867188,
      "learning_rate": 1.7573152036259416e-05,
      "loss": 0.9531,
      "step": 9730
    },
    {
      "epoch": 1.9476104779044192,
      "grad_norm": 9.757780075073242,
      "learning_rate": 1.7539825368259682e-05,
      "loss": 0.9216,
      "step": 9740
    },
    {
      "epoch": 1.9496100779844032,
      "grad_norm": 21.43222427368164,
      "learning_rate": 1.750649870025995e-05,
      "loss": 0.8357,
      "step": 9750
    },
    {
      "epoch": 1.9516096780643872,
      "grad_norm": 7.027749538421631,
      "learning_rate": 1.7473172032260216e-05,
      "loss": 0.8046,
      "step": 9760
    },
    {
      "epoch": 1.9536092781443712,
      "grad_norm": 18.131330490112305,
      "learning_rate": 1.7439845364260483e-05,
      "loss": 0.9148,
      "step": 9770
    },
    {
      "epoch": 1.9556088782243553,
      "grad_norm": 13.484237670898438,
      "learning_rate": 1.740651869626075e-05,
      "loss": 0.7505,
      "step": 9780
    },
    {
      "epoch": 1.9576084783043393,
      "grad_norm": 11.152361869812012,
      "learning_rate": 1.7373192028261013e-05,
      "loss": 0.9874,
      "step": 9790
    },
    {
      "epoch": 1.9596080783843233,
      "grad_norm": 9.916866302490234,
      "learning_rate": 1.733986536026128e-05,
      "loss": 1.018,
      "step": 9800
    },
    {
      "epoch": 1.9616076784643073,
      "grad_norm": 12.454010009765625,
      "learning_rate": 1.7306538692261547e-05,
      "loss": 0.9393,
      "step": 9810
    },
    {
      "epoch": 1.9636072785442913,
      "grad_norm": 13.449954986572266,
      "learning_rate": 1.7273212024261814e-05,
      "loss": 1.2293,
      "step": 9820
    },
    {
      "epoch": 1.9656068786242753,
      "grad_norm": 12.880757331848145,
      "learning_rate": 1.723988535626208e-05,
      "loss": 0.7754,
      "step": 9830
    },
    {
      "epoch": 1.9676064787042593,
      "grad_norm": 15.530356407165527,
      "learning_rate": 1.7206558688262348e-05,
      "loss": 0.9341,
      "step": 9840
    },
    {
      "epoch": 1.969606078784243,
      "grad_norm": 7.1057634353637695,
      "learning_rate": 1.7173232020262615e-05,
      "loss": 0.9456,
      "step": 9850
    },
    {
      "epoch": 1.971605678864227,
      "grad_norm": 5.957801818847656,
      "learning_rate": 1.713990535226288e-05,
      "loss": 0.9068,
      "step": 9860
    },
    {
      "epoch": 1.9736052789442111,
      "grad_norm": 16.901195526123047,
      "learning_rate": 1.710657868426315e-05,
      "loss": 1.0635,
      "step": 9870
    },
    {
      "epoch": 1.9756048790241951,
      "grad_norm": 14.567813873291016,
      "learning_rate": 1.7073252016263412e-05,
      "loss": 1.1333,
      "step": 9880
    },
    {
      "epoch": 1.9776044791041791,
      "grad_norm": 11.198480606079102,
      "learning_rate": 1.703992534826368e-05,
      "loss": 0.6127,
      "step": 9890
    },
    {
      "epoch": 1.9796040791841631,
      "grad_norm": 11.764053344726562,
      "learning_rate": 1.700659868026395e-05,
      "loss": 0.8163,
      "step": 9900
    },
    {
      "epoch": 1.9816036792641472,
      "grad_norm": 11.911630630493164,
      "learning_rate": 1.6973272012264216e-05,
      "loss": 0.8993,
      "step": 9910
    },
    {
      "epoch": 1.9836032793441312,
      "grad_norm": 13.472591400146484,
      "learning_rate": 1.6939945344264483e-05,
      "loss": 0.8304,
      "step": 9920
    },
    {
      "epoch": 1.9856028794241152,
      "grad_norm": 13.898326873779297,
      "learning_rate": 1.690661867626475e-05,
      "loss": 0.9375,
      "step": 9930
    },
    {
      "epoch": 1.9876024795040992,
      "grad_norm": 12.9910888671875,
      "learning_rate": 1.6873292008265017e-05,
      "loss": 0.9237,
      "step": 9940
    },
    {
      "epoch": 1.9896020795840832,
      "grad_norm": 10.924692153930664,
      "learning_rate": 1.683996534026528e-05,
      "loss": 0.6581,
      "step": 9950
    },
    {
      "epoch": 1.9916016796640672,
      "grad_norm": 17.41364097595215,
      "learning_rate": 1.6806638672265547e-05,
      "loss": 0.7483,
      "step": 9960
    },
    {
      "epoch": 1.9936012797440512,
      "grad_norm": 10.70089340209961,
      "learning_rate": 1.6773312004265814e-05,
      "loss": 0.8363,
      "step": 9970
    },
    {
      "epoch": 1.9956008798240352,
      "grad_norm": 29.129552841186523,
      "learning_rate": 1.673998533626608e-05,
      "loss": 0.7077,
      "step": 9980
    },
    {
      "epoch": 1.9976004799040192,
      "grad_norm": 15.672335624694824,
      "learning_rate": 1.6706658668266348e-05,
      "loss": 0.844,
      "step": 9990
    },
    {
      "epoch": 1.9996000799840032,
      "grad_norm": 14.855646133422852,
      "learning_rate": 1.6673332000266615e-05,
      "loss": 0.9896,
      "step": 10000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6614677064587082,
      "eval_loss": 1.0968632698059082,
      "eval_runtime": 42.8092,
      "eval_samples_per_second": 233.641,
      "eval_steps_per_second": 29.223,
      "step": 10002
    }
  ],
  "logging_steps": 10,
  "max_steps": 15003,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5263651605897216.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
