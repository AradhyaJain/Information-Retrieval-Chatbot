{
  "best_metric": 0.6666666666666666,
  "best_model_checkpoint": "./trained_model/checkpoint-15003",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 15003,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001999600079984003,
      "grad_norm": 8.486483573913574,
      "learning_rate": 4.9966673332000266e-05,
      "loss": 2.29,
      "step": 10
    },
    {
      "epoch": 0.003999200159968006,
      "grad_norm": 23.02979850769043,
      "learning_rate": 4.9933346664000536e-05,
      "loss": 2.3425,
      "step": 20
    },
    {
      "epoch": 0.00599880023995201,
      "grad_norm": 8.020190238952637,
      "learning_rate": 4.99000199960008e-05,
      "loss": 2.3202,
      "step": 30
    },
    {
      "epoch": 0.007998400319936013,
      "grad_norm": 6.065441608428955,
      "learning_rate": 4.986669332800107e-05,
      "loss": 2.2276,
      "step": 40
    },
    {
      "epoch": 0.009998000399920015,
      "grad_norm": 6.884082794189453,
      "learning_rate": 4.9833366660001333e-05,
      "loss": 2.2507,
      "step": 50
    },
    {
      "epoch": 0.01199760047990402,
      "grad_norm": 7.542108058929443,
      "learning_rate": 4.9800039992001604e-05,
      "loss": 2.2436,
      "step": 60
    },
    {
      "epoch": 0.013997200559888023,
      "grad_norm": 8.273296356201172,
      "learning_rate": 4.976671332400187e-05,
      "loss": 2.2028,
      "step": 70
    },
    {
      "epoch": 0.015996800639872025,
      "grad_norm": 7.5410661697387695,
      "learning_rate": 4.973338665600214e-05,
      "loss": 2.2457,
      "step": 80
    },
    {
      "epoch": 0.017996400719856028,
      "grad_norm": 6.637073040008545,
      "learning_rate": 4.97000599880024e-05,
      "loss": 2.1048,
      "step": 90
    },
    {
      "epoch": 0.01999600079984003,
      "grad_norm": 7.577936172485352,
      "learning_rate": 4.9666733320002665e-05,
      "loss": 2.1358,
      "step": 100
    },
    {
      "epoch": 0.021995600879824034,
      "grad_norm": 6.453656196594238,
      "learning_rate": 4.9633406652002935e-05,
      "loss": 2.1912,
      "step": 110
    },
    {
      "epoch": 0.02399520095980804,
      "grad_norm": 7.3829121589660645,
      "learning_rate": 4.96000799840032e-05,
      "loss": 2.1957,
      "step": 120
    },
    {
      "epoch": 0.025994801039792043,
      "grad_norm": 8.003046035766602,
      "learning_rate": 4.956675331600347e-05,
      "loss": 2.1056,
      "step": 130
    },
    {
      "epoch": 0.027994401119776045,
      "grad_norm": 6.657829284667969,
      "learning_rate": 4.953342664800373e-05,
      "loss": 2.1079,
      "step": 140
    },
    {
      "epoch": 0.029994001199760048,
      "grad_norm": 6.863305568695068,
      "learning_rate": 4.9500099980004e-05,
      "loss": 2.0691,
      "step": 150
    },
    {
      "epoch": 0.03199360127974405,
      "grad_norm": 7.502652168273926,
      "learning_rate": 4.9466773312004266e-05,
      "loss": 2.0581,
      "step": 160
    },
    {
      "epoch": 0.033993201359728054,
      "grad_norm": 6.225324630737305,
      "learning_rate": 4.9433446644004536e-05,
      "loss": 2.0105,
      "step": 170
    },
    {
      "epoch": 0.035992801439712056,
      "grad_norm": 7.140869140625,
      "learning_rate": 4.94001199760048e-05,
      "loss": 1.9497,
      "step": 180
    },
    {
      "epoch": 0.03799240151969606,
      "grad_norm": 7.257124423980713,
      "learning_rate": 4.936679330800506e-05,
      "loss": 2.0594,
      "step": 190
    },
    {
      "epoch": 0.03999200159968006,
      "grad_norm": 8.895133018493652,
      "learning_rate": 4.9333466640005333e-05,
      "loss": 1.9021,
      "step": 200
    },
    {
      "epoch": 0.041991601679664065,
      "grad_norm": 8.184698104858398,
      "learning_rate": 4.93001399720056e-05,
      "loss": 2.0463,
      "step": 210
    },
    {
      "epoch": 0.04399120175964807,
      "grad_norm": 7.4633002281188965,
      "learning_rate": 4.926681330400587e-05,
      "loss": 1.8619,
      "step": 220
    },
    {
      "epoch": 0.04599080183963208,
      "grad_norm": 10.627445220947266,
      "learning_rate": 4.923348663600613e-05,
      "loss": 1.9402,
      "step": 230
    },
    {
      "epoch": 0.04799040191961608,
      "grad_norm": 6.723014831542969,
      "learning_rate": 4.92001599680064e-05,
      "loss": 1.8906,
      "step": 240
    },
    {
      "epoch": 0.04999000199960008,
      "grad_norm": 8.850643157958984,
      "learning_rate": 4.9166833300006664e-05,
      "loss": 1.9113,
      "step": 250
    },
    {
      "epoch": 0.051989602079584085,
      "grad_norm": 7.870445251464844,
      "learning_rate": 4.9133506632006935e-05,
      "loss": 1.9412,
      "step": 260
    },
    {
      "epoch": 0.05398920215956809,
      "grad_norm": 7.138870716094971,
      "learning_rate": 4.9100179964007205e-05,
      "loss": 1.9972,
      "step": 270
    },
    {
      "epoch": 0.05598880223955209,
      "grad_norm": 10.539412498474121,
      "learning_rate": 4.906685329600747e-05,
      "loss": 1.9351,
      "step": 280
    },
    {
      "epoch": 0.05798840231953609,
      "grad_norm": 13.7241792678833,
      "learning_rate": 4.903352662800774e-05,
      "loss": 1.8579,
      "step": 290
    },
    {
      "epoch": 0.059988002399520096,
      "grad_norm": 6.764505863189697,
      "learning_rate": 4.9000199960008e-05,
      "loss": 1.7772,
      "step": 300
    },
    {
      "epoch": 0.0619876024795041,
      "grad_norm": 8.268332481384277,
      "learning_rate": 4.896687329200827e-05,
      "loss": 1.8775,
      "step": 310
    },
    {
      "epoch": 0.0639872025594881,
      "grad_norm": 9.532472610473633,
      "learning_rate": 4.8933546624008536e-05,
      "loss": 1.8566,
      "step": 320
    },
    {
      "epoch": 0.06598680263947211,
      "grad_norm": 9.410441398620605,
      "learning_rate": 4.89002199560088e-05,
      "loss": 1.7573,
      "step": 330
    },
    {
      "epoch": 0.06798640271945611,
      "grad_norm": 8.815823554992676,
      "learning_rate": 4.886689328800907e-05,
      "loss": 1.8349,
      "step": 340
    },
    {
      "epoch": 0.06998600279944012,
      "grad_norm": 10.014233589172363,
      "learning_rate": 4.8833566620009333e-05,
      "loss": 1.7115,
      "step": 350
    },
    {
      "epoch": 0.07198560287942411,
      "grad_norm": 7.97587776184082,
      "learning_rate": 4.8800239952009604e-05,
      "loss": 1.7167,
      "step": 360
    },
    {
      "epoch": 0.07398520295940812,
      "grad_norm": 10.347760200500488,
      "learning_rate": 4.876691328400987e-05,
      "loss": 1.9247,
      "step": 370
    },
    {
      "epoch": 0.07598480303939212,
      "grad_norm": 9.23169231414795,
      "learning_rate": 4.873358661601014e-05,
      "loss": 1.6842,
      "step": 380
    },
    {
      "epoch": 0.07798440311937613,
      "grad_norm": 8.449100494384766,
      "learning_rate": 4.87002599480104e-05,
      "loss": 1.6756,
      "step": 390
    },
    {
      "epoch": 0.07998400319936012,
      "grad_norm": 6.70433235168457,
      "learning_rate": 4.866693328001067e-05,
      "loss": 1.4329,
      "step": 400
    },
    {
      "epoch": 0.08198360327934413,
      "grad_norm": 9.499358177185059,
      "learning_rate": 4.8633606612010935e-05,
      "loss": 1.9025,
      "step": 410
    },
    {
      "epoch": 0.08398320335932813,
      "grad_norm": 9.831555366516113,
      "learning_rate": 4.86002799440112e-05,
      "loss": 1.8554,
      "step": 420
    },
    {
      "epoch": 0.08598280343931214,
      "grad_norm": 8.456974983215332,
      "learning_rate": 4.856695327601147e-05,
      "loss": 1.6412,
      "step": 430
    },
    {
      "epoch": 0.08798240351929613,
      "grad_norm": 6.636467933654785,
      "learning_rate": 4.853362660801173e-05,
      "loss": 1.5953,
      "step": 440
    },
    {
      "epoch": 0.08998200359928014,
      "grad_norm": 14.0305757522583,
      "learning_rate": 4.8500299940012e-05,
      "loss": 1.4538,
      "step": 450
    },
    {
      "epoch": 0.09198160367926415,
      "grad_norm": 10.056876182556152,
      "learning_rate": 4.8466973272012266e-05,
      "loss": 1.6688,
      "step": 460
    },
    {
      "epoch": 0.09398120375924815,
      "grad_norm": 11.251740455627441,
      "learning_rate": 4.8433646604012536e-05,
      "loss": 1.6104,
      "step": 470
    },
    {
      "epoch": 0.09598080383923216,
      "grad_norm": 11.618427276611328,
      "learning_rate": 4.84003199360128e-05,
      "loss": 1.6278,
      "step": 480
    },
    {
      "epoch": 0.09798040391921616,
      "grad_norm": 11.514531135559082,
      "learning_rate": 4.836699326801307e-05,
      "loss": 1.525,
      "step": 490
    },
    {
      "epoch": 0.09998000399920016,
      "grad_norm": 12.38217830657959,
      "learning_rate": 4.833366660001333e-05,
      "loss": 1.7233,
      "step": 500
    },
    {
      "epoch": 0.10197960407918416,
      "grad_norm": 9.643281936645508,
      "learning_rate": 4.83003399320136e-05,
      "loss": 1.5336,
      "step": 510
    },
    {
      "epoch": 0.10397920415916817,
      "grad_norm": 9.240829467773438,
      "learning_rate": 4.826701326401387e-05,
      "loss": 1.497,
      "step": 520
    },
    {
      "epoch": 0.10597880423915217,
      "grad_norm": 11.246994972229004,
      "learning_rate": 4.823368659601413e-05,
      "loss": 1.5234,
      "step": 530
    },
    {
      "epoch": 0.10797840431913618,
      "grad_norm": 10.207000732421875,
      "learning_rate": 4.82003599280144e-05,
      "loss": 1.9054,
      "step": 540
    },
    {
      "epoch": 0.10997800439912017,
      "grad_norm": 14.543898582458496,
      "learning_rate": 4.8167033260014664e-05,
      "loss": 1.5054,
      "step": 550
    },
    {
      "epoch": 0.11197760447910418,
      "grad_norm": 8.36032485961914,
      "learning_rate": 4.8133706592014935e-05,
      "loss": 1.5388,
      "step": 560
    },
    {
      "epoch": 0.11397720455908818,
      "grad_norm": 13.23294448852539,
      "learning_rate": 4.81003799240152e-05,
      "loss": 1.5393,
      "step": 570
    },
    {
      "epoch": 0.11597680463907219,
      "grad_norm": 9.620367050170898,
      "learning_rate": 4.806705325601547e-05,
      "loss": 1.4854,
      "step": 580
    },
    {
      "epoch": 0.11797640471905618,
      "grad_norm": 11.607989311218262,
      "learning_rate": 4.803372658801573e-05,
      "loss": 1.8466,
      "step": 590
    },
    {
      "epoch": 0.11997600479904019,
      "grad_norm": 9.428267478942871,
      "learning_rate": 4.8000399920015995e-05,
      "loss": 1.4937,
      "step": 600
    },
    {
      "epoch": 0.1219756048790242,
      "grad_norm": 9.171460151672363,
      "learning_rate": 4.7967073252016266e-05,
      "loss": 1.6304,
      "step": 610
    },
    {
      "epoch": 0.1239752049590082,
      "grad_norm": 11.798443794250488,
      "learning_rate": 4.793374658401653e-05,
      "loss": 1.4827,
      "step": 620
    },
    {
      "epoch": 0.1259748050389922,
      "grad_norm": 8.675115585327148,
      "learning_rate": 4.79004199160168e-05,
      "loss": 1.6025,
      "step": 630
    },
    {
      "epoch": 0.1279744051189762,
      "grad_norm": 18.90923500061035,
      "learning_rate": 4.786709324801706e-05,
      "loss": 1.6164,
      "step": 640
    },
    {
      "epoch": 0.1299740051989602,
      "grad_norm": 10.327335357666016,
      "learning_rate": 4.783376658001733e-05,
      "loss": 1.4738,
      "step": 650
    },
    {
      "epoch": 0.13197360527894422,
      "grad_norm": 7.653177738189697,
      "learning_rate": 4.78004399120176e-05,
      "loss": 1.5338,
      "step": 660
    },
    {
      "epoch": 0.1339732053589282,
      "grad_norm": 12.2083740234375,
      "learning_rate": 4.776711324401786e-05,
      "loss": 1.5009,
      "step": 670
    },
    {
      "epoch": 0.13597280543891221,
      "grad_norm": 9.088950157165527,
      "learning_rate": 4.773378657601813e-05,
      "loss": 1.4126,
      "step": 680
    },
    {
      "epoch": 0.13797240551889622,
      "grad_norm": 10.023622512817383,
      "learning_rate": 4.7700459908018394e-05,
      "loss": 1.8376,
      "step": 690
    },
    {
      "epoch": 0.13997200559888023,
      "grad_norm": 12.354560852050781,
      "learning_rate": 4.7667133240018664e-05,
      "loss": 1.3388,
      "step": 700
    },
    {
      "epoch": 0.14197160567886422,
      "grad_norm": 10.93286418914795,
      "learning_rate": 4.763380657201893e-05,
      "loss": 1.5739,
      "step": 710
    },
    {
      "epoch": 0.14397120575884823,
      "grad_norm": 7.3670477867126465,
      "learning_rate": 4.76004799040192e-05,
      "loss": 1.6356,
      "step": 720
    },
    {
      "epoch": 0.14597080583883223,
      "grad_norm": 11.679475784301758,
      "learning_rate": 4.756715323601946e-05,
      "loss": 1.48,
      "step": 730
    },
    {
      "epoch": 0.14797040591881624,
      "grad_norm": 10.872512817382812,
      "learning_rate": 4.753382656801973e-05,
      "loss": 1.7101,
      "step": 740
    },
    {
      "epoch": 0.14997000599880023,
      "grad_norm": 8.5926513671875,
      "learning_rate": 4.7500499900019995e-05,
      "loss": 1.4482,
      "step": 750
    },
    {
      "epoch": 0.15196960607878424,
      "grad_norm": 15.137410163879395,
      "learning_rate": 4.746717323202026e-05,
      "loss": 1.6292,
      "step": 760
    },
    {
      "epoch": 0.15396920615876825,
      "grad_norm": 7.711568832397461,
      "learning_rate": 4.743384656402053e-05,
      "loss": 1.596,
      "step": 770
    },
    {
      "epoch": 0.15596880623875226,
      "grad_norm": 8.692476272583008,
      "learning_rate": 4.74005198960208e-05,
      "loss": 1.6412,
      "step": 780
    },
    {
      "epoch": 0.15796840631873627,
      "grad_norm": 11.786295890808105,
      "learning_rate": 4.736719322802107e-05,
      "loss": 1.2785,
      "step": 790
    },
    {
      "epoch": 0.15996800639872025,
      "grad_norm": 13.041200637817383,
      "learning_rate": 4.733386656002133e-05,
      "loss": 1.3944,
      "step": 800
    },
    {
      "epoch": 0.16196760647870426,
      "grad_norm": 9.187958717346191,
      "learning_rate": 4.7300539892021604e-05,
      "loss": 1.5101,
      "step": 810
    },
    {
      "epoch": 0.16396720655868827,
      "grad_norm": 7.86534309387207,
      "learning_rate": 4.726721322402187e-05,
      "loss": 1.4288,
      "step": 820
    },
    {
      "epoch": 0.16596680663867228,
      "grad_norm": 17.428510665893555,
      "learning_rate": 4.723388655602213e-05,
      "loss": 1.3366,
      "step": 830
    },
    {
      "epoch": 0.16796640671865626,
      "grad_norm": 10.72262954711914,
      "learning_rate": 4.72005598880224e-05,
      "loss": 1.4652,
      "step": 840
    },
    {
      "epoch": 0.16996600679864027,
      "grad_norm": 9.7687349319458,
      "learning_rate": 4.7167233220022664e-05,
      "loss": 1.4758,
      "step": 850
    },
    {
      "epoch": 0.17196560687862428,
      "grad_norm": 9.781272888183594,
      "learning_rate": 4.7133906552022935e-05,
      "loss": 1.3669,
      "step": 860
    },
    {
      "epoch": 0.1739652069586083,
      "grad_norm": 13.76578426361084,
      "learning_rate": 4.71005798840232e-05,
      "loss": 1.5577,
      "step": 870
    },
    {
      "epoch": 0.17596480703859227,
      "grad_norm": 6.283199310302734,
      "learning_rate": 4.706725321602347e-05,
      "loss": 1.5136,
      "step": 880
    },
    {
      "epoch": 0.17796440711857628,
      "grad_norm": 7.023477077484131,
      "learning_rate": 4.703392654802373e-05,
      "loss": 1.3839,
      "step": 890
    },
    {
      "epoch": 0.1799640071985603,
      "grad_norm": 10.161480903625488,
      "learning_rate": 4.7000599880024e-05,
      "loss": 1.6868,
      "step": 900
    },
    {
      "epoch": 0.1819636072785443,
      "grad_norm": 12.608484268188477,
      "learning_rate": 4.6967273212024266e-05,
      "loss": 1.6066,
      "step": 910
    },
    {
      "epoch": 0.1839632073585283,
      "grad_norm": 9.216276168823242,
      "learning_rate": 4.693394654402453e-05,
      "loss": 1.7574,
      "step": 920
    },
    {
      "epoch": 0.1859628074385123,
      "grad_norm": 13.055495262145996,
      "learning_rate": 4.69006198760248e-05,
      "loss": 1.584,
      "step": 930
    },
    {
      "epoch": 0.1879624075184963,
      "grad_norm": 14.09416389465332,
      "learning_rate": 4.686729320802506e-05,
      "loss": 1.3843,
      "step": 940
    },
    {
      "epoch": 0.1899620075984803,
      "grad_norm": 7.26009464263916,
      "learning_rate": 4.683396654002533e-05,
      "loss": 1.2144,
      "step": 950
    },
    {
      "epoch": 0.19196160767846432,
      "grad_norm": 7.7341437339782715,
      "learning_rate": 4.68006398720256e-05,
      "loss": 1.3398,
      "step": 960
    },
    {
      "epoch": 0.1939612077584483,
      "grad_norm": 12.265687942504883,
      "learning_rate": 4.676731320402587e-05,
      "loss": 1.6898,
      "step": 970
    },
    {
      "epoch": 0.1959608078384323,
      "grad_norm": 11.596131324768066,
      "learning_rate": 4.673398653602613e-05,
      "loss": 1.5245,
      "step": 980
    },
    {
      "epoch": 0.19796040791841632,
      "grad_norm": 12.545600891113281,
      "learning_rate": 4.6700659868026394e-05,
      "loss": 1.4145,
      "step": 990
    },
    {
      "epoch": 0.19996000799840033,
      "grad_norm": 12.142346382141113,
      "learning_rate": 4.6667333200026664e-05,
      "loss": 1.4091,
      "step": 1000
    },
    {
      "epoch": 0.2019596080783843,
      "grad_norm": 12.169853210449219,
      "learning_rate": 4.663400653202693e-05,
      "loss": 1.5774,
      "step": 1010
    },
    {
      "epoch": 0.20395920815836832,
      "grad_norm": 11.01085376739502,
      "learning_rate": 4.66006798640272e-05,
      "loss": 1.5963,
      "step": 1020
    },
    {
      "epoch": 0.20595880823835233,
      "grad_norm": 9.94249439239502,
      "learning_rate": 4.656735319602746e-05,
      "loss": 1.7998,
      "step": 1030
    },
    {
      "epoch": 0.20795840831833634,
      "grad_norm": 13.21448040008545,
      "learning_rate": 4.653402652802773e-05,
      "loss": 1.5053,
      "step": 1040
    },
    {
      "epoch": 0.20995800839832032,
      "grad_norm": 12.69991683959961,
      "learning_rate": 4.6500699860027995e-05,
      "loss": 1.3867,
      "step": 1050
    },
    {
      "epoch": 0.21195760847830433,
      "grad_norm": 9.030729293823242,
      "learning_rate": 4.6467373192028266e-05,
      "loss": 1.3164,
      "step": 1060
    },
    {
      "epoch": 0.21395720855828834,
      "grad_norm": 9.211381912231445,
      "learning_rate": 4.643404652402853e-05,
      "loss": 1.4043,
      "step": 1070
    },
    {
      "epoch": 0.21595680863827235,
      "grad_norm": 10.702520370483398,
      "learning_rate": 4.640071985602879e-05,
      "loss": 1.3004,
      "step": 1080
    },
    {
      "epoch": 0.21795640871825636,
      "grad_norm": 11.85316276550293,
      "learning_rate": 4.636739318802906e-05,
      "loss": 1.1928,
      "step": 1090
    },
    {
      "epoch": 0.21995600879824034,
      "grad_norm": 9.451969146728516,
      "learning_rate": 4.6334066520029326e-05,
      "loss": 1.6014,
      "step": 1100
    },
    {
      "epoch": 0.22195560887822435,
      "grad_norm": 12.471943855285645,
      "learning_rate": 4.63007398520296e-05,
      "loss": 1.7064,
      "step": 1110
    },
    {
      "epoch": 0.22395520895820836,
      "grad_norm": 8.981622695922852,
      "learning_rate": 4.626741318402986e-05,
      "loss": 1.459,
      "step": 1120
    },
    {
      "epoch": 0.22595480903819237,
      "grad_norm": 8.550056457519531,
      "learning_rate": 4.623408651603013e-05,
      "loss": 1.6949,
      "step": 1130
    },
    {
      "epoch": 0.22795440911817635,
      "grad_norm": 9.892953872680664,
      "learning_rate": 4.6200759848030394e-05,
      "loss": 1.7617,
      "step": 1140
    },
    {
      "epoch": 0.22995400919816036,
      "grad_norm": 8.144174575805664,
      "learning_rate": 4.6167433180030664e-05,
      "loss": 1.4758,
      "step": 1150
    },
    {
      "epoch": 0.23195360927814437,
      "grad_norm": 8.207463264465332,
      "learning_rate": 4.613410651203093e-05,
      "loss": 1.4159,
      "step": 1160
    },
    {
      "epoch": 0.23395320935812838,
      "grad_norm": 10.39710521697998,
      "learning_rate": 4.610077984403119e-05,
      "loss": 1.2303,
      "step": 1170
    },
    {
      "epoch": 0.23595280943811237,
      "grad_norm": 12.242365837097168,
      "learning_rate": 4.606745317603146e-05,
      "loss": 1.6174,
      "step": 1180
    },
    {
      "epoch": 0.23795240951809637,
      "grad_norm": 6.95166540145874,
      "learning_rate": 4.6034126508031725e-05,
      "loss": 1.2082,
      "step": 1190
    },
    {
      "epoch": 0.23995200959808038,
      "grad_norm": 11.375865936279297,
      "learning_rate": 4.6000799840031995e-05,
      "loss": 1.4974,
      "step": 1200
    },
    {
      "epoch": 0.2419516096780644,
      "grad_norm": 11.419027328491211,
      "learning_rate": 4.596747317203226e-05,
      "loss": 1.572,
      "step": 1210
    },
    {
      "epoch": 0.2439512097580484,
      "grad_norm": 10.515029907226562,
      "learning_rate": 4.593414650403253e-05,
      "loss": 1.4316,
      "step": 1220
    },
    {
      "epoch": 0.24595080983803239,
      "grad_norm": 8.428004264831543,
      "learning_rate": 4.590081983603279e-05,
      "loss": 1.3163,
      "step": 1230
    },
    {
      "epoch": 0.2479504099180164,
      "grad_norm": 10.471663475036621,
      "learning_rate": 4.586749316803306e-05,
      "loss": 1.3908,
      "step": 1240
    },
    {
      "epoch": 0.2499500099980004,
      "grad_norm": 10.277301788330078,
      "learning_rate": 4.5834166500033326e-05,
      "loss": 1.5551,
      "step": 1250
    },
    {
      "epoch": 0.2519496100779844,
      "grad_norm": 6.981058597564697,
      "learning_rate": 4.580083983203359e-05,
      "loss": 1.4664,
      "step": 1260
    },
    {
      "epoch": 0.2539492101579684,
      "grad_norm": 12.309026718139648,
      "learning_rate": 4.576751316403386e-05,
      "loss": 1.3181,
      "step": 1270
    },
    {
      "epoch": 0.2559488102379524,
      "grad_norm": 12.076576232910156,
      "learning_rate": 4.5734186496034124e-05,
      "loss": 1.5454,
      "step": 1280
    },
    {
      "epoch": 0.2579484103179364,
      "grad_norm": 10.341789245605469,
      "learning_rate": 4.57008598280344e-05,
      "loss": 1.4374,
      "step": 1290
    },
    {
      "epoch": 0.2599480103979204,
      "grad_norm": 8.169737815856934,
      "learning_rate": 4.5667533160034664e-05,
      "loss": 1.3568,
      "step": 1300
    },
    {
      "epoch": 0.26194761047790444,
      "grad_norm": 7.443861961364746,
      "learning_rate": 4.563420649203493e-05,
      "loss": 1.3275,
      "step": 1310
    },
    {
      "epoch": 0.26394721055788845,
      "grad_norm": 10.078680038452148,
      "learning_rate": 4.56008798240352e-05,
      "loss": 1.4387,
      "step": 1320
    },
    {
      "epoch": 0.2659468106378724,
      "grad_norm": 9.901040077209473,
      "learning_rate": 4.556755315603546e-05,
      "loss": 1.0959,
      "step": 1330
    },
    {
      "epoch": 0.2679464107178564,
      "grad_norm": 9.337478637695312,
      "learning_rate": 4.553422648803573e-05,
      "loss": 1.5797,
      "step": 1340
    },
    {
      "epoch": 0.2699460107978404,
      "grad_norm": 16.274538040161133,
      "learning_rate": 4.5500899820035995e-05,
      "loss": 1.1363,
      "step": 1350
    },
    {
      "epoch": 0.27194561087782443,
      "grad_norm": 8.576570510864258,
      "learning_rate": 4.5467573152036266e-05,
      "loss": 1.2673,
      "step": 1360
    },
    {
      "epoch": 0.27394521095780844,
      "grad_norm": 14.18104362487793,
      "learning_rate": 4.543424648403653e-05,
      "loss": 1.3367,
      "step": 1370
    },
    {
      "epoch": 0.27594481103779245,
      "grad_norm": 16.912872314453125,
      "learning_rate": 4.54009198160368e-05,
      "loss": 1.75,
      "step": 1380
    },
    {
      "epoch": 0.27794441111777646,
      "grad_norm": 9.762338638305664,
      "learning_rate": 4.536759314803706e-05,
      "loss": 1.2588,
      "step": 1390
    },
    {
      "epoch": 0.27994401119776047,
      "grad_norm": 8.185384750366211,
      "learning_rate": 4.5334266480037326e-05,
      "loss": 1.4889,
      "step": 1400
    },
    {
      "epoch": 0.2819436112777445,
      "grad_norm": 12.606782913208008,
      "learning_rate": 4.53009398120376e-05,
      "loss": 1.3724,
      "step": 1410
    },
    {
      "epoch": 0.28394321135772843,
      "grad_norm": 7.541880130767822,
      "learning_rate": 4.526761314403786e-05,
      "loss": 1.3016,
      "step": 1420
    },
    {
      "epoch": 0.28594281143771244,
      "grad_norm": 7.293546676635742,
      "learning_rate": 4.523428647603813e-05,
      "loss": 1.2568,
      "step": 1430
    },
    {
      "epoch": 0.28794241151769645,
      "grad_norm": 10.203044891357422,
      "learning_rate": 4.5200959808038394e-05,
      "loss": 1.2787,
      "step": 1440
    },
    {
      "epoch": 0.28994201159768046,
      "grad_norm": 11.306634902954102,
      "learning_rate": 4.5167633140038664e-05,
      "loss": 1.3075,
      "step": 1450
    },
    {
      "epoch": 0.29194161167766447,
      "grad_norm": 11.130594253540039,
      "learning_rate": 4.513430647203893e-05,
      "loss": 1.4412,
      "step": 1460
    },
    {
      "epoch": 0.2939412117576485,
      "grad_norm": 12.61267375946045,
      "learning_rate": 4.51009798040392e-05,
      "loss": 1.4638,
      "step": 1470
    },
    {
      "epoch": 0.2959408118376325,
      "grad_norm": 11.044858932495117,
      "learning_rate": 4.506765313603946e-05,
      "loss": 1.2142,
      "step": 1480
    },
    {
      "epoch": 0.2979404119176165,
      "grad_norm": 10.749207496643066,
      "learning_rate": 4.5034326468039725e-05,
      "loss": 1.2413,
      "step": 1490
    },
    {
      "epoch": 0.29994001199760045,
      "grad_norm": 13.847024917602539,
      "learning_rate": 4.5000999800039995e-05,
      "loss": 1.3353,
      "step": 1500
    },
    {
      "epoch": 0.30193961207758446,
      "grad_norm": 7.468137741088867,
      "learning_rate": 4.496767313204026e-05,
      "loss": 1.2833,
      "step": 1510
    },
    {
      "epoch": 0.3039392121575685,
      "grad_norm": 7.363351821899414,
      "learning_rate": 4.493434646404053e-05,
      "loss": 1.2521,
      "step": 1520
    },
    {
      "epoch": 0.3059388122375525,
      "grad_norm": 11.773109436035156,
      "learning_rate": 4.490101979604079e-05,
      "loss": 1.5557,
      "step": 1530
    },
    {
      "epoch": 0.3079384123175365,
      "grad_norm": 6.45768404006958,
      "learning_rate": 4.486769312804106e-05,
      "loss": 1.1955,
      "step": 1540
    },
    {
      "epoch": 0.3099380123975205,
      "grad_norm": 10.601333618164062,
      "learning_rate": 4.4834366460041326e-05,
      "loss": 1.4657,
      "step": 1550
    },
    {
      "epoch": 0.3119376124775045,
      "grad_norm": 12.380545616149902,
      "learning_rate": 4.4801039792041597e-05,
      "loss": 1.3831,
      "step": 1560
    },
    {
      "epoch": 0.3139372125574885,
      "grad_norm": 9.64351749420166,
      "learning_rate": 4.476771312404186e-05,
      "loss": 1.4464,
      "step": 1570
    },
    {
      "epoch": 0.31593681263747253,
      "grad_norm": 10.940901756286621,
      "learning_rate": 4.4734386456042124e-05,
      "loss": 1.404,
      "step": 1580
    },
    {
      "epoch": 0.3179364127174565,
      "grad_norm": 14.800042152404785,
      "learning_rate": 4.4701059788042394e-05,
      "loss": 1.2978,
      "step": 1590
    },
    {
      "epoch": 0.3199360127974405,
      "grad_norm": 8.449132919311523,
      "learning_rate": 4.466773312004266e-05,
      "loss": 1.526,
      "step": 1600
    },
    {
      "epoch": 0.3219356128774245,
      "grad_norm": 12.05859661102295,
      "learning_rate": 4.463440645204293e-05,
      "loss": 1.314,
      "step": 1610
    },
    {
      "epoch": 0.3239352129574085,
      "grad_norm": 13.708454132080078,
      "learning_rate": 4.460107978404319e-05,
      "loss": 1.4002,
      "step": 1620
    },
    {
      "epoch": 0.3259348130373925,
      "grad_norm": 13.73060131072998,
      "learning_rate": 4.456775311604346e-05,
      "loss": 1.3451,
      "step": 1630
    },
    {
      "epoch": 0.32793441311737653,
      "grad_norm": 12.938565254211426,
      "learning_rate": 4.4534426448043725e-05,
      "loss": 1.55,
      "step": 1640
    },
    {
      "epoch": 0.32993401319736054,
      "grad_norm": 12.568390846252441,
      "learning_rate": 4.4501099780043995e-05,
      "loss": 1.447,
      "step": 1650
    },
    {
      "epoch": 0.33193361327734455,
      "grad_norm": 15.179072380065918,
      "learning_rate": 4.446777311204426e-05,
      "loss": 1.4206,
      "step": 1660
    },
    {
      "epoch": 0.3339332133573285,
      "grad_norm": 10.558350563049316,
      "learning_rate": 4.443444644404452e-05,
      "loss": 1.328,
      "step": 1670
    },
    {
      "epoch": 0.3359328134373125,
      "grad_norm": 7.207569122314453,
      "learning_rate": 4.440111977604479e-05,
      "loss": 1.4097,
      "step": 1680
    },
    {
      "epoch": 0.3379324135172965,
      "grad_norm": 13.435464859008789,
      "learning_rate": 4.4367793108045056e-05,
      "loss": 1.3629,
      "step": 1690
    },
    {
      "epoch": 0.33993201359728054,
      "grad_norm": 10.228316307067871,
      "learning_rate": 4.4334466440045326e-05,
      "loss": 1.3451,
      "step": 1700
    },
    {
      "epoch": 0.34193161367726455,
      "grad_norm": 10.333452224731445,
      "learning_rate": 4.430113977204559e-05,
      "loss": 1.4554,
      "step": 1710
    },
    {
      "epoch": 0.34393121375724856,
      "grad_norm": 7.805350303649902,
      "learning_rate": 4.426781310404586e-05,
      "loss": 1.4457,
      "step": 1720
    },
    {
      "epoch": 0.34593081383723256,
      "grad_norm": 14.084467887878418,
      "learning_rate": 4.4234486436046124e-05,
      "loss": 1.326,
      "step": 1730
    },
    {
      "epoch": 0.3479304139172166,
      "grad_norm": 6.707535266876221,
      "learning_rate": 4.4201159768046394e-05,
      "loss": 1.5048,
      "step": 1740
    },
    {
      "epoch": 0.3499300139972006,
      "grad_norm": 7.939605236053467,
      "learning_rate": 4.416783310004666e-05,
      "loss": 1.1874,
      "step": 1750
    },
    {
      "epoch": 0.35192961407718454,
      "grad_norm": 10.233711242675781,
      "learning_rate": 4.413450643204692e-05,
      "loss": 1.2484,
      "step": 1760
    },
    {
      "epoch": 0.35392921415716855,
      "grad_norm": 15.285201072692871,
      "learning_rate": 4.410117976404719e-05,
      "loss": 1.2695,
      "step": 1770
    },
    {
      "epoch": 0.35592881423715256,
      "grad_norm": 12.951933860778809,
      "learning_rate": 4.4067853096047455e-05,
      "loss": 1.4355,
      "step": 1780
    },
    {
      "epoch": 0.35792841431713657,
      "grad_norm": 15.830033302307129,
      "learning_rate": 4.4034526428047725e-05,
      "loss": 1.4417,
      "step": 1790
    },
    {
      "epoch": 0.3599280143971206,
      "grad_norm": 7.617671489715576,
      "learning_rate": 4.4001199760047995e-05,
      "loss": 1.3658,
      "step": 1800
    },
    {
      "epoch": 0.3619276144771046,
      "grad_norm": 14.878056526184082,
      "learning_rate": 4.396787309204826e-05,
      "loss": 1.2227,
      "step": 1810
    },
    {
      "epoch": 0.3639272145570886,
      "grad_norm": 13.695913314819336,
      "learning_rate": 4.393454642404853e-05,
      "loss": 1.4606,
      "step": 1820
    },
    {
      "epoch": 0.3659268146370726,
      "grad_norm": 12.099872589111328,
      "learning_rate": 4.390121975604879e-05,
      "loss": 1.2938,
      "step": 1830
    },
    {
      "epoch": 0.3679264147170566,
      "grad_norm": 10.036174774169922,
      "learning_rate": 4.386789308804906e-05,
      "loss": 1.3093,
      "step": 1840
    },
    {
      "epoch": 0.36992601479704057,
      "grad_norm": 6.709839820861816,
      "learning_rate": 4.3834566420049326e-05,
      "loss": 1.4853,
      "step": 1850
    },
    {
      "epoch": 0.3719256148770246,
      "grad_norm": 4.925860404968262,
      "learning_rate": 4.3801239752049597e-05,
      "loss": 1.4173,
      "step": 1860
    },
    {
      "epoch": 0.3739252149570086,
      "grad_norm": 11.966093063354492,
      "learning_rate": 4.376791308404986e-05,
      "loss": 1.2895,
      "step": 1870
    },
    {
      "epoch": 0.3759248150369926,
      "grad_norm": 10.420259475708008,
      "learning_rate": 4.373458641605013e-05,
      "loss": 1.3862,
      "step": 1880
    },
    {
      "epoch": 0.3779244151169766,
      "grad_norm": 15.055235862731934,
      "learning_rate": 4.3701259748050394e-05,
      "loss": 1.4851,
      "step": 1890
    },
    {
      "epoch": 0.3799240151969606,
      "grad_norm": 16.069576263427734,
      "learning_rate": 4.366793308005066e-05,
      "loss": 1.4612,
      "step": 1900
    },
    {
      "epoch": 0.38192361527694463,
      "grad_norm": 9.623614311218262,
      "learning_rate": 4.363460641205093e-05,
      "loss": 1.3047,
      "step": 1910
    },
    {
      "epoch": 0.38392321535692864,
      "grad_norm": 6.300931930541992,
      "learning_rate": 4.360127974405119e-05,
      "loss": 1.5649,
      "step": 1920
    },
    {
      "epoch": 0.3859228154369126,
      "grad_norm": 12.016615867614746,
      "learning_rate": 4.356795307605146e-05,
      "loss": 1.3232,
      "step": 1930
    },
    {
      "epoch": 0.3879224155168966,
      "grad_norm": 11.33920669555664,
      "learning_rate": 4.3534626408051725e-05,
      "loss": 1.4063,
      "step": 1940
    },
    {
      "epoch": 0.3899220155968806,
      "grad_norm": 8.669095039367676,
      "learning_rate": 4.3501299740051995e-05,
      "loss": 1.4773,
      "step": 1950
    },
    {
      "epoch": 0.3919216156768646,
      "grad_norm": 10.304688453674316,
      "learning_rate": 4.346797307205226e-05,
      "loss": 1.196,
      "step": 1960
    },
    {
      "epoch": 0.39392121575684863,
      "grad_norm": 8.115824699401855,
      "learning_rate": 4.343464640405253e-05,
      "loss": 1.1125,
      "step": 1970
    },
    {
      "epoch": 0.39592081583683264,
      "grad_norm": 7.063497066497803,
      "learning_rate": 4.340131973605279e-05,
      "loss": 1.3435,
      "step": 1980
    },
    {
      "epoch": 0.39792041591681665,
      "grad_norm": 8.129348754882812,
      "learning_rate": 4.3367993068053056e-05,
      "loss": 1.1309,
      "step": 1990
    },
    {
      "epoch": 0.39992001599680066,
      "grad_norm": 8.476826667785645,
      "learning_rate": 4.3334666400053326e-05,
      "loss": 1.4277,
      "step": 2000
    },
    {
      "epoch": 0.40191961607678467,
      "grad_norm": 14.241874694824219,
      "learning_rate": 4.330133973205359e-05,
      "loss": 1.2404,
      "step": 2010
    },
    {
      "epoch": 0.4039192161567686,
      "grad_norm": 7.7841315269470215,
      "learning_rate": 4.326801306405386e-05,
      "loss": 1.0315,
      "step": 2020
    },
    {
      "epoch": 0.40591881623675263,
      "grad_norm": 9.687084197998047,
      "learning_rate": 4.3234686396054124e-05,
      "loss": 1.3285,
      "step": 2030
    },
    {
      "epoch": 0.40791841631673664,
      "grad_norm": 11.329412460327148,
      "learning_rate": 4.3201359728054394e-05,
      "loss": 1.3635,
      "step": 2040
    },
    {
      "epoch": 0.40991801639672065,
      "grad_norm": 9.064562797546387,
      "learning_rate": 4.316803306005466e-05,
      "loss": 1.541,
      "step": 2050
    },
    {
      "epoch": 0.41191761647670466,
      "grad_norm": 7.466919422149658,
      "learning_rate": 4.313470639205493e-05,
      "loss": 1.2689,
      "step": 2060
    },
    {
      "epoch": 0.41391721655668867,
      "grad_norm": 8.256321907043457,
      "learning_rate": 4.310137972405519e-05,
      "loss": 1.6079,
      "step": 2070
    },
    {
      "epoch": 0.4159168166366727,
      "grad_norm": 8.191024780273438,
      "learning_rate": 4.3068053056055455e-05,
      "loss": 1.2105,
      "step": 2080
    },
    {
      "epoch": 0.4179164167166567,
      "grad_norm": 16.89287567138672,
      "learning_rate": 4.3034726388055725e-05,
      "loss": 1.3279,
      "step": 2090
    },
    {
      "epoch": 0.41991601679664065,
      "grad_norm": 6.810368537902832,
      "learning_rate": 4.300139972005599e-05,
      "loss": 1.3173,
      "step": 2100
    },
    {
      "epoch": 0.42191561687662466,
      "grad_norm": 6.354698657989502,
      "learning_rate": 4.296807305205626e-05,
      "loss": 1.3385,
      "step": 2110
    },
    {
      "epoch": 0.42391521695660866,
      "grad_norm": 12.645870208740234,
      "learning_rate": 4.293474638405652e-05,
      "loss": 1.4435,
      "step": 2120
    },
    {
      "epoch": 0.4259148170365927,
      "grad_norm": 9.203939437866211,
      "learning_rate": 4.290141971605679e-05,
      "loss": 1.4086,
      "step": 2130
    },
    {
      "epoch": 0.4279144171165767,
      "grad_norm": 12.003053665161133,
      "learning_rate": 4.2868093048057056e-05,
      "loss": 1.3651,
      "step": 2140
    },
    {
      "epoch": 0.4299140171965607,
      "grad_norm": 7.764573574066162,
      "learning_rate": 4.2834766380057326e-05,
      "loss": 1.4471,
      "step": 2150
    },
    {
      "epoch": 0.4319136172765447,
      "grad_norm": 9.585809707641602,
      "learning_rate": 4.280143971205759e-05,
      "loss": 1.3044,
      "step": 2160
    },
    {
      "epoch": 0.4339132173565287,
      "grad_norm": 19.769412994384766,
      "learning_rate": 4.276811304405785e-05,
      "loss": 1.2034,
      "step": 2170
    },
    {
      "epoch": 0.4359128174365127,
      "grad_norm": 10.261392593383789,
      "learning_rate": 4.2734786376058123e-05,
      "loss": 1.3423,
      "step": 2180
    },
    {
      "epoch": 0.4379124175164967,
      "grad_norm": 6.7034478187561035,
      "learning_rate": 4.270145970805839e-05,
      "loss": 1.1584,
      "step": 2190
    },
    {
      "epoch": 0.4399120175964807,
      "grad_norm": 13.299567222595215,
      "learning_rate": 4.266813304005866e-05,
      "loss": 1.5279,
      "step": 2200
    },
    {
      "epoch": 0.4419116176764647,
      "grad_norm": 8.817646980285645,
      "learning_rate": 4.263480637205892e-05,
      "loss": 1.4097,
      "step": 2210
    },
    {
      "epoch": 0.4439112177564487,
      "grad_norm": 10.895479202270508,
      "learning_rate": 4.260147970405919e-05,
      "loss": 1.1805,
      "step": 2220
    },
    {
      "epoch": 0.4459108178364327,
      "grad_norm": 7.209839344024658,
      "learning_rate": 4.2568153036059455e-05,
      "loss": 1.0239,
      "step": 2230
    },
    {
      "epoch": 0.4479104179164167,
      "grad_norm": 10.350363731384277,
      "learning_rate": 4.2534826368059725e-05,
      "loss": 1.4044,
      "step": 2240
    },
    {
      "epoch": 0.44991001799640074,
      "grad_norm": 9.756516456604004,
      "learning_rate": 4.250149970005999e-05,
      "loss": 1.1526,
      "step": 2250
    },
    {
      "epoch": 0.45190961807638474,
      "grad_norm": 11.944194793701172,
      "learning_rate": 4.246817303206025e-05,
      "loss": 1.6343,
      "step": 2260
    },
    {
      "epoch": 0.4539092181563687,
      "grad_norm": 11.264093399047852,
      "learning_rate": 4.243484636406052e-05,
      "loss": 1.2512,
      "step": 2270
    },
    {
      "epoch": 0.4559088182363527,
      "grad_norm": 9.32564926147461,
      "learning_rate": 4.2401519696060786e-05,
      "loss": 1.2642,
      "step": 2280
    },
    {
      "epoch": 0.4579084183163367,
      "grad_norm": 10.246231079101562,
      "learning_rate": 4.2368193028061056e-05,
      "loss": 1.2339,
      "step": 2290
    },
    {
      "epoch": 0.45990801839632073,
      "grad_norm": 7.710769176483154,
      "learning_rate": 4.233486636006132e-05,
      "loss": 1.1466,
      "step": 2300
    },
    {
      "epoch": 0.46190761847630474,
      "grad_norm": 10.273658752441406,
      "learning_rate": 4.230153969206159e-05,
      "loss": 1.3991,
      "step": 2310
    },
    {
      "epoch": 0.46390721855628875,
      "grad_norm": 9.619648933410645,
      "learning_rate": 4.226821302406186e-05,
      "loss": 1.2619,
      "step": 2320
    },
    {
      "epoch": 0.46590681863627276,
      "grad_norm": 5.6224493980407715,
      "learning_rate": 4.2234886356062123e-05,
      "loss": 1.1529,
      "step": 2330
    },
    {
      "epoch": 0.46790641871625677,
      "grad_norm": 6.693117618560791,
      "learning_rate": 4.2201559688062394e-05,
      "loss": 1.0262,
      "step": 2340
    },
    {
      "epoch": 0.4699060187962408,
      "grad_norm": 12.3672513961792,
      "learning_rate": 4.216823302006266e-05,
      "loss": 1.2952,
      "step": 2350
    },
    {
      "epoch": 0.47190561887622473,
      "grad_norm": 14.722169876098633,
      "learning_rate": 4.213490635206293e-05,
      "loss": 1.1985,
      "step": 2360
    },
    {
      "epoch": 0.47390521895620874,
      "grad_norm": 8.191874504089355,
      "learning_rate": 4.210157968406319e-05,
      "loss": 1.5322,
      "step": 2370
    },
    {
      "epoch": 0.47590481903619275,
      "grad_norm": 6.439329624176025,
      "learning_rate": 4.206825301606346e-05,
      "loss": 1.3296,
      "step": 2380
    },
    {
      "epoch": 0.47790441911617676,
      "grad_norm": 5.884023189544678,
      "learning_rate": 4.2034926348063725e-05,
      "loss": 1.2206,
      "step": 2390
    },
    {
      "epoch": 0.47990401919616077,
      "grad_norm": 11.148348808288574,
      "learning_rate": 4.200159968006399e-05,
      "loss": 1.4249,
      "step": 2400
    },
    {
      "epoch": 0.4819036192761448,
      "grad_norm": 10.005032539367676,
      "learning_rate": 4.196827301206426e-05,
      "loss": 1.0862,
      "step": 2410
    },
    {
      "epoch": 0.4839032193561288,
      "grad_norm": 6.979532241821289,
      "learning_rate": 4.193494634406452e-05,
      "loss": 1.2102,
      "step": 2420
    },
    {
      "epoch": 0.4859028194361128,
      "grad_norm": 10.417128562927246,
      "learning_rate": 4.190161967606479e-05,
      "loss": 1.3869,
      "step": 2430
    },
    {
      "epoch": 0.4879024195160968,
      "grad_norm": 9.855016708374023,
      "learning_rate": 4.1868293008065056e-05,
      "loss": 1.4188,
      "step": 2440
    },
    {
      "epoch": 0.48990201959608076,
      "grad_norm": 8.409106254577637,
      "learning_rate": 4.1834966340065326e-05,
      "loss": 1.3839,
      "step": 2450
    },
    {
      "epoch": 0.49190161967606477,
      "grad_norm": 9.7792329788208,
      "learning_rate": 4.180163967206559e-05,
      "loss": 1.3037,
      "step": 2460
    },
    {
      "epoch": 0.4939012197560488,
      "grad_norm": 10.044366836547852,
      "learning_rate": 4.176831300406586e-05,
      "loss": 1.2748,
      "step": 2470
    },
    {
      "epoch": 0.4959008198360328,
      "grad_norm": 9.707967758178711,
      "learning_rate": 4.1734986336066123e-05,
      "loss": 1.2917,
      "step": 2480
    },
    {
      "epoch": 0.4979004199160168,
      "grad_norm": 10.713278770446777,
      "learning_rate": 4.170165966806639e-05,
      "loss": 1.2926,
      "step": 2490
    },
    {
      "epoch": 0.4999000199960008,
      "grad_norm": 8.194883346557617,
      "learning_rate": 4.166833300006666e-05,
      "loss": 1.3953,
      "step": 2500
    },
    {
      "epoch": 0.5018996200759848,
      "grad_norm": 12.108803749084473,
      "learning_rate": 4.163500633206692e-05,
      "loss": 1.2468,
      "step": 2510
    },
    {
      "epoch": 0.5038992201559688,
      "grad_norm": 8.096705436706543,
      "learning_rate": 4.160167966406719e-05,
      "loss": 1.5883,
      "step": 2520
    },
    {
      "epoch": 0.5058988202359528,
      "grad_norm": 6.243859767913818,
      "learning_rate": 4.1568352996067454e-05,
      "loss": 1.5474,
      "step": 2530
    },
    {
      "epoch": 0.5078984203159368,
      "grad_norm": 7.659424304962158,
      "learning_rate": 4.1535026328067725e-05,
      "loss": 1.5919,
      "step": 2540
    },
    {
      "epoch": 0.5098980203959208,
      "grad_norm": 8.2278470993042,
      "learning_rate": 4.150169966006799e-05,
      "loss": 1.1875,
      "step": 2550
    },
    {
      "epoch": 0.5118976204759048,
      "grad_norm": 10.82964038848877,
      "learning_rate": 4.146837299206825e-05,
      "loss": 1.5497,
      "step": 2560
    },
    {
      "epoch": 0.5138972205558888,
      "grad_norm": 8.458332061767578,
      "learning_rate": 4.143504632406852e-05,
      "loss": 1.2138,
      "step": 2570
    },
    {
      "epoch": 0.5158968206358728,
      "grad_norm": 10.303706169128418,
      "learning_rate": 4.1401719656068786e-05,
      "loss": 1.167,
      "step": 2580
    },
    {
      "epoch": 0.5178964207158568,
      "grad_norm": 8.409741401672363,
      "learning_rate": 4.1368392988069056e-05,
      "loss": 1.3977,
      "step": 2590
    },
    {
      "epoch": 0.5198960207958409,
      "grad_norm": 9.65929126739502,
      "learning_rate": 4.133506632006932e-05,
      "loss": 1.4502,
      "step": 2600
    },
    {
      "epoch": 0.5218956208758249,
      "grad_norm": 9.120354652404785,
      "learning_rate": 4.130173965206959e-05,
      "loss": 1.2112,
      "step": 2610
    },
    {
      "epoch": 0.5238952209558089,
      "grad_norm": 11.970780372619629,
      "learning_rate": 4.126841298406985e-05,
      "loss": 1.2607,
      "step": 2620
    },
    {
      "epoch": 0.5258948210357929,
      "grad_norm": 11.290714263916016,
      "learning_rate": 4.123508631607012e-05,
      "loss": 1.3214,
      "step": 2630
    },
    {
      "epoch": 0.5278944211157769,
      "grad_norm": 8.56667709350586,
      "learning_rate": 4.120175964807039e-05,
      "loss": 1.2839,
      "step": 2640
    },
    {
      "epoch": 0.5298940211957609,
      "grad_norm": 14.916085243225098,
      "learning_rate": 4.116843298007065e-05,
      "loss": 1.644,
      "step": 2650
    },
    {
      "epoch": 0.5318936212757448,
      "grad_norm": 19.220802307128906,
      "learning_rate": 4.113510631207092e-05,
      "loss": 1.3359,
      "step": 2660
    },
    {
      "epoch": 0.5338932213557288,
      "grad_norm": 10.924009323120117,
      "learning_rate": 4.1101779644071184e-05,
      "loss": 1.222,
      "step": 2670
    },
    {
      "epoch": 0.5358928214357128,
      "grad_norm": 7.355231761932373,
      "learning_rate": 4.1068452976071454e-05,
      "loss": 1.3494,
      "step": 2680
    },
    {
      "epoch": 0.5378924215156968,
      "grad_norm": 11.765384674072266,
      "learning_rate": 4.103512630807172e-05,
      "loss": 1.2495,
      "step": 2690
    },
    {
      "epoch": 0.5398920215956808,
      "grad_norm": 17.74324607849121,
      "learning_rate": 4.100179964007199e-05,
      "loss": 1.3723,
      "step": 2700
    },
    {
      "epoch": 0.5418916216756648,
      "grad_norm": 8.6763334274292,
      "learning_rate": 4.096847297207225e-05,
      "loss": 1.2759,
      "step": 2710
    },
    {
      "epoch": 0.5438912217556489,
      "grad_norm": 11.438884735107422,
      "learning_rate": 4.093514630407252e-05,
      "loss": 1.2887,
      "step": 2720
    },
    {
      "epoch": 0.5458908218356329,
      "grad_norm": 13.431958198547363,
      "learning_rate": 4.0901819636072785e-05,
      "loss": 1.1635,
      "step": 2730
    },
    {
      "epoch": 0.5478904219156169,
      "grad_norm": 13.024909973144531,
      "learning_rate": 4.086849296807305e-05,
      "loss": 1.3041,
      "step": 2740
    },
    {
      "epoch": 0.5498900219956009,
      "grad_norm": 12.283001899719238,
      "learning_rate": 4.083516630007332e-05,
      "loss": 1.3644,
      "step": 2750
    },
    {
      "epoch": 0.5518896220755849,
      "grad_norm": 7.722893238067627,
      "learning_rate": 4.080183963207358e-05,
      "loss": 1.2789,
      "step": 2760
    },
    {
      "epoch": 0.5538892221555689,
      "grad_norm": 9.883880615234375,
      "learning_rate": 4.076851296407385e-05,
      "loss": 1.1748,
      "step": 2770
    },
    {
      "epoch": 0.5558888222355529,
      "grad_norm": 6.70943021774292,
      "learning_rate": 4.0735186296074117e-05,
      "loss": 1.5044,
      "step": 2780
    },
    {
      "epoch": 0.5578884223155369,
      "grad_norm": 12.161844253540039,
      "learning_rate": 4.070185962807439e-05,
      "loss": 1.2099,
      "step": 2790
    },
    {
      "epoch": 0.5598880223955209,
      "grad_norm": 10.237136840820312,
      "learning_rate": 4.066853296007465e-05,
      "loss": 1.2317,
      "step": 2800
    },
    {
      "epoch": 0.5618876224755049,
      "grad_norm": 15.287521362304688,
      "learning_rate": 4.063520629207492e-05,
      "loss": 1.2061,
      "step": 2810
    },
    {
      "epoch": 0.563887222555489,
      "grad_norm": 14.262770652770996,
      "learning_rate": 4.0601879624075184e-05,
      "loss": 1.1119,
      "step": 2820
    },
    {
      "epoch": 0.5658868226354729,
      "grad_norm": 6.919824600219727,
      "learning_rate": 4.0568552956075454e-05,
      "loss": 1.3667,
      "step": 2830
    },
    {
      "epoch": 0.5678864227154569,
      "grad_norm": 14.183902740478516,
      "learning_rate": 4.0535226288075725e-05,
      "loss": 1.3654,
      "step": 2840
    },
    {
      "epoch": 0.5698860227954409,
      "grad_norm": 9.950738906860352,
      "learning_rate": 4.050189962007599e-05,
      "loss": 1.5347,
      "step": 2850
    },
    {
      "epoch": 0.5718856228754249,
      "grad_norm": 13.368810653686523,
      "learning_rate": 4.046857295207626e-05,
      "loss": 1.049,
      "step": 2860
    },
    {
      "epoch": 0.5738852229554089,
      "grad_norm": 8.22337532043457,
      "learning_rate": 4.043524628407652e-05,
      "loss": 1.3517,
      "step": 2870
    },
    {
      "epoch": 0.5758848230353929,
      "grad_norm": 7.839128017425537,
      "learning_rate": 4.0401919616076785e-05,
      "loss": 1.1669,
      "step": 2880
    },
    {
      "epoch": 0.5778844231153769,
      "grad_norm": 7.4061970710754395,
      "learning_rate": 4.0368592948077056e-05,
      "loss": 1.0605,
      "step": 2890
    },
    {
      "epoch": 0.5798840231953609,
      "grad_norm": 9.438356399536133,
      "learning_rate": 4.033526628007732e-05,
      "loss": 1.6649,
      "step": 2900
    },
    {
      "epoch": 0.5818836232753449,
      "grad_norm": 11.02318286895752,
      "learning_rate": 4.030193961207759e-05,
      "loss": 1.1835,
      "step": 2910
    },
    {
      "epoch": 0.5838832233553289,
      "grad_norm": 9.416038513183594,
      "learning_rate": 4.026861294407785e-05,
      "loss": 1.3389,
      "step": 2920
    },
    {
      "epoch": 0.585882823435313,
      "grad_norm": 8.444393157958984,
      "learning_rate": 4.023528627607812e-05,
      "loss": 1.3769,
      "step": 2930
    },
    {
      "epoch": 0.587882423515297,
      "grad_norm": 6.090178966522217,
      "learning_rate": 4.020195960807839e-05,
      "loss": 1.2626,
      "step": 2940
    },
    {
      "epoch": 0.589882023595281,
      "grad_norm": 14.910218238830566,
      "learning_rate": 4.016863294007866e-05,
      "loss": 1.176,
      "step": 2950
    },
    {
      "epoch": 0.591881623675265,
      "grad_norm": 16.014402389526367,
      "learning_rate": 4.013530627207892e-05,
      "loss": 1.1762,
      "step": 2960
    },
    {
      "epoch": 0.593881223755249,
      "grad_norm": 6.923309326171875,
      "learning_rate": 4.0101979604079184e-05,
      "loss": 1.0968,
      "step": 2970
    },
    {
      "epoch": 0.595880823835233,
      "grad_norm": 17.23565101623535,
      "learning_rate": 4.0068652936079454e-05,
      "loss": 1.199,
      "step": 2980
    },
    {
      "epoch": 0.597880423915217,
      "grad_norm": 12.468880653381348,
      "learning_rate": 4.003532626807972e-05,
      "loss": 1.3443,
      "step": 2990
    },
    {
      "epoch": 0.5998800239952009,
      "grad_norm": 10.071290969848633,
      "learning_rate": 4.000199960007999e-05,
      "loss": 1.1263,
      "step": 3000
    },
    {
      "epoch": 0.6018796240751849,
      "grad_norm": 10.354352951049805,
      "learning_rate": 3.996867293208025e-05,
      "loss": 1.3717,
      "step": 3010
    },
    {
      "epoch": 0.6038792241551689,
      "grad_norm": 11.601980209350586,
      "learning_rate": 3.993534626408052e-05,
      "loss": 1.1071,
      "step": 3020
    },
    {
      "epoch": 0.6058788242351529,
      "grad_norm": 18.797748565673828,
      "learning_rate": 3.9902019596080785e-05,
      "loss": 1.3429,
      "step": 3030
    },
    {
      "epoch": 0.607878424315137,
      "grad_norm": 9.523914337158203,
      "learning_rate": 3.9868692928081056e-05,
      "loss": 1.3549,
      "step": 3040
    },
    {
      "epoch": 0.609878024395121,
      "grad_norm": 20.726600646972656,
      "learning_rate": 3.983536626008132e-05,
      "loss": 1.219,
      "step": 3050
    },
    {
      "epoch": 0.611877624475105,
      "grad_norm": 13.368647575378418,
      "learning_rate": 3.980203959208158e-05,
      "loss": 1.3893,
      "step": 3060
    },
    {
      "epoch": 0.613877224555089,
      "grad_norm": 10.923848152160645,
      "learning_rate": 3.976871292408185e-05,
      "loss": 1.2772,
      "step": 3070
    },
    {
      "epoch": 0.615876824635073,
      "grad_norm": 7.875560760498047,
      "learning_rate": 3.9735386256082116e-05,
      "loss": 1.1633,
      "step": 3080
    },
    {
      "epoch": 0.617876424715057,
      "grad_norm": 11.409017562866211,
      "learning_rate": 3.970205958808239e-05,
      "loss": 1.0883,
      "step": 3090
    },
    {
      "epoch": 0.619876024795041,
      "grad_norm": 7.789727687835693,
      "learning_rate": 3.966873292008265e-05,
      "loss": 1.3719,
      "step": 3100
    },
    {
      "epoch": 0.621875624875025,
      "grad_norm": 8.09294319152832,
      "learning_rate": 3.963540625208292e-05,
      "loss": 1.1676,
      "step": 3110
    },
    {
      "epoch": 0.623875224955009,
      "grad_norm": 9.983613014221191,
      "learning_rate": 3.9602079584083184e-05,
      "loss": 1.2901,
      "step": 3120
    },
    {
      "epoch": 0.625874825034993,
      "grad_norm": 16.45145034790039,
      "learning_rate": 3.9568752916083454e-05,
      "loss": 1.4913,
      "step": 3130
    },
    {
      "epoch": 0.627874425114977,
      "grad_norm": 13.140570640563965,
      "learning_rate": 3.953542624808372e-05,
      "loss": 1.3606,
      "step": 3140
    },
    {
      "epoch": 0.629874025194961,
      "grad_norm": 9.14824104309082,
      "learning_rate": 3.950209958008398e-05,
      "loss": 0.9801,
      "step": 3150
    },
    {
      "epoch": 0.6318736252749451,
      "grad_norm": 10.571456909179688,
      "learning_rate": 3.946877291208425e-05,
      "loss": 1.3986,
      "step": 3160
    },
    {
      "epoch": 0.633873225354929,
      "grad_norm": 9.13987922668457,
      "learning_rate": 3.9435446244084515e-05,
      "loss": 1.2015,
      "step": 3170
    },
    {
      "epoch": 0.635872825434913,
      "grad_norm": 19.51978874206543,
      "learning_rate": 3.9402119576084785e-05,
      "loss": 1.3393,
      "step": 3180
    },
    {
      "epoch": 0.637872425514897,
      "grad_norm": 7.830604553222656,
      "learning_rate": 3.936879290808505e-05,
      "loss": 1.4424,
      "step": 3190
    },
    {
      "epoch": 0.639872025594881,
      "grad_norm": 13.370462417602539,
      "learning_rate": 3.933546624008532e-05,
      "loss": 1.2709,
      "step": 3200
    },
    {
      "epoch": 0.641871625674865,
      "grad_norm": 10.603973388671875,
      "learning_rate": 3.930213957208558e-05,
      "loss": 1.4042,
      "step": 3210
    },
    {
      "epoch": 0.643871225754849,
      "grad_norm": 10.764043807983398,
      "learning_rate": 3.926881290408585e-05,
      "loss": 1.1157,
      "step": 3220
    },
    {
      "epoch": 0.645870825834833,
      "grad_norm": 13.663467407226562,
      "learning_rate": 3.9235486236086116e-05,
      "loss": 1.1413,
      "step": 3230
    },
    {
      "epoch": 0.647870425914817,
      "grad_norm": 9.756425857543945,
      "learning_rate": 3.920215956808638e-05,
      "loss": 1.2439,
      "step": 3240
    },
    {
      "epoch": 0.649870025994801,
      "grad_norm": 7.393795490264893,
      "learning_rate": 3.916883290008665e-05,
      "loss": 0.9973,
      "step": 3250
    },
    {
      "epoch": 0.651869626074785,
      "grad_norm": 11.991584777832031,
      "learning_rate": 3.9135506232086914e-05,
      "loss": 1.1343,
      "step": 3260
    },
    {
      "epoch": 0.6538692261547691,
      "grad_norm": 7.072484493255615,
      "learning_rate": 3.9102179564087184e-05,
      "loss": 1.1824,
      "step": 3270
    },
    {
      "epoch": 0.6558688262347531,
      "grad_norm": 14.368646621704102,
      "learning_rate": 3.906885289608745e-05,
      "loss": 1.3315,
      "step": 3280
    },
    {
      "epoch": 0.6578684263147371,
      "grad_norm": 12.82465934753418,
      "learning_rate": 3.903552622808772e-05,
      "loss": 1.2157,
      "step": 3290
    },
    {
      "epoch": 0.6598680263947211,
      "grad_norm": 13.874706268310547,
      "learning_rate": 3.900219956008798e-05,
      "loss": 1.1154,
      "step": 3300
    },
    {
      "epoch": 0.6618676264747051,
      "grad_norm": 11.70471477508545,
      "learning_rate": 3.896887289208825e-05,
      "loss": 1.4185,
      "step": 3310
    },
    {
      "epoch": 0.6638672265546891,
      "grad_norm": 14.185334205627441,
      "learning_rate": 3.8935546224088515e-05,
      "loss": 1.3941,
      "step": 3320
    },
    {
      "epoch": 0.6658668266346731,
      "grad_norm": 11.32946491241455,
      "learning_rate": 3.890221955608878e-05,
      "loss": 1.4729,
      "step": 3330
    },
    {
      "epoch": 0.667866426714657,
      "grad_norm": 8.55795669555664,
      "learning_rate": 3.8868892888089056e-05,
      "loss": 1.2327,
      "step": 3340
    },
    {
      "epoch": 0.669866026794641,
      "grad_norm": 18.567283630371094,
      "learning_rate": 3.883556622008932e-05,
      "loss": 1.3003,
      "step": 3350
    },
    {
      "epoch": 0.671865626874625,
      "grad_norm": 14.00277328491211,
      "learning_rate": 3.880223955208959e-05,
      "loss": 1.3131,
      "step": 3360
    },
    {
      "epoch": 0.673865226954609,
      "grad_norm": 9.731492042541504,
      "learning_rate": 3.876891288408985e-05,
      "loss": 1.3762,
      "step": 3370
    },
    {
      "epoch": 0.675864827034593,
      "grad_norm": 11.062589645385742,
      "learning_rate": 3.8735586216090116e-05,
      "loss": 1.1645,
      "step": 3380
    },
    {
      "epoch": 0.6778644271145771,
      "grad_norm": 12.758499145507812,
      "learning_rate": 3.870225954809039e-05,
      "loss": 1.2923,
      "step": 3390
    },
    {
      "epoch": 0.6798640271945611,
      "grad_norm": 11.518110275268555,
      "learning_rate": 3.866893288009065e-05,
      "loss": 1.2215,
      "step": 3400
    },
    {
      "epoch": 0.6818636272745451,
      "grad_norm": 8.667983055114746,
      "learning_rate": 3.863560621209092e-05,
      "loss": 1.1419,
      "step": 3410
    },
    {
      "epoch": 0.6838632273545291,
      "grad_norm": 10.714900970458984,
      "learning_rate": 3.8602279544091184e-05,
      "loss": 1.4783,
      "step": 3420
    },
    {
      "epoch": 0.6858628274345131,
      "grad_norm": 5.994640827178955,
      "learning_rate": 3.8568952876091454e-05,
      "loss": 1.2491,
      "step": 3430
    },
    {
      "epoch": 0.6878624275144971,
      "grad_norm": 7.661020755767822,
      "learning_rate": 3.853562620809172e-05,
      "loss": 1.2888,
      "step": 3440
    },
    {
      "epoch": 0.6898620275944811,
      "grad_norm": 8.590173721313477,
      "learning_rate": 3.850229954009199e-05,
      "loss": 1.2561,
      "step": 3450
    },
    {
      "epoch": 0.6918616276744651,
      "grad_norm": 6.554612636566162,
      "learning_rate": 3.846897287209225e-05,
      "loss": 1.3874,
      "step": 3460
    },
    {
      "epoch": 0.6938612277544491,
      "grad_norm": 7.30633544921875,
      "learning_rate": 3.8435646204092515e-05,
      "loss": 1.0389,
      "step": 3470
    },
    {
      "epoch": 0.6958608278344331,
      "grad_norm": 9.767383575439453,
      "learning_rate": 3.8402319536092785e-05,
      "loss": 1.1564,
      "step": 3480
    },
    {
      "epoch": 0.6978604279144172,
      "grad_norm": 17.199951171875,
      "learning_rate": 3.836899286809305e-05,
      "loss": 1.3786,
      "step": 3490
    },
    {
      "epoch": 0.6998600279944012,
      "grad_norm": 12.99984073638916,
      "learning_rate": 3.833566620009332e-05,
      "loss": 1.0717,
      "step": 3500
    },
    {
      "epoch": 0.7018596280743852,
      "grad_norm": 10.522557258605957,
      "learning_rate": 3.830233953209358e-05,
      "loss": 1.3302,
      "step": 3510
    },
    {
      "epoch": 0.7038592281543691,
      "grad_norm": 6.328661918640137,
      "learning_rate": 3.826901286409385e-05,
      "loss": 1.0549,
      "step": 3520
    },
    {
      "epoch": 0.7058588282343531,
      "grad_norm": 6.699892997741699,
      "learning_rate": 3.8235686196094116e-05,
      "loss": 1.0214,
      "step": 3530
    },
    {
      "epoch": 0.7078584283143371,
      "grad_norm": 9.971946716308594,
      "learning_rate": 3.820235952809439e-05,
      "loss": 1.4373,
      "step": 3540
    },
    {
      "epoch": 0.7098580283943211,
      "grad_norm": 9.8873291015625,
      "learning_rate": 3.816903286009465e-05,
      "loss": 1.0573,
      "step": 3550
    },
    {
      "epoch": 0.7118576284743051,
      "grad_norm": 11.81954288482666,
      "learning_rate": 3.8135706192094914e-05,
      "loss": 1.1712,
      "step": 3560
    },
    {
      "epoch": 0.7138572285542891,
      "grad_norm": 8.340261459350586,
      "learning_rate": 3.8102379524095184e-05,
      "loss": 1.324,
      "step": 3570
    },
    {
      "epoch": 0.7158568286342731,
      "grad_norm": 9.43123722076416,
      "learning_rate": 3.806905285609545e-05,
      "loss": 1.1287,
      "step": 3580
    },
    {
      "epoch": 0.7178564287142571,
      "grad_norm": 14.495919227600098,
      "learning_rate": 3.803572618809572e-05,
      "loss": 1.4844,
      "step": 3590
    },
    {
      "epoch": 0.7198560287942412,
      "grad_norm": 8.78662395477295,
      "learning_rate": 3.800239952009598e-05,
      "loss": 1.3817,
      "step": 3600
    },
    {
      "epoch": 0.7218556288742252,
      "grad_norm": 9.767730712890625,
      "learning_rate": 3.796907285209625e-05,
      "loss": 0.9198,
      "step": 3610
    },
    {
      "epoch": 0.7238552289542092,
      "grad_norm": 12.570734024047852,
      "learning_rate": 3.7935746184096515e-05,
      "loss": 1.1038,
      "step": 3620
    },
    {
      "epoch": 0.7258548290341932,
      "grad_norm": 21.11334228515625,
      "learning_rate": 3.7902419516096785e-05,
      "loss": 1.1425,
      "step": 3630
    },
    {
      "epoch": 0.7278544291141772,
      "grad_norm": 7.805469512939453,
      "learning_rate": 3.786909284809705e-05,
      "loss": 1.0135,
      "step": 3640
    },
    {
      "epoch": 0.7298540291941612,
      "grad_norm": 8.64013671875,
      "learning_rate": 3.783576618009731e-05,
      "loss": 1.1986,
      "step": 3650
    },
    {
      "epoch": 0.7318536292741452,
      "grad_norm": 14.696758270263672,
      "learning_rate": 3.780243951209758e-05,
      "loss": 1.3546,
      "step": 3660
    },
    {
      "epoch": 0.7338532293541292,
      "grad_norm": 11.276936531066895,
      "learning_rate": 3.7769112844097846e-05,
      "loss": 1.3429,
      "step": 3670
    },
    {
      "epoch": 0.7358528294341132,
      "grad_norm": 17.966609954833984,
      "learning_rate": 3.7735786176098116e-05,
      "loss": 1.0624,
      "step": 3680
    },
    {
      "epoch": 0.7378524295140971,
      "grad_norm": 13.770709037780762,
      "learning_rate": 3.770245950809838e-05,
      "loss": 1.2056,
      "step": 3690
    },
    {
      "epoch": 0.7398520295940811,
      "grad_norm": 6.752033710479736,
      "learning_rate": 3.766913284009865e-05,
      "loss": 1.1658,
      "step": 3700
    },
    {
      "epoch": 0.7418516296740651,
      "grad_norm": 11.799617767333984,
      "learning_rate": 3.7635806172098914e-05,
      "loss": 1.1856,
      "step": 3710
    },
    {
      "epoch": 0.7438512297540492,
      "grad_norm": 17.172138214111328,
      "learning_rate": 3.7602479504099184e-05,
      "loss": 1.2781,
      "step": 3720
    },
    {
      "epoch": 0.7458508298340332,
      "grad_norm": 15.196943283081055,
      "learning_rate": 3.756915283609945e-05,
      "loss": 1.2169,
      "step": 3730
    },
    {
      "epoch": 0.7478504299140172,
      "grad_norm": 4.767385482788086,
      "learning_rate": 3.753582616809971e-05,
      "loss": 1.2243,
      "step": 3740
    },
    {
      "epoch": 0.7498500299940012,
      "grad_norm": 13.462945938110352,
      "learning_rate": 3.750249950009998e-05,
      "loss": 1.4067,
      "step": 3750
    },
    {
      "epoch": 0.7518496300739852,
      "grad_norm": 7.2137370109558105,
      "learning_rate": 3.7469172832100245e-05,
      "loss": 1.2452,
      "step": 3760
    },
    {
      "epoch": 0.7538492301539692,
      "grad_norm": 6.6230058670043945,
      "learning_rate": 3.7435846164100515e-05,
      "loss": 1.0704,
      "step": 3770
    },
    {
      "epoch": 0.7558488302339532,
      "grad_norm": 11.077132225036621,
      "learning_rate": 3.740251949610078e-05,
      "loss": 1.4128,
      "step": 3780
    },
    {
      "epoch": 0.7578484303139372,
      "grad_norm": 15.635919570922852,
      "learning_rate": 3.736919282810105e-05,
      "loss": 1.2471,
      "step": 3790
    },
    {
      "epoch": 0.7598480303939212,
      "grad_norm": 25.102005004882812,
      "learning_rate": 3.733586616010131e-05,
      "loss": 1.2006,
      "step": 3800
    },
    {
      "epoch": 0.7618476304739052,
      "grad_norm": 4.534594535827637,
      "learning_rate": 3.730253949210158e-05,
      "loss": 1.2177,
      "step": 3810
    },
    {
      "epoch": 0.7638472305538893,
      "grad_norm": 8.330376625061035,
      "learning_rate": 3.7269212824101846e-05,
      "loss": 1.2563,
      "step": 3820
    },
    {
      "epoch": 0.7658468306338733,
      "grad_norm": 5.789188861846924,
      "learning_rate": 3.723588615610211e-05,
      "loss": 1.1185,
      "step": 3830
    },
    {
      "epoch": 0.7678464307138573,
      "grad_norm": 11.026089668273926,
      "learning_rate": 3.720255948810238e-05,
      "loss": 1.0352,
      "step": 3840
    },
    {
      "epoch": 0.7698460307938413,
      "grad_norm": 7.858421802520752,
      "learning_rate": 3.716923282010264e-05,
      "loss": 1.1111,
      "step": 3850
    },
    {
      "epoch": 0.7718456308738252,
      "grad_norm": 12.779881477355957,
      "learning_rate": 3.713590615210292e-05,
      "loss": 1.113,
      "step": 3860
    },
    {
      "epoch": 0.7738452309538092,
      "grad_norm": 2.9998302459716797,
      "learning_rate": 3.7102579484103184e-05,
      "loss": 1.2392,
      "step": 3870
    },
    {
      "epoch": 0.7758448310337932,
      "grad_norm": 9.57577133178711,
      "learning_rate": 3.706925281610345e-05,
      "loss": 1.1163,
      "step": 3880
    },
    {
      "epoch": 0.7778444311137772,
      "grad_norm": 9.906031608581543,
      "learning_rate": 3.703592614810372e-05,
      "loss": 1.2483,
      "step": 3890
    },
    {
      "epoch": 0.7798440311937612,
      "grad_norm": 18.57105827331543,
      "learning_rate": 3.700259948010398e-05,
      "loss": 1.2075,
      "step": 3900
    },
    {
      "epoch": 0.7818436312737452,
      "grad_norm": 26.144001007080078,
      "learning_rate": 3.696927281210425e-05,
      "loss": 1.3552,
      "step": 3910
    },
    {
      "epoch": 0.7838432313537292,
      "grad_norm": 11.23480224609375,
      "learning_rate": 3.6935946144104515e-05,
      "loss": 1.2501,
      "step": 3920
    },
    {
      "epoch": 0.7858428314337133,
      "grad_norm": 8.739659309387207,
      "learning_rate": 3.6902619476104785e-05,
      "loss": 0.9989,
      "step": 3930
    },
    {
      "epoch": 0.7878424315136973,
      "grad_norm": 8.682856559753418,
      "learning_rate": 3.686929280810505e-05,
      "loss": 1.0536,
      "step": 3940
    },
    {
      "epoch": 0.7898420315936813,
      "grad_norm": 17.458547592163086,
      "learning_rate": 3.683596614010532e-05,
      "loss": 1.2393,
      "step": 3950
    },
    {
      "epoch": 0.7918416316736653,
      "grad_norm": 11.164270401000977,
      "learning_rate": 3.680263947210558e-05,
      "loss": 1.4945,
      "step": 3960
    },
    {
      "epoch": 0.7938412317536493,
      "grad_norm": 16.356037139892578,
      "learning_rate": 3.6769312804105846e-05,
      "loss": 1.2136,
      "step": 3970
    },
    {
      "epoch": 0.7958408318336333,
      "grad_norm": 8.346263885498047,
      "learning_rate": 3.6735986136106116e-05,
      "loss": 1.113,
      "step": 3980
    },
    {
      "epoch": 0.7978404319136173,
      "grad_norm": 14.635550498962402,
      "learning_rate": 3.670265946810638e-05,
      "loss": 1.3943,
      "step": 3990
    },
    {
      "epoch": 0.7998400319936013,
      "grad_norm": 14.845978736877441,
      "learning_rate": 3.666933280010665e-05,
      "loss": 0.8147,
      "step": 4000
    },
    {
      "epoch": 0.8018396320735853,
      "grad_norm": 14.854844093322754,
      "learning_rate": 3.6636006132106914e-05,
      "loss": 1.1508,
      "step": 4010
    },
    {
      "epoch": 0.8038392321535693,
      "grad_norm": 12.797466278076172,
      "learning_rate": 3.6602679464107184e-05,
      "loss": 1.3471,
      "step": 4020
    },
    {
      "epoch": 0.8058388322335532,
      "grad_norm": 9.929220199584961,
      "learning_rate": 3.656935279610745e-05,
      "loss": 1.273,
      "step": 4030
    },
    {
      "epoch": 0.8078384323135372,
      "grad_norm": 12.258395195007324,
      "learning_rate": 3.653602612810772e-05,
      "loss": 1.3169,
      "step": 4040
    },
    {
      "epoch": 0.8098380323935213,
      "grad_norm": 4.256494045257568,
      "learning_rate": 3.650269946010798e-05,
      "loss": 1.1675,
      "step": 4050
    },
    {
      "epoch": 0.8118376324735053,
      "grad_norm": 11.364853858947754,
      "learning_rate": 3.6469372792108245e-05,
      "loss": 1.276,
      "step": 4060
    },
    {
      "epoch": 0.8138372325534893,
      "grad_norm": 14.28345775604248,
      "learning_rate": 3.6436046124108515e-05,
      "loss": 0.9736,
      "step": 4070
    },
    {
      "epoch": 0.8158368326334733,
      "grad_norm": 13.391210556030273,
      "learning_rate": 3.640271945610878e-05,
      "loss": 1.276,
      "step": 4080
    },
    {
      "epoch": 0.8178364327134573,
      "grad_norm": 7.524712562561035,
      "learning_rate": 3.636939278810905e-05,
      "loss": 1.1983,
      "step": 4090
    },
    {
      "epoch": 0.8198360327934413,
      "grad_norm": 8.591636657714844,
      "learning_rate": 3.633606612010931e-05,
      "loss": 1.0727,
      "step": 4100
    },
    {
      "epoch": 0.8218356328734253,
      "grad_norm": 9.45606517791748,
      "learning_rate": 3.630273945210958e-05,
      "loss": 1.1102,
      "step": 4110
    },
    {
      "epoch": 0.8238352329534093,
      "grad_norm": 16.31220245361328,
      "learning_rate": 3.6269412784109846e-05,
      "loss": 1.1206,
      "step": 4120
    },
    {
      "epoch": 0.8258348330333933,
      "grad_norm": 8.74605655670166,
      "learning_rate": 3.623608611611011e-05,
      "loss": 1.2116,
      "step": 4130
    },
    {
      "epoch": 0.8278344331133773,
      "grad_norm": 12.346248626708984,
      "learning_rate": 3.620275944811038e-05,
      "loss": 1.0673,
      "step": 4140
    },
    {
      "epoch": 0.8298340331933614,
      "grad_norm": 12.661551475524902,
      "learning_rate": 3.616943278011064e-05,
      "loss": 1.3105,
      "step": 4150
    },
    {
      "epoch": 0.8318336332733454,
      "grad_norm": 11.918707847595215,
      "learning_rate": 3.6136106112110914e-05,
      "loss": 1.1331,
      "step": 4160
    },
    {
      "epoch": 0.8338332333533294,
      "grad_norm": 17.380149841308594,
      "learning_rate": 3.610277944411118e-05,
      "loss": 1.3181,
      "step": 4170
    },
    {
      "epoch": 0.8358328334333134,
      "grad_norm": 10.09432315826416,
      "learning_rate": 3.606945277611145e-05,
      "loss": 1.2067,
      "step": 4180
    },
    {
      "epoch": 0.8378324335132974,
      "grad_norm": 8.803640365600586,
      "learning_rate": 3.603612610811171e-05,
      "loss": 1.1309,
      "step": 4190
    },
    {
      "epoch": 0.8398320335932813,
      "grad_norm": 13.035538673400879,
      "learning_rate": 3.600279944011198e-05,
      "loss": 1.1715,
      "step": 4200
    },
    {
      "epoch": 0.8418316336732653,
      "grad_norm": 11.911402702331543,
      "learning_rate": 3.5969472772112245e-05,
      "loss": 1.5066,
      "step": 4210
    },
    {
      "epoch": 0.8438312337532493,
      "grad_norm": 16.935985565185547,
      "learning_rate": 3.593614610411251e-05,
      "loss": 1.2687,
      "step": 4220
    },
    {
      "epoch": 0.8458308338332333,
      "grad_norm": 15.074732780456543,
      "learning_rate": 3.590281943611278e-05,
      "loss": 1.168,
      "step": 4230
    },
    {
      "epoch": 0.8478304339132173,
      "grad_norm": 6.399120807647705,
      "learning_rate": 3.586949276811304e-05,
      "loss": 1.2842,
      "step": 4240
    },
    {
      "epoch": 0.8498300339932013,
      "grad_norm": 14.004207611083984,
      "learning_rate": 3.583616610011331e-05,
      "loss": 1.3401,
      "step": 4250
    },
    {
      "epoch": 0.8518296340731853,
      "grad_norm": 14.454651832580566,
      "learning_rate": 3.5802839432113576e-05,
      "loss": 1.1477,
      "step": 4260
    },
    {
      "epoch": 0.8538292341531694,
      "grad_norm": 16.35297203063965,
      "learning_rate": 3.5769512764113846e-05,
      "loss": 1.2992,
      "step": 4270
    },
    {
      "epoch": 0.8558288342331534,
      "grad_norm": 9.263208389282227,
      "learning_rate": 3.573618609611411e-05,
      "loss": 1.0686,
      "step": 4280
    },
    {
      "epoch": 0.8578284343131374,
      "grad_norm": 8.38028621673584,
      "learning_rate": 3.570285942811438e-05,
      "loss": 0.8009,
      "step": 4290
    },
    {
      "epoch": 0.8598280343931214,
      "grad_norm": 4.291645526885986,
      "learning_rate": 3.566953276011464e-05,
      "loss": 0.8402,
      "step": 4300
    },
    {
      "epoch": 0.8618276344731054,
      "grad_norm": 15.904095649719238,
      "learning_rate": 3.563620609211491e-05,
      "loss": 1.0968,
      "step": 4310
    },
    {
      "epoch": 0.8638272345530894,
      "grad_norm": 6.9572625160217285,
      "learning_rate": 3.560287942411518e-05,
      "loss": 1.2145,
      "step": 4320
    },
    {
      "epoch": 0.8658268346330734,
      "grad_norm": 12.459647178649902,
      "learning_rate": 3.556955275611544e-05,
      "loss": 0.9938,
      "step": 4330
    },
    {
      "epoch": 0.8678264347130574,
      "grad_norm": 11.682072639465332,
      "learning_rate": 3.553622608811571e-05,
      "loss": 1.0895,
      "step": 4340
    },
    {
      "epoch": 0.8698260347930414,
      "grad_norm": 14.969725608825684,
      "learning_rate": 3.5502899420115974e-05,
      "loss": 1.284,
      "step": 4350
    },
    {
      "epoch": 0.8718256348730254,
      "grad_norm": 9.82462215423584,
      "learning_rate": 3.5469572752116245e-05,
      "loss": 1.2934,
      "step": 4360
    },
    {
      "epoch": 0.8738252349530093,
      "grad_norm": 9.050975799560547,
      "learning_rate": 3.5436246084116515e-05,
      "loss": 1.1432,
      "step": 4370
    },
    {
      "epoch": 0.8758248350329934,
      "grad_norm": 10.72243881225586,
      "learning_rate": 3.540291941611678e-05,
      "loss": 1.0824,
      "step": 4380
    },
    {
      "epoch": 0.8778244351129774,
      "grad_norm": 39.329166412353516,
      "learning_rate": 3.536959274811705e-05,
      "loss": 1.3842,
      "step": 4390
    },
    {
      "epoch": 0.8798240351929614,
      "grad_norm": 8.453580856323242,
      "learning_rate": 3.533626608011731e-05,
      "loss": 1.309,
      "step": 4400
    },
    {
      "epoch": 0.8818236352729454,
      "grad_norm": 14.04312801361084,
      "learning_rate": 3.530293941211758e-05,
      "loss": 1.2582,
      "step": 4410
    },
    {
      "epoch": 0.8838232353529294,
      "grad_norm": 8.614508628845215,
      "learning_rate": 3.5269612744117846e-05,
      "loss": 1.1154,
      "step": 4420
    },
    {
      "epoch": 0.8858228354329134,
      "grad_norm": 12.570950508117676,
      "learning_rate": 3.5236286076118116e-05,
      "loss": 1.2485,
      "step": 4430
    },
    {
      "epoch": 0.8878224355128974,
      "grad_norm": 6.513674736022949,
      "learning_rate": 3.520295940811838e-05,
      "loss": 1.1752,
      "step": 4440
    },
    {
      "epoch": 0.8898220355928814,
      "grad_norm": 9.635892868041992,
      "learning_rate": 3.516963274011864e-05,
      "loss": 1.3091,
      "step": 4450
    },
    {
      "epoch": 0.8918216356728654,
      "grad_norm": 14.788000106811523,
      "learning_rate": 3.5136306072118913e-05,
      "loss": 1.3712,
      "step": 4460
    },
    {
      "epoch": 0.8938212357528494,
      "grad_norm": 14.451257705688477,
      "learning_rate": 3.510297940411918e-05,
      "loss": 1.0481,
      "step": 4470
    },
    {
      "epoch": 0.8958208358328335,
      "grad_norm": 7.513576984405518,
      "learning_rate": 3.506965273611945e-05,
      "loss": 1.2806,
      "step": 4480
    },
    {
      "epoch": 0.8978204359128175,
      "grad_norm": 9.49386978149414,
      "learning_rate": 3.503632606811971e-05,
      "loss": 1.1525,
      "step": 4490
    },
    {
      "epoch": 0.8998200359928015,
      "grad_norm": 10.454849243164062,
      "learning_rate": 3.500299940011998e-05,
      "loss": 1.0675,
      "step": 4500
    },
    {
      "epoch": 0.9018196360727855,
      "grad_norm": 13.658658981323242,
      "learning_rate": 3.4969672732120245e-05,
      "loss": 0.9555,
      "step": 4510
    },
    {
      "epoch": 0.9038192361527695,
      "grad_norm": 10.609099388122559,
      "learning_rate": 3.4936346064120515e-05,
      "loss": 1.3651,
      "step": 4520
    },
    {
      "epoch": 0.9058188362327535,
      "grad_norm": 10.870566368103027,
      "learning_rate": 3.490301939612078e-05,
      "loss": 0.9185,
      "step": 4530
    },
    {
      "epoch": 0.9078184363127374,
      "grad_norm": 11.157939910888672,
      "learning_rate": 3.486969272812104e-05,
      "loss": 1.2257,
      "step": 4540
    },
    {
      "epoch": 0.9098180363927214,
      "grad_norm": 11.493034362792969,
      "learning_rate": 3.483636606012131e-05,
      "loss": 1.3589,
      "step": 4550
    },
    {
      "epoch": 0.9118176364727054,
      "grad_norm": 13.887752532958984,
      "learning_rate": 3.4803039392121576e-05,
      "loss": 1.3894,
      "step": 4560
    },
    {
      "epoch": 0.9138172365526894,
      "grad_norm": 15.50894832611084,
      "learning_rate": 3.4769712724121846e-05,
      "loss": 1.07,
      "step": 4570
    },
    {
      "epoch": 0.9158168366326734,
      "grad_norm": 8.387073516845703,
      "learning_rate": 3.473638605612211e-05,
      "loss": 1.1459,
      "step": 4580
    },
    {
      "epoch": 0.9178164367126574,
      "grad_norm": 11.760666847229004,
      "learning_rate": 3.470305938812238e-05,
      "loss": 1.1632,
      "step": 4590
    },
    {
      "epoch": 0.9198160367926415,
      "grad_norm": 8.760969161987305,
      "learning_rate": 3.466973272012264e-05,
      "loss": 1.1117,
      "step": 4600
    },
    {
      "epoch": 0.9218156368726255,
      "grad_norm": 6.207510471343994,
      "learning_rate": 3.4636406052122913e-05,
      "loss": 1.3161,
      "step": 4610
    },
    {
      "epoch": 0.9238152369526095,
      "grad_norm": 7.408812999725342,
      "learning_rate": 3.460307938412318e-05,
      "loss": 1.0473,
      "step": 4620
    },
    {
      "epoch": 0.9258148370325935,
      "grad_norm": 4.378427505493164,
      "learning_rate": 3.456975271612344e-05,
      "loss": 1.1607,
      "step": 4630
    },
    {
      "epoch": 0.9278144371125775,
      "grad_norm": 10.167492866516113,
      "learning_rate": 3.453642604812371e-05,
      "loss": 1.2536,
      "step": 4640
    },
    {
      "epoch": 0.9298140371925615,
      "grad_norm": 14.910898208618164,
      "learning_rate": 3.4503099380123974e-05,
      "loss": 1.2916,
      "step": 4650
    },
    {
      "epoch": 0.9318136372725455,
      "grad_norm": 10.41291618347168,
      "learning_rate": 3.4469772712124244e-05,
      "loss": 1.247,
      "step": 4660
    },
    {
      "epoch": 0.9338132373525295,
      "grad_norm": 11.666461944580078,
      "learning_rate": 3.443644604412451e-05,
      "loss": 1.4462,
      "step": 4670
    },
    {
      "epoch": 0.9358128374325135,
      "grad_norm": 10.939391136169434,
      "learning_rate": 3.440311937612478e-05,
      "loss": 1.1648,
      "step": 4680
    },
    {
      "epoch": 0.9378124375124975,
      "grad_norm": 9.107660293579102,
      "learning_rate": 3.436979270812504e-05,
      "loss": 1.2819,
      "step": 4690
    },
    {
      "epoch": 0.9398120375924816,
      "grad_norm": 11.61727523803711,
      "learning_rate": 3.433646604012531e-05,
      "loss": 1.1475,
      "step": 4700
    },
    {
      "epoch": 0.9418116376724655,
      "grad_norm": 17.280546188354492,
      "learning_rate": 3.4303139372125576e-05,
      "loss": 1.2758,
      "step": 4710
    },
    {
      "epoch": 0.9438112377524495,
      "grad_norm": 12.218339920043945,
      "learning_rate": 3.426981270412584e-05,
      "loss": 1.3179,
      "step": 4720
    },
    {
      "epoch": 0.9458108378324335,
      "grad_norm": 7.429111003875732,
      "learning_rate": 3.423648603612611e-05,
      "loss": 1.1278,
      "step": 4730
    },
    {
      "epoch": 0.9478104379124175,
      "grad_norm": 6.1706624031066895,
      "learning_rate": 3.420315936812637e-05,
      "loss": 1.2153,
      "step": 4740
    },
    {
      "epoch": 0.9498100379924015,
      "grad_norm": 9.295451164245605,
      "learning_rate": 3.416983270012664e-05,
      "loss": 1.1627,
      "step": 4750
    },
    {
      "epoch": 0.9518096380723855,
      "grad_norm": 9.740483283996582,
      "learning_rate": 3.4136506032126907e-05,
      "loss": 1.3606,
      "step": 4760
    },
    {
      "epoch": 0.9538092381523695,
      "grad_norm": 6.441413879394531,
      "learning_rate": 3.410317936412718e-05,
      "loss": 1.002,
      "step": 4770
    },
    {
      "epoch": 0.9558088382323535,
      "grad_norm": 8.092058181762695,
      "learning_rate": 3.406985269612744e-05,
      "loss": 1.3924,
      "step": 4780
    },
    {
      "epoch": 0.9578084383123375,
      "grad_norm": 7.319852828979492,
      "learning_rate": 3.403652602812771e-05,
      "loss": 1.2565,
      "step": 4790
    },
    {
      "epoch": 0.9598080383923215,
      "grad_norm": 10.23288345336914,
      "learning_rate": 3.4003199360127974e-05,
      "loss": 1.3499,
      "step": 4800
    },
    {
      "epoch": 0.9618076384723055,
      "grad_norm": 20.769672393798828,
      "learning_rate": 3.396987269212824e-05,
      "loss": 1.3413,
      "step": 4810
    },
    {
      "epoch": 0.9638072385522896,
      "grad_norm": 7.018844127655029,
      "learning_rate": 3.393654602412851e-05,
      "loss": 1.2197,
      "step": 4820
    },
    {
      "epoch": 0.9658068386322736,
      "grad_norm": 7.582221508026123,
      "learning_rate": 3.390321935612877e-05,
      "loss": 1.1589,
      "step": 4830
    },
    {
      "epoch": 0.9678064387122576,
      "grad_norm": 8.07785701751709,
      "learning_rate": 3.386989268812904e-05,
      "loss": 1.0716,
      "step": 4840
    },
    {
      "epoch": 0.9698060387922416,
      "grad_norm": 9.573235511779785,
      "learning_rate": 3.3836566020129305e-05,
      "loss": 0.9411,
      "step": 4850
    },
    {
      "epoch": 0.9718056388722256,
      "grad_norm": 13.495512008666992,
      "learning_rate": 3.3803239352129576e-05,
      "loss": 1.2313,
      "step": 4860
    },
    {
      "epoch": 0.9738052389522096,
      "grad_norm": 6.523849964141846,
      "learning_rate": 3.376991268412984e-05,
      "loss": 0.9583,
      "step": 4870
    },
    {
      "epoch": 0.9758048390321936,
      "grad_norm": 8.444738388061523,
      "learning_rate": 3.373658601613011e-05,
      "loss": 1.0424,
      "step": 4880
    },
    {
      "epoch": 0.9778044391121775,
      "grad_norm": 9.013163566589355,
      "learning_rate": 3.370325934813038e-05,
      "loss": 1.1927,
      "step": 4890
    },
    {
      "epoch": 0.9798040391921615,
      "grad_norm": 14.2869234085083,
      "learning_rate": 3.366993268013064e-05,
      "loss": 1.6121,
      "step": 4900
    },
    {
      "epoch": 0.9818036392721455,
      "grad_norm": 7.120988368988037,
      "learning_rate": 3.363660601213091e-05,
      "loss": 1.3173,
      "step": 4910
    },
    {
      "epoch": 0.9838032393521295,
      "grad_norm": 17.530702590942383,
      "learning_rate": 3.360327934413118e-05,
      "loss": 1.1298,
      "step": 4920
    },
    {
      "epoch": 0.9858028394321136,
      "grad_norm": 7.3383073806762695,
      "learning_rate": 3.356995267613145e-05,
      "loss": 1.2524,
      "step": 4930
    },
    {
      "epoch": 0.9878024395120976,
      "grad_norm": 9.781767845153809,
      "learning_rate": 3.353662600813171e-05,
      "loss": 1.0125,
      "step": 4940
    },
    {
      "epoch": 0.9898020395920816,
      "grad_norm": 13.11892318725586,
      "learning_rate": 3.3503299340131974e-05,
      "loss": 0.9793,
      "step": 4950
    },
    {
      "epoch": 0.9918016396720656,
      "grad_norm": 11.650012969970703,
      "learning_rate": 3.3469972672132244e-05,
      "loss": 1.2953,
      "step": 4960
    },
    {
      "epoch": 0.9938012397520496,
      "grad_norm": 14.943472862243652,
      "learning_rate": 3.343664600413251e-05,
      "loss": 1.1491,
      "step": 4970
    },
    {
      "epoch": 0.9958008398320336,
      "grad_norm": 13.538097381591797,
      "learning_rate": 3.340331933613278e-05,
      "loss": 1.1344,
      "step": 4980
    },
    {
      "epoch": 0.9978004399120176,
      "grad_norm": 6.88840913772583,
      "learning_rate": 3.336999266813304e-05,
      "loss": 1.2435,
      "step": 4990
    },
    {
      "epoch": 0.9998000399920016,
      "grad_norm": 24.508262634277344,
      "learning_rate": 3.333666600013331e-05,
      "loss": 1.1372,
      "step": 5000
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.6237752449510098,
      "eval_loss": 1.1715487241744995,
      "eval_runtime": 42.8644,
      "eval_samples_per_second": 233.34,
      "eval_steps_per_second": 29.185,
      "step": 5001
    },
    {
      "epoch": 1.0017996400719855,
      "grad_norm": 10.80722427368164,
      "learning_rate": 3.3303339332133575e-05,
      "loss": 1.163,
      "step": 5010
    },
    {
      "epoch": 1.0037992401519695,
      "grad_norm": 8.493399620056152,
      "learning_rate": 3.3270012664133846e-05,
      "loss": 0.9369,
      "step": 5020
    },
    {
      "epoch": 1.0057988402319535,
      "grad_norm": 15.178129196166992,
      "learning_rate": 3.323668599613411e-05,
      "loss": 0.9431,
      "step": 5030
    },
    {
      "epoch": 1.0077984403119375,
      "grad_norm": 6.269545078277588,
      "learning_rate": 3.320335932813437e-05,
      "loss": 0.9414,
      "step": 5040
    },
    {
      "epoch": 1.0097980403919216,
      "grad_norm": 6.547365665435791,
      "learning_rate": 3.317003266013464e-05,
      "loss": 0.9277,
      "step": 5050
    },
    {
      "epoch": 1.0117976404719056,
      "grad_norm": 7.8420491218566895,
      "learning_rate": 3.3136705992134907e-05,
      "loss": 1.1022,
      "step": 5060
    },
    {
      "epoch": 1.0137972405518896,
      "grad_norm": 6.245758056640625,
      "learning_rate": 3.310337932413518e-05,
      "loss": 0.7611,
      "step": 5070
    },
    {
      "epoch": 1.0157968406318736,
      "grad_norm": 9.121499061584473,
      "learning_rate": 3.307005265613544e-05,
      "loss": 0.9662,
      "step": 5080
    },
    {
      "epoch": 1.0177964407118576,
      "grad_norm": 9.858642578125,
      "learning_rate": 3.303672598813571e-05,
      "loss": 1.0268,
      "step": 5090
    },
    {
      "epoch": 1.0197960407918416,
      "grad_norm": 11.590520858764648,
      "learning_rate": 3.3003399320135974e-05,
      "loss": 0.8286,
      "step": 5100
    },
    {
      "epoch": 1.0217956408718256,
      "grad_norm": 8.963308334350586,
      "learning_rate": 3.2970072652136244e-05,
      "loss": 0.9525,
      "step": 5110
    },
    {
      "epoch": 1.0237952409518096,
      "grad_norm": 12.631857872009277,
      "learning_rate": 3.293674598413651e-05,
      "loss": 1.0707,
      "step": 5120
    },
    {
      "epoch": 1.0257948410317936,
      "grad_norm": 13.74509048461914,
      "learning_rate": 3.290341931613677e-05,
      "loss": 0.9984,
      "step": 5130
    },
    {
      "epoch": 1.0277944411117776,
      "grad_norm": 8.911447525024414,
      "learning_rate": 3.287009264813704e-05,
      "loss": 1.1386,
      "step": 5140
    },
    {
      "epoch": 1.0297940411917617,
      "grad_norm": 10.373127937316895,
      "learning_rate": 3.2836765980137305e-05,
      "loss": 0.8283,
      "step": 5150
    },
    {
      "epoch": 1.0317936412717457,
      "grad_norm": 13.13809871673584,
      "learning_rate": 3.2803439312137575e-05,
      "loss": 0.9403,
      "step": 5160
    },
    {
      "epoch": 1.0337932413517297,
      "grad_norm": 14.591425895690918,
      "learning_rate": 3.277011264413784e-05,
      "loss": 1.065,
      "step": 5170
    },
    {
      "epoch": 1.0357928414317137,
      "grad_norm": 7.198284149169922,
      "learning_rate": 3.273678597613811e-05,
      "loss": 0.8203,
      "step": 5180
    },
    {
      "epoch": 1.0377924415116977,
      "grad_norm": 11.379169464111328,
      "learning_rate": 3.270345930813837e-05,
      "loss": 1.1749,
      "step": 5190
    },
    {
      "epoch": 1.0397920415916817,
      "grad_norm": 13.003775596618652,
      "learning_rate": 3.267013264013864e-05,
      "loss": 0.9096,
      "step": 5200
    },
    {
      "epoch": 1.0417916416716657,
      "grad_norm": 17.678098678588867,
      "learning_rate": 3.2636805972138906e-05,
      "loss": 0.9329,
      "step": 5210
    },
    {
      "epoch": 1.0437912417516497,
      "grad_norm": 5.709446907043457,
      "learning_rate": 3.260347930413917e-05,
      "loss": 0.9144,
      "step": 5220
    },
    {
      "epoch": 1.0457908418316337,
      "grad_norm": 14.666826248168945,
      "learning_rate": 3.257015263613944e-05,
      "loss": 0.7632,
      "step": 5230
    },
    {
      "epoch": 1.0477904419116177,
      "grad_norm": 6.8980231285095215,
      "learning_rate": 3.2536825968139704e-05,
      "loss": 0.6891,
      "step": 5240
    },
    {
      "epoch": 1.0497900419916018,
      "grad_norm": 13.20803451538086,
      "learning_rate": 3.2503499300139974e-05,
      "loss": 0.937,
      "step": 5250
    },
    {
      "epoch": 1.0517896420715858,
      "grad_norm": 12.968145370483398,
      "learning_rate": 3.247017263214024e-05,
      "loss": 0.7012,
      "step": 5260
    },
    {
      "epoch": 1.0537892421515698,
      "grad_norm": 16.991886138916016,
      "learning_rate": 3.243684596414051e-05,
      "loss": 1.2341,
      "step": 5270
    },
    {
      "epoch": 1.0557888422315538,
      "grad_norm": 21.708141326904297,
      "learning_rate": 3.240351929614077e-05,
      "loss": 1.2953,
      "step": 5280
    },
    {
      "epoch": 1.0577884423115378,
      "grad_norm": 11.243534088134766,
      "learning_rate": 3.237019262814104e-05,
      "loss": 1.1495,
      "step": 5290
    },
    {
      "epoch": 1.0597880423915218,
      "grad_norm": 7.377511978149414,
      "learning_rate": 3.2336865960141305e-05,
      "loss": 0.7342,
      "step": 5300
    },
    {
      "epoch": 1.0617876424715056,
      "grad_norm": 8.163166999816895,
      "learning_rate": 3.230353929214157e-05,
      "loss": 0.954,
      "step": 5310
    },
    {
      "epoch": 1.0637872425514896,
      "grad_norm": 7.441366672515869,
      "learning_rate": 3.227021262414184e-05,
      "loss": 0.9379,
      "step": 5320
    },
    {
      "epoch": 1.0657868426314736,
      "grad_norm": 12.603632926940918,
      "learning_rate": 3.22368859561421e-05,
      "loss": 0.8223,
      "step": 5330
    },
    {
      "epoch": 1.0677864427114576,
      "grad_norm": 8.971607208251953,
      "learning_rate": 3.220355928814237e-05,
      "loss": 0.845,
      "step": 5340
    },
    {
      "epoch": 1.0697860427914416,
      "grad_norm": 3.259829521179199,
      "learning_rate": 3.2170232620142636e-05,
      "loss": 1.0544,
      "step": 5350
    },
    {
      "epoch": 1.0717856428714256,
      "grad_norm": 6.0408782958984375,
      "learning_rate": 3.2136905952142906e-05,
      "loss": 0.9631,
      "step": 5360
    },
    {
      "epoch": 1.0737852429514096,
      "grad_norm": 7.043534278869629,
      "learning_rate": 3.210357928414317e-05,
      "loss": 0.5751,
      "step": 5370
    },
    {
      "epoch": 1.0757848430313937,
      "grad_norm": 9.236739158630371,
      "learning_rate": 3.207025261614344e-05,
      "loss": 1.0121,
      "step": 5380
    },
    {
      "epoch": 1.0777844431113777,
      "grad_norm": 6.585311412811279,
      "learning_rate": 3.2036925948143704e-05,
      "loss": 0.8082,
      "step": 5390
    },
    {
      "epoch": 1.0797840431913617,
      "grad_norm": 11.481274604797363,
      "learning_rate": 3.2003599280143974e-05,
      "loss": 0.6042,
      "step": 5400
    },
    {
      "epoch": 1.0817836432713457,
      "grad_norm": 8.64932632446289,
      "learning_rate": 3.1970272612144244e-05,
      "loss": 1.0637,
      "step": 5410
    },
    {
      "epoch": 1.0837832433513297,
      "grad_norm": 9.197028160095215,
      "learning_rate": 3.193694594414451e-05,
      "loss": 0.7352,
      "step": 5420
    },
    {
      "epoch": 1.0857828434313137,
      "grad_norm": 18.155794143676758,
      "learning_rate": 3.190361927614478e-05,
      "loss": 0.8185,
      "step": 5430
    },
    {
      "epoch": 1.0877824435112977,
      "grad_norm": 16.271743774414062,
      "learning_rate": 3.187029260814504e-05,
      "loss": 1.1765,
      "step": 5440
    },
    {
      "epoch": 1.0897820435912817,
      "grad_norm": 13.116707801818848,
      "learning_rate": 3.1836965940145305e-05,
      "loss": 0.8313,
      "step": 5450
    },
    {
      "epoch": 1.0917816436712657,
      "grad_norm": 6.3304762840271,
      "learning_rate": 3.1803639272145575e-05,
      "loss": 0.8482,
      "step": 5460
    },
    {
      "epoch": 1.0937812437512497,
      "grad_norm": 14.8366117477417,
      "learning_rate": 3.177031260414584e-05,
      "loss": 1.2845,
      "step": 5470
    },
    {
      "epoch": 1.0957808438312338,
      "grad_norm": 16.442058563232422,
      "learning_rate": 3.173698593614611e-05,
      "loss": 0.824,
      "step": 5480
    },
    {
      "epoch": 1.0977804439112178,
      "grad_norm": 7.993218898773193,
      "learning_rate": 3.170365926814637e-05,
      "loss": 1.1316,
      "step": 5490
    },
    {
      "epoch": 1.0997800439912018,
      "grad_norm": 11.515085220336914,
      "learning_rate": 3.167033260014664e-05,
      "loss": 0.9085,
      "step": 5500
    },
    {
      "epoch": 1.1017796440711858,
      "grad_norm": 7.664937973022461,
      "learning_rate": 3.1637005932146906e-05,
      "loss": 0.776,
      "step": 5510
    },
    {
      "epoch": 1.1037792441511698,
      "grad_norm": 9.206944465637207,
      "learning_rate": 3.160367926414718e-05,
      "loss": 1.0517,
      "step": 5520
    },
    {
      "epoch": 1.1057788442311538,
      "grad_norm": 13.307101249694824,
      "learning_rate": 3.157035259614744e-05,
      "loss": 0.8342,
      "step": 5530
    },
    {
      "epoch": 1.1077784443111378,
      "grad_norm": 12.264434814453125,
      "learning_rate": 3.1537025928147704e-05,
      "loss": 1.0454,
      "step": 5540
    },
    {
      "epoch": 1.1097780443911218,
      "grad_norm": 5.8035569190979,
      "learning_rate": 3.1503699260147974e-05,
      "loss": 0.6741,
      "step": 5550
    },
    {
      "epoch": 1.1117776444711058,
      "grad_norm": 16.37992286682129,
      "learning_rate": 3.147037259214824e-05,
      "loss": 1.066,
      "step": 5560
    },
    {
      "epoch": 1.1137772445510898,
      "grad_norm": 17.01441192626953,
      "learning_rate": 3.143704592414851e-05,
      "loss": 0.8589,
      "step": 5570
    },
    {
      "epoch": 1.1157768446310738,
      "grad_norm": 14.23218822479248,
      "learning_rate": 3.140371925614877e-05,
      "loss": 0.9207,
      "step": 5580
    },
    {
      "epoch": 1.1177764447110579,
      "grad_norm": 11.696382522583008,
      "learning_rate": 3.137039258814904e-05,
      "loss": 1.1679,
      "step": 5590
    },
    {
      "epoch": 1.1197760447910419,
      "grad_norm": 8.917357444763184,
      "learning_rate": 3.1337065920149305e-05,
      "loss": 0.6825,
      "step": 5600
    },
    {
      "epoch": 1.1217756448710259,
      "grad_norm": 12.26638126373291,
      "learning_rate": 3.1303739252149575e-05,
      "loss": 0.8601,
      "step": 5610
    },
    {
      "epoch": 1.1237752449510099,
      "grad_norm": 11.834938049316406,
      "learning_rate": 3.127041258414984e-05,
      "loss": 0.8281,
      "step": 5620
    },
    {
      "epoch": 1.125774845030994,
      "grad_norm": 9.974050521850586,
      "learning_rate": 3.12370859161501e-05,
      "loss": 0.9397,
      "step": 5630
    },
    {
      "epoch": 1.127774445110978,
      "grad_norm": 11.04778003692627,
      "learning_rate": 3.120375924815037e-05,
      "loss": 1.1122,
      "step": 5640
    },
    {
      "epoch": 1.129774045190962,
      "grad_norm": 9.386514663696289,
      "learning_rate": 3.1170432580150636e-05,
      "loss": 0.9728,
      "step": 5650
    },
    {
      "epoch": 1.131773645270946,
      "grad_norm": 17.45366668701172,
      "learning_rate": 3.1137105912150906e-05,
      "loss": 1.0274,
      "step": 5660
    },
    {
      "epoch": 1.13377324535093,
      "grad_norm": 15.180498123168945,
      "learning_rate": 3.110377924415117e-05,
      "loss": 1.1871,
      "step": 5670
    },
    {
      "epoch": 1.1357728454309137,
      "grad_norm": 7.698610305786133,
      "learning_rate": 3.107045257615144e-05,
      "loss": 0.8722,
      "step": 5680
    },
    {
      "epoch": 1.1377724455108977,
      "grad_norm": 3.4791507720947266,
      "learning_rate": 3.1037125908151704e-05,
      "loss": 0.7908,
      "step": 5690
    },
    {
      "epoch": 1.1397720455908817,
      "grad_norm": 9.352248191833496,
      "learning_rate": 3.1003799240151974e-05,
      "loss": 0.8986,
      "step": 5700
    },
    {
      "epoch": 1.1417716456708658,
      "grad_norm": 8.973991394042969,
      "learning_rate": 3.097047257215224e-05,
      "loss": 0.8611,
      "step": 5710
    },
    {
      "epoch": 1.1437712457508498,
      "grad_norm": 17.87181854248047,
      "learning_rate": 3.09371459041525e-05,
      "loss": 1.0444,
      "step": 5720
    },
    {
      "epoch": 1.1457708458308338,
      "grad_norm": 7.875071048736572,
      "learning_rate": 3.090381923615277e-05,
      "loss": 0.7987,
      "step": 5730
    },
    {
      "epoch": 1.1477704459108178,
      "grad_norm": 18.167049407958984,
      "learning_rate": 3.0870492568153035e-05,
      "loss": 1.1651,
      "step": 5740
    },
    {
      "epoch": 1.1497700459908018,
      "grad_norm": 10.68212890625,
      "learning_rate": 3.0837165900153305e-05,
      "loss": 0.6871,
      "step": 5750
    },
    {
      "epoch": 1.1517696460707858,
      "grad_norm": 12.9796142578125,
      "learning_rate": 3.080383923215357e-05,
      "loss": 0.8647,
      "step": 5760
    },
    {
      "epoch": 1.1537692461507698,
      "grad_norm": 14.928367614746094,
      "learning_rate": 3.077051256415384e-05,
      "loss": 0.8908,
      "step": 5770
    },
    {
      "epoch": 1.1557688462307538,
      "grad_norm": 14.036500930786133,
      "learning_rate": 3.07371858961541e-05,
      "loss": 1.0174,
      "step": 5780
    },
    {
      "epoch": 1.1577684463107378,
      "grad_norm": 16.86418914794922,
      "learning_rate": 3.0703859228154366e-05,
      "loss": 0.8741,
      "step": 5790
    },
    {
      "epoch": 1.1597680463907218,
      "grad_norm": 9.419305801391602,
      "learning_rate": 3.0670532560154636e-05,
      "loss": 0.9169,
      "step": 5800
    },
    {
      "epoch": 1.1617676464707059,
      "grad_norm": 19.17961883544922,
      "learning_rate": 3.06372058921549e-05,
      "loss": 0.8474,
      "step": 5810
    },
    {
      "epoch": 1.1637672465506899,
      "grad_norm": 16.903907775878906,
      "learning_rate": 3.060387922415517e-05,
      "loss": 0.9598,
      "step": 5820
    },
    {
      "epoch": 1.1657668466306739,
      "grad_norm": 10.185314178466797,
      "learning_rate": 3.057055255615543e-05,
      "loss": 1.1211,
      "step": 5830
    },
    {
      "epoch": 1.1677664467106579,
      "grad_norm": 11.333818435668945,
      "learning_rate": 3.0537225888155704e-05,
      "loss": 1.1428,
      "step": 5840
    },
    {
      "epoch": 1.169766046790642,
      "grad_norm": 12.338004112243652,
      "learning_rate": 3.0503899220155967e-05,
      "loss": 0.69,
      "step": 5850
    },
    {
      "epoch": 1.171765646870626,
      "grad_norm": 14.337776184082031,
      "learning_rate": 3.0470572552156234e-05,
      "loss": 1.1756,
      "step": 5860
    },
    {
      "epoch": 1.17376524695061,
      "grad_norm": 11.719893455505371,
      "learning_rate": 3.04372458841565e-05,
      "loss": 0.9811,
      "step": 5870
    },
    {
      "epoch": 1.175764847030594,
      "grad_norm": 21.65361785888672,
      "learning_rate": 3.0403919216156768e-05,
      "loss": 1.1197,
      "step": 5880
    },
    {
      "epoch": 1.177764447110578,
      "grad_norm": 18.096418380737305,
      "learning_rate": 3.0370592548157035e-05,
      "loss": 1.1154,
      "step": 5890
    },
    {
      "epoch": 1.179764047190562,
      "grad_norm": 20.325082778930664,
      "learning_rate": 3.03372658801573e-05,
      "loss": 0.8448,
      "step": 5900
    },
    {
      "epoch": 1.181763647270546,
      "grad_norm": 6.630752086639404,
      "learning_rate": 3.0303939212157572e-05,
      "loss": 0.9076,
      "step": 5910
    },
    {
      "epoch": 1.18376324735053,
      "grad_norm": 7.704328536987305,
      "learning_rate": 3.027061254415784e-05,
      "loss": 1.0016,
      "step": 5920
    },
    {
      "epoch": 1.185762847430514,
      "grad_norm": 9.372870445251465,
      "learning_rate": 3.0237285876158106e-05,
      "loss": 1.2018,
      "step": 5930
    },
    {
      "epoch": 1.187762447510498,
      "grad_norm": 22.715303421020508,
      "learning_rate": 3.0203959208158373e-05,
      "loss": 1.1119,
      "step": 5940
    },
    {
      "epoch": 1.189762047590482,
      "grad_norm": 8.567610740661621,
      "learning_rate": 3.017063254015864e-05,
      "loss": 0.9934,
      "step": 5950
    },
    {
      "epoch": 1.191761647670466,
      "grad_norm": 18.01203155517578,
      "learning_rate": 3.0137305872158906e-05,
      "loss": 1.2177,
      "step": 5960
    },
    {
      "epoch": 1.19376124775045,
      "grad_norm": 10.317336082458496,
      "learning_rate": 3.0103979204159173e-05,
      "loss": 0.6794,
      "step": 5970
    },
    {
      "epoch": 1.1957608478304338,
      "grad_norm": 5.600399494171143,
      "learning_rate": 3.0070652536159437e-05,
      "loss": 0.8211,
      "step": 5980
    },
    {
      "epoch": 1.1977604479104178,
      "grad_norm": 14.516963005065918,
      "learning_rate": 3.0037325868159704e-05,
      "loss": 0.813,
      "step": 5990
    },
    {
      "epoch": 1.1997600479904018,
      "grad_norm": 17.314960479736328,
      "learning_rate": 3.000399920015997e-05,
      "loss": 0.925,
      "step": 6000
    },
    {
      "epoch": 1.2017596480703858,
      "grad_norm": 13.642000198364258,
      "learning_rate": 2.9970672532160237e-05,
      "loss": 0.8871,
      "step": 6010
    },
    {
      "epoch": 1.2037592481503698,
      "grad_norm": 7.610594272613525,
      "learning_rate": 2.9937345864160504e-05,
      "loss": 0.9739,
      "step": 6020
    },
    {
      "epoch": 1.2057588482303538,
      "grad_norm": 1.6586260795593262,
      "learning_rate": 2.990401919616077e-05,
      "loss": 0.7223,
      "step": 6030
    },
    {
      "epoch": 1.2077584483103379,
      "grad_norm": 16.722536087036133,
      "learning_rate": 2.9870692528161038e-05,
      "loss": 0.914,
      "step": 6040
    },
    {
      "epoch": 1.2097580483903219,
      "grad_norm": 15.140969276428223,
      "learning_rate": 2.9837365860161305e-05,
      "loss": 0.7815,
      "step": 6050
    },
    {
      "epoch": 1.2117576484703059,
      "grad_norm": 10.601222038269043,
      "learning_rate": 2.9804039192161572e-05,
      "loss": 0.9558,
      "step": 6060
    },
    {
      "epoch": 1.2137572485502899,
      "grad_norm": 10.353321075439453,
      "learning_rate": 2.9770712524161835e-05,
      "loss": 0.9428,
      "step": 6070
    },
    {
      "epoch": 1.215756848630274,
      "grad_norm": 11.745906829833984,
      "learning_rate": 2.9737385856162102e-05,
      "loss": 1.24,
      "step": 6080
    },
    {
      "epoch": 1.217756448710258,
      "grad_norm": 13.115339279174805,
      "learning_rate": 2.970405918816237e-05,
      "loss": 0.9802,
      "step": 6090
    },
    {
      "epoch": 1.219756048790242,
      "grad_norm": 8.960882186889648,
      "learning_rate": 2.9670732520162636e-05,
      "loss": 0.9058,
      "step": 6100
    },
    {
      "epoch": 1.221755648870226,
      "grad_norm": 12.755616188049316,
      "learning_rate": 2.9637405852162903e-05,
      "loss": 0.7299,
      "step": 6110
    },
    {
      "epoch": 1.22375524895021,
      "grad_norm": 11.39201831817627,
      "learning_rate": 2.960407918416317e-05,
      "loss": 0.6987,
      "step": 6120
    },
    {
      "epoch": 1.225754849030194,
      "grad_norm": 7.725442409515381,
      "learning_rate": 2.9570752516163437e-05,
      "loss": 1.0189,
      "step": 6130
    },
    {
      "epoch": 1.227754449110178,
      "grad_norm": 7.407413005828857,
      "learning_rate": 2.9537425848163704e-05,
      "loss": 0.816,
      "step": 6140
    },
    {
      "epoch": 1.229754049190162,
      "grad_norm": 17.09162712097168,
      "learning_rate": 2.950409918016397e-05,
      "loss": 1.1845,
      "step": 6150
    },
    {
      "epoch": 1.231753649270146,
      "grad_norm": 5.858207702636719,
      "learning_rate": 2.9470772512164234e-05,
      "loss": 0.8728,
      "step": 6160
    },
    {
      "epoch": 1.23375324935013,
      "grad_norm": 21.666532516479492,
      "learning_rate": 2.94374458441645e-05,
      "loss": 0.9011,
      "step": 6170
    },
    {
      "epoch": 1.235752849430114,
      "grad_norm": 23.256362915039062,
      "learning_rate": 2.9404119176164768e-05,
      "loss": 0.8465,
      "step": 6180
    },
    {
      "epoch": 1.237752449510098,
      "grad_norm": 17.853059768676758,
      "learning_rate": 2.9370792508165035e-05,
      "loss": 0.9146,
      "step": 6190
    },
    {
      "epoch": 1.239752049590082,
      "grad_norm": 8.557899475097656,
      "learning_rate": 2.93374658401653e-05,
      "loss": 1.2533,
      "step": 6200
    },
    {
      "epoch": 1.241751649670066,
      "grad_norm": 11.35753059387207,
      "learning_rate": 2.930413917216557e-05,
      "loss": 1.0858,
      "step": 6210
    },
    {
      "epoch": 1.24375124975005,
      "grad_norm": 19.554758071899414,
      "learning_rate": 2.9270812504165835e-05,
      "loss": 0.916,
      "step": 6220
    },
    {
      "epoch": 1.245750849830034,
      "grad_norm": 9.69373607635498,
      "learning_rate": 2.9237485836166102e-05,
      "loss": 0.8962,
      "step": 6230
    },
    {
      "epoch": 1.247750449910018,
      "grad_norm": 18.72163200378418,
      "learning_rate": 2.9204159168166366e-05,
      "loss": 0.854,
      "step": 6240
    },
    {
      "epoch": 1.249750049990002,
      "grad_norm": 8.972244262695312,
      "learning_rate": 2.9170832500166633e-05,
      "loss": 0.6995,
      "step": 6250
    },
    {
      "epoch": 1.251749650069986,
      "grad_norm": 14.772855758666992,
      "learning_rate": 2.91375058321669e-05,
      "loss": 0.9528,
      "step": 6260
    },
    {
      "epoch": 1.25374925014997,
      "grad_norm": 4.612979888916016,
      "learning_rate": 2.9104179164167166e-05,
      "loss": 0.8021,
      "step": 6270
    },
    {
      "epoch": 1.255748850229954,
      "grad_norm": 10.883307456970215,
      "learning_rate": 2.9070852496167433e-05,
      "loss": 1.0495,
      "step": 6280
    },
    {
      "epoch": 1.257748450309938,
      "grad_norm": 8.313063621520996,
      "learning_rate": 2.90375258281677e-05,
      "loss": 0.8974,
      "step": 6290
    },
    {
      "epoch": 1.259748050389922,
      "grad_norm": 13.614985466003418,
      "learning_rate": 2.9004199160167967e-05,
      "loss": 0.7921,
      "step": 6300
    },
    {
      "epoch": 1.2617476504699061,
      "grad_norm": 8.801847457885742,
      "learning_rate": 2.8970872492168234e-05,
      "loss": 0.7225,
      "step": 6310
    },
    {
      "epoch": 1.2637472505498901,
      "grad_norm": 11.777344703674316,
      "learning_rate": 2.89375458241685e-05,
      "loss": 1.1624,
      "step": 6320
    },
    {
      "epoch": 1.2657468506298741,
      "grad_norm": 24.87760353088379,
      "learning_rate": 2.8904219156168764e-05,
      "loss": 1.2754,
      "step": 6330
    },
    {
      "epoch": 1.2677464507098581,
      "grad_norm": 8.853668212890625,
      "learning_rate": 2.887089248816903e-05,
      "loss": 0.8719,
      "step": 6340
    },
    {
      "epoch": 1.2697460507898422,
      "grad_norm": 13.559314727783203,
      "learning_rate": 2.8837565820169298e-05,
      "loss": 1.0044,
      "step": 6350
    },
    {
      "epoch": 1.2717456508698262,
      "grad_norm": 13.709931373596191,
      "learning_rate": 2.8804239152169565e-05,
      "loss": 0.9355,
      "step": 6360
    },
    {
      "epoch": 1.2737452509498102,
      "grad_norm": 9.021919250488281,
      "learning_rate": 2.8770912484169832e-05,
      "loss": 0.8504,
      "step": 6370
    },
    {
      "epoch": 1.275744851029794,
      "grad_norm": 12.384428024291992,
      "learning_rate": 2.87375858161701e-05,
      "loss": 1.1855,
      "step": 6380
    },
    {
      "epoch": 1.277744451109778,
      "grad_norm": 7.291859149932861,
      "learning_rate": 2.8704259148170366e-05,
      "loss": 0.9032,
      "step": 6390
    },
    {
      "epoch": 1.279744051189762,
      "grad_norm": 12.984981536865234,
      "learning_rate": 2.8670932480170633e-05,
      "loss": 0.9998,
      "step": 6400
    },
    {
      "epoch": 1.281743651269746,
      "grad_norm": 7.080036163330078,
      "learning_rate": 2.86376058121709e-05,
      "loss": 0.9465,
      "step": 6410
    },
    {
      "epoch": 1.28374325134973,
      "grad_norm": 11.403557777404785,
      "learning_rate": 2.8604279144171163e-05,
      "loss": 0.8742,
      "step": 6420
    },
    {
      "epoch": 1.285742851429714,
      "grad_norm": 10.634895324707031,
      "learning_rate": 2.8570952476171437e-05,
      "loss": 1.0217,
      "step": 6430
    },
    {
      "epoch": 1.287742451509698,
      "grad_norm": 14.626508712768555,
      "learning_rate": 2.8537625808171703e-05,
      "loss": 0.7267,
      "step": 6440
    },
    {
      "epoch": 1.289742051589682,
      "grad_norm": 15.354958534240723,
      "learning_rate": 2.850429914017197e-05,
      "loss": 0.9299,
      "step": 6450
    },
    {
      "epoch": 1.291741651669666,
      "grad_norm": 10.222518920898438,
      "learning_rate": 2.8470972472172237e-05,
      "loss": 1.0182,
      "step": 6460
    },
    {
      "epoch": 1.29374125174965,
      "grad_norm": 8.366691589355469,
      "learning_rate": 2.84376458041725e-05,
      "loss": 0.8228,
      "step": 6470
    },
    {
      "epoch": 1.295740851829634,
      "grad_norm": 14.903400421142578,
      "learning_rate": 2.8404319136172768e-05,
      "loss": 0.9604,
      "step": 6480
    },
    {
      "epoch": 1.297740451909618,
      "grad_norm": 15.889300346374512,
      "learning_rate": 2.8370992468173035e-05,
      "loss": 0.8788,
      "step": 6490
    },
    {
      "epoch": 1.299740051989602,
      "grad_norm": 12.399665832519531,
      "learning_rate": 2.83376658001733e-05,
      "loss": 1.0812,
      "step": 6500
    },
    {
      "epoch": 1.301739652069586,
      "grad_norm": 6.555566787719727,
      "learning_rate": 2.830433913217357e-05,
      "loss": 0.8717,
      "step": 6510
    },
    {
      "epoch": 1.30373925214957,
      "grad_norm": 9.936058044433594,
      "learning_rate": 2.8271012464173835e-05,
      "loss": 0.778,
      "step": 6520
    },
    {
      "epoch": 1.305738852229554,
      "grad_norm": 14.35396957397461,
      "learning_rate": 2.8237685796174102e-05,
      "loss": 1.0525,
      "step": 6530
    },
    {
      "epoch": 1.3077384523095381,
      "grad_norm": 23.216873168945312,
      "learning_rate": 2.820435912817437e-05,
      "loss": 1.0334,
      "step": 6540
    },
    {
      "epoch": 1.3097380523895221,
      "grad_norm": 13.739562034606934,
      "learning_rate": 2.8171032460174636e-05,
      "loss": 0.8104,
      "step": 6550
    },
    {
      "epoch": 1.3117376524695061,
      "grad_norm": 10.515190124511719,
      "learning_rate": 2.81377057921749e-05,
      "loss": 0.9828,
      "step": 6560
    },
    {
      "epoch": 1.3137372525494901,
      "grad_norm": 11.02917766571045,
      "learning_rate": 2.8104379124175166e-05,
      "loss": 0.8839,
      "step": 6570
    },
    {
      "epoch": 1.3157368526294742,
      "grad_norm": 11.638957977294922,
      "learning_rate": 2.8071052456175433e-05,
      "loss": 1.0147,
      "step": 6580
    },
    {
      "epoch": 1.3177364527094582,
      "grad_norm": 11.463875770568848,
      "learning_rate": 2.80377257881757e-05,
      "loss": 0.786,
      "step": 6590
    },
    {
      "epoch": 1.3197360527894422,
      "grad_norm": 14.522214889526367,
      "learning_rate": 2.8004399120175967e-05,
      "loss": 0.7976,
      "step": 6600
    },
    {
      "epoch": 1.3217356528694262,
      "grad_norm": 17.6164493560791,
      "learning_rate": 2.7971072452176234e-05,
      "loss": 1.1838,
      "step": 6610
    },
    {
      "epoch": 1.3237352529494102,
      "grad_norm": 13.60181713104248,
      "learning_rate": 2.79377457841765e-05,
      "loss": 1.0541,
      "step": 6620
    },
    {
      "epoch": 1.3257348530293942,
      "grad_norm": 8.429830551147461,
      "learning_rate": 2.7904419116176768e-05,
      "loss": 1.1562,
      "step": 6630
    },
    {
      "epoch": 1.327734453109378,
      "grad_norm": 17.81913948059082,
      "learning_rate": 2.7871092448177035e-05,
      "loss": 1.1951,
      "step": 6640
    },
    {
      "epoch": 1.329734053189362,
      "grad_norm": 9.632257461547852,
      "learning_rate": 2.7837765780177298e-05,
      "loss": 0.8749,
      "step": 6650
    },
    {
      "epoch": 1.331733653269346,
      "grad_norm": 10.270376205444336,
      "learning_rate": 2.7804439112177565e-05,
      "loss": 0.8676,
      "step": 6660
    },
    {
      "epoch": 1.33373325334933,
      "grad_norm": 10.948896408081055,
      "learning_rate": 2.7771112444177832e-05,
      "loss": 0.7519,
      "step": 6670
    },
    {
      "epoch": 1.335732853429314,
      "grad_norm": 5.359228134155273,
      "learning_rate": 2.77377857761781e-05,
      "loss": 0.8179,
      "step": 6680
    },
    {
      "epoch": 1.337732453509298,
      "grad_norm": 8.488261222839355,
      "learning_rate": 2.7704459108178366e-05,
      "loss": 0.9285,
      "step": 6690
    },
    {
      "epoch": 1.339732053589282,
      "grad_norm": 19.76957893371582,
      "learning_rate": 2.7671132440178632e-05,
      "loss": 1.2708,
      "step": 6700
    },
    {
      "epoch": 1.341731653669266,
      "grad_norm": 12.818047523498535,
      "learning_rate": 2.76378057721789e-05,
      "loss": 0.6396,
      "step": 6710
    },
    {
      "epoch": 1.34373125374925,
      "grad_norm": 17.417322158813477,
      "learning_rate": 2.7604479104179166e-05,
      "loss": 1.1852,
      "step": 6720
    },
    {
      "epoch": 1.345730853829234,
      "grad_norm": 9.09740924835205,
      "learning_rate": 2.7571152436179433e-05,
      "loss": 0.7137,
      "step": 6730
    },
    {
      "epoch": 1.347730453909218,
      "grad_norm": 19.589433670043945,
      "learning_rate": 2.7537825768179697e-05,
      "loss": 0.8319,
      "step": 6740
    },
    {
      "epoch": 1.349730053989202,
      "grad_norm": 15.73856258392334,
      "learning_rate": 2.7504499100179964e-05,
      "loss": 1.0773,
      "step": 6750
    },
    {
      "epoch": 1.351729654069186,
      "grad_norm": 11.582460403442383,
      "learning_rate": 2.747117243218023e-05,
      "loss": 0.9006,
      "step": 6760
    },
    {
      "epoch": 1.3537292541491701,
      "grad_norm": 20.336475372314453,
      "learning_rate": 2.7437845764180497e-05,
      "loss": 1.1132,
      "step": 6770
    },
    {
      "epoch": 1.3557288542291541,
      "grad_norm": 12.115336418151855,
      "learning_rate": 2.7404519096180764e-05,
      "loss": 1.1474,
      "step": 6780
    },
    {
      "epoch": 1.3577284543091381,
      "grad_norm": 11.624287605285645,
      "learning_rate": 2.737119242818103e-05,
      "loss": 0.9837,
      "step": 6790
    },
    {
      "epoch": 1.3597280543891221,
      "grad_norm": 6.931879043579102,
      "learning_rate": 2.7337865760181298e-05,
      "loss": 0.8315,
      "step": 6800
    },
    {
      "epoch": 1.3617276544691062,
      "grad_norm": 12.117997169494629,
      "learning_rate": 2.7304539092181565e-05,
      "loss": 0.9889,
      "step": 6810
    },
    {
      "epoch": 1.3637272545490902,
      "grad_norm": 5.22815465927124,
      "learning_rate": 2.7271212424181832e-05,
      "loss": 0.8471,
      "step": 6820
    },
    {
      "epoch": 1.3657268546290742,
      "grad_norm": 8.787679672241211,
      "learning_rate": 2.7237885756182095e-05,
      "loss": 0.9312,
      "step": 6830
    },
    {
      "epoch": 1.3677264547090582,
      "grad_norm": 6.934794902801514,
      "learning_rate": 2.7204559088182362e-05,
      "loss": 0.9513,
      "step": 6840
    },
    {
      "epoch": 1.3697260547890422,
      "grad_norm": 17.58083724975586,
      "learning_rate": 2.717123242018263e-05,
      "loss": 1.1248,
      "step": 6850
    },
    {
      "epoch": 1.3717256548690262,
      "grad_norm": 8.527792930603027,
      "learning_rate": 2.7137905752182896e-05,
      "loss": 0.9333,
      "step": 6860
    },
    {
      "epoch": 1.3737252549490102,
      "grad_norm": 10.242974281311035,
      "learning_rate": 2.7104579084183163e-05,
      "loss": 0.9426,
      "step": 6870
    },
    {
      "epoch": 1.3757248550289942,
      "grad_norm": 22.47539710998535,
      "learning_rate": 2.707125241618343e-05,
      "loss": 0.7765,
      "step": 6880
    },
    {
      "epoch": 1.3777244551089782,
      "grad_norm": 12.483972549438477,
      "learning_rate": 2.7037925748183697e-05,
      "loss": 1.1841,
      "step": 6890
    },
    {
      "epoch": 1.3797240551889622,
      "grad_norm": 11.942330360412598,
      "learning_rate": 2.7004599080183964e-05,
      "loss": 0.8036,
      "step": 6900
    },
    {
      "epoch": 1.3817236552689462,
      "grad_norm": 13.240253448486328,
      "learning_rate": 2.6971272412184227e-05,
      "loss": 0.924,
      "step": 6910
    },
    {
      "epoch": 1.3837232553489303,
      "grad_norm": 13.964170455932617,
      "learning_rate": 2.6937945744184494e-05,
      "loss": 0.7811,
      "step": 6920
    },
    {
      "epoch": 1.3857228554289143,
      "grad_norm": 8.87219524383545,
      "learning_rate": 2.690461907618476e-05,
      "loss": 0.8682,
      "step": 6930
    },
    {
      "epoch": 1.3877224555088983,
      "grad_norm": 12.940777778625488,
      "learning_rate": 2.6871292408185034e-05,
      "loss": 1.2073,
      "step": 6940
    },
    {
      "epoch": 1.3897220555888823,
      "grad_norm": 9.404668807983398,
      "learning_rate": 2.68379657401853e-05,
      "loss": 0.831,
      "step": 6950
    },
    {
      "epoch": 1.3917216556688663,
      "grad_norm": 6.532227516174316,
      "learning_rate": 2.6804639072185568e-05,
      "loss": 0.8743,
      "step": 6960
    },
    {
      "epoch": 1.3937212557488503,
      "grad_norm": 13.367127418518066,
      "learning_rate": 2.6771312404185832e-05,
      "loss": 1.0597,
      "step": 6970
    },
    {
      "epoch": 1.3957208558288343,
      "grad_norm": 7.954504489898682,
      "learning_rate": 2.67379857361861e-05,
      "loss": 1.1373,
      "step": 6980
    },
    {
      "epoch": 1.3977204559088183,
      "grad_norm": 8.636181831359863,
      "learning_rate": 2.6704659068186366e-05,
      "loss": 0.9004,
      "step": 6990
    },
    {
      "epoch": 1.3997200559888023,
      "grad_norm": 12.443596839904785,
      "learning_rate": 2.6671332400186632e-05,
      "loss": 0.8751,
      "step": 7000
    },
    {
      "epoch": 1.4017196560687863,
      "grad_norm": 13.689605712890625,
      "learning_rate": 2.66380057321869e-05,
      "loss": 0.9799,
      "step": 7010
    },
    {
      "epoch": 1.4037192561487704,
      "grad_norm": 15.823539733886719,
      "learning_rate": 2.6604679064187166e-05,
      "loss": 1.1044,
      "step": 7020
    },
    {
      "epoch": 1.4057188562287544,
      "grad_norm": 15.663901329040527,
      "learning_rate": 2.6571352396187433e-05,
      "loss": 0.9731,
      "step": 7030
    },
    {
      "epoch": 1.4077184563087384,
      "grad_norm": 6.513339996337891,
      "learning_rate": 2.65380257281877e-05,
      "loss": 0.804,
      "step": 7040
    },
    {
      "epoch": 1.4097180563887224,
      "grad_norm": 14.790840148925781,
      "learning_rate": 2.6504699060187967e-05,
      "loss": 0.9067,
      "step": 7050
    },
    {
      "epoch": 1.4117176564687064,
      "grad_norm": 20.572933197021484,
      "learning_rate": 2.647137239218823e-05,
      "loss": 0.9788,
      "step": 7060
    },
    {
      "epoch": 1.4137172565486902,
      "grad_norm": 20.656469345092773,
      "learning_rate": 2.6438045724188497e-05,
      "loss": 1.1067,
      "step": 7070
    },
    {
      "epoch": 1.4157168566286742,
      "grad_norm": 6.411489009857178,
      "learning_rate": 2.6404719056188764e-05,
      "loss": 0.9607,
      "step": 7080
    },
    {
      "epoch": 1.4177164567086582,
      "grad_norm": 14.918583869934082,
      "learning_rate": 2.637139238818903e-05,
      "loss": 0.9686,
      "step": 7090
    },
    {
      "epoch": 1.4197160567886422,
      "grad_norm": 9.188017845153809,
      "learning_rate": 2.6338065720189298e-05,
      "loss": 0.7416,
      "step": 7100
    },
    {
      "epoch": 1.4217156568686262,
      "grad_norm": 18.110563278198242,
      "learning_rate": 2.6304739052189565e-05,
      "loss": 0.8623,
      "step": 7110
    },
    {
      "epoch": 1.4237152569486102,
      "grad_norm": 18.458988189697266,
      "learning_rate": 2.6271412384189832e-05,
      "loss": 0.8589,
      "step": 7120
    },
    {
      "epoch": 1.4257148570285942,
      "grad_norm": 12.38065242767334,
      "learning_rate": 2.62380857161901e-05,
      "loss": 0.9043,
      "step": 7130
    },
    {
      "epoch": 1.4277144571085782,
      "grad_norm": 9.07278823852539,
      "learning_rate": 2.6204759048190362e-05,
      "loss": 0.9625,
      "step": 7140
    },
    {
      "epoch": 1.4297140571885623,
      "grad_norm": 15.144899368286133,
      "learning_rate": 2.617143238019063e-05,
      "loss": 1.0104,
      "step": 7150
    },
    {
      "epoch": 1.4317136572685463,
      "grad_norm": 14.707409858703613,
      "learning_rate": 2.6138105712190896e-05,
      "loss": 1.0069,
      "step": 7160
    },
    {
      "epoch": 1.4337132573485303,
      "grad_norm": 8.884513854980469,
      "learning_rate": 2.6104779044191163e-05,
      "loss": 1.0113,
      "step": 7170
    },
    {
      "epoch": 1.4357128574285143,
      "grad_norm": 11.681099891662598,
      "learning_rate": 2.607145237619143e-05,
      "loss": 1.0447,
      "step": 7180
    },
    {
      "epoch": 1.4377124575084983,
      "grad_norm": 13.35605239868164,
      "learning_rate": 2.6038125708191697e-05,
      "loss": 0.975,
      "step": 7190
    },
    {
      "epoch": 1.4397120575884823,
      "grad_norm": 9.162775993347168,
      "learning_rate": 2.6004799040191963e-05,
      "loss": 0.7162,
      "step": 7200
    },
    {
      "epoch": 1.4417116576684663,
      "grad_norm": 7.408576488494873,
      "learning_rate": 2.597147237219223e-05,
      "loss": 0.6877,
      "step": 7210
    },
    {
      "epoch": 1.4437112577484503,
      "grad_norm": 13.404364585876465,
      "learning_rate": 2.5938145704192497e-05,
      "loss": 0.873,
      "step": 7220
    },
    {
      "epoch": 1.4457108578284343,
      "grad_norm": 22.734363555908203,
      "learning_rate": 2.590481903619276e-05,
      "loss": 1.05,
      "step": 7230
    },
    {
      "epoch": 1.4477104579084183,
      "grad_norm": 21.75366973876953,
      "learning_rate": 2.5871492368193028e-05,
      "loss": 0.7489,
      "step": 7240
    },
    {
      "epoch": 1.4497100579884024,
      "grad_norm": 11.867043495178223,
      "learning_rate": 2.5838165700193295e-05,
      "loss": 1.085,
      "step": 7250
    },
    {
      "epoch": 1.4517096580683864,
      "grad_norm": 10.844705581665039,
      "learning_rate": 2.580483903219356e-05,
      "loss": 1.0174,
      "step": 7260
    },
    {
      "epoch": 1.4537092581483704,
      "grad_norm": 11.816149711608887,
      "learning_rate": 2.5771512364193828e-05,
      "loss": 1.2057,
      "step": 7270
    },
    {
      "epoch": 1.4557088582283544,
      "grad_norm": 14.048501014709473,
      "learning_rate": 2.5738185696194095e-05,
      "loss": 0.7638,
      "step": 7280
    },
    {
      "epoch": 1.4577084583083384,
      "grad_norm": 11.649151802062988,
      "learning_rate": 2.5704859028194362e-05,
      "loss": 0.6783,
      "step": 7290
    },
    {
      "epoch": 1.4597080583883224,
      "grad_norm": 6.235522270202637,
      "learning_rate": 2.567153236019463e-05,
      "loss": 0.9782,
      "step": 7300
    },
    {
      "epoch": 1.4617076584683064,
      "grad_norm": 9.081123352050781,
      "learning_rate": 2.5638205692194896e-05,
      "loss": 1.0608,
      "step": 7310
    },
    {
      "epoch": 1.4637072585482904,
      "grad_norm": 5.738706588745117,
      "learning_rate": 2.560487902419516e-05,
      "loss": 0.8616,
      "step": 7320
    },
    {
      "epoch": 1.4657068586282742,
      "grad_norm": 13.131057739257812,
      "learning_rate": 2.5571552356195426e-05,
      "loss": 1.2056,
      "step": 7330
    },
    {
      "epoch": 1.4677064587082582,
      "grad_norm": 7.340749740600586,
      "learning_rate": 2.5538225688195693e-05,
      "loss": 0.9675,
      "step": 7340
    },
    {
      "epoch": 1.4697060587882422,
      "grad_norm": 6.818540573120117,
      "learning_rate": 2.550489902019596e-05,
      "loss": 0.8906,
      "step": 7350
    },
    {
      "epoch": 1.4717056588682262,
      "grad_norm": 17.247026443481445,
      "learning_rate": 2.5471572352196227e-05,
      "loss": 1.0204,
      "step": 7360
    },
    {
      "epoch": 1.4737052589482103,
      "grad_norm": 18.791730880737305,
      "learning_rate": 2.5438245684196494e-05,
      "loss": 0.7862,
      "step": 7370
    },
    {
      "epoch": 1.4757048590281943,
      "grad_norm": 7.952294826507568,
      "learning_rate": 2.540491901619676e-05,
      "loss": 0.7909,
      "step": 7380
    },
    {
      "epoch": 1.4777044591081783,
      "grad_norm": 14.551902770996094,
      "learning_rate": 2.5371592348197028e-05,
      "loss": 1.0117,
      "step": 7390
    },
    {
      "epoch": 1.4797040591881623,
      "grad_norm": 11.587174415588379,
      "learning_rate": 2.5338265680197294e-05,
      "loss": 0.8765,
      "step": 7400
    },
    {
      "epoch": 1.4817036592681463,
      "grad_norm": 12.649856567382812,
      "learning_rate": 2.5304939012197558e-05,
      "loss": 1.0564,
      "step": 7410
    },
    {
      "epoch": 1.4837032593481303,
      "grad_norm": 12.81641674041748,
      "learning_rate": 2.5271612344197825e-05,
      "loss": 0.7648,
      "step": 7420
    },
    {
      "epoch": 1.4857028594281143,
      "grad_norm": 13.240670204162598,
      "learning_rate": 2.5238285676198092e-05,
      "loss": 0.9227,
      "step": 7430
    },
    {
      "epoch": 1.4877024595080983,
      "grad_norm": 11.492237091064453,
      "learning_rate": 2.520495900819836e-05,
      "loss": 1.1133,
      "step": 7440
    },
    {
      "epoch": 1.4897020595880823,
      "grad_norm": 7.153125762939453,
      "learning_rate": 2.5171632340198626e-05,
      "loss": 0.6128,
      "step": 7450
    },
    {
      "epoch": 1.4917016596680663,
      "grad_norm": 17.558624267578125,
      "learning_rate": 2.5138305672198896e-05,
      "loss": 0.9597,
      "step": 7460
    },
    {
      "epoch": 1.4937012597480503,
      "grad_norm": 9.896039009094238,
      "learning_rate": 2.5104979004199163e-05,
      "loss": 0.9112,
      "step": 7470
    },
    {
      "epoch": 1.4957008598280344,
      "grad_norm": 8.17568588256836,
      "learning_rate": 2.507165233619943e-05,
      "loss": 1.0399,
      "step": 7480
    },
    {
      "epoch": 1.4977004599080184,
      "grad_norm": 22.580636978149414,
      "learning_rate": 2.5038325668199696e-05,
      "loss": 0.9348,
      "step": 7490
    },
    {
      "epoch": 1.4997000599880024,
      "grad_norm": 10.156831741333008,
      "learning_rate": 2.5004999000199963e-05,
      "loss": 0.8761,
      "step": 7500
    },
    {
      "epoch": 1.5016996600679864,
      "grad_norm": 14.45241928100586,
      "learning_rate": 2.4971672332200227e-05,
      "loss": 0.9312,
      "step": 7510
    },
    {
      "epoch": 1.5036992601479704,
      "grad_norm": 9.735308647155762,
      "learning_rate": 2.4938345664200494e-05,
      "loss": 0.8087,
      "step": 7520
    },
    {
      "epoch": 1.5056988602279544,
      "grad_norm": 18.337432861328125,
      "learning_rate": 2.490501899620076e-05,
      "loss": 0.9242,
      "step": 7530
    },
    {
      "epoch": 1.5076984603079384,
      "grad_norm": 9.010652542114258,
      "learning_rate": 2.4871692328201028e-05,
      "loss": 1.1598,
      "step": 7540
    },
    {
      "epoch": 1.5096980603879224,
      "grad_norm": 33.32761764526367,
      "learning_rate": 2.4838365660201294e-05,
      "loss": 0.8301,
      "step": 7550
    },
    {
      "epoch": 1.5116976604679064,
      "grad_norm": 13.128631591796875,
      "learning_rate": 2.480503899220156e-05,
      "loss": 0.7415,
      "step": 7560
    },
    {
      "epoch": 1.5136972605478904,
      "grad_norm": 20.426292419433594,
      "learning_rate": 2.4771712324201825e-05,
      "loss": 0.7812,
      "step": 7570
    },
    {
      "epoch": 1.5156968606278745,
      "grad_norm": 13.392491340637207,
      "learning_rate": 2.4738385656202095e-05,
      "loss": 1.1417,
      "step": 7580
    },
    {
      "epoch": 1.5176964607078585,
      "grad_norm": 16.634199142456055,
      "learning_rate": 2.4705058988202362e-05,
      "loss": 0.9964,
      "step": 7590
    },
    {
      "epoch": 1.5196960607878425,
      "grad_norm": 12.163681983947754,
      "learning_rate": 2.467173232020263e-05,
      "loss": 0.6733,
      "step": 7600
    },
    {
      "epoch": 1.5216956608678265,
      "grad_norm": 9.320735931396484,
      "learning_rate": 2.4638405652202896e-05,
      "loss": 0.9446,
      "step": 7610
    },
    {
      "epoch": 1.5236952609478105,
      "grad_norm": 10.349356651306152,
      "learning_rate": 2.4605078984203163e-05,
      "loss": 0.9627,
      "step": 7620
    },
    {
      "epoch": 1.5256948610277945,
      "grad_norm": 10.908788681030273,
      "learning_rate": 2.457175231620343e-05,
      "loss": 1.0342,
      "step": 7630
    },
    {
      "epoch": 1.5276944611077785,
      "grad_norm": 20.068336486816406,
      "learning_rate": 2.4538425648203693e-05,
      "loss": 1.0513,
      "step": 7640
    },
    {
      "epoch": 1.5296940611877625,
      "grad_norm": 10.737165451049805,
      "learning_rate": 2.450509898020396e-05,
      "loss": 0.9646,
      "step": 7650
    },
    {
      "epoch": 1.5316936612677465,
      "grad_norm": 13.725641250610352,
      "learning_rate": 2.4471772312204227e-05,
      "loss": 0.898,
      "step": 7660
    },
    {
      "epoch": 1.5336932613477305,
      "grad_norm": 12.5100679397583,
      "learning_rate": 2.4438445644204494e-05,
      "loss": 0.8647,
      "step": 7670
    },
    {
      "epoch": 1.5356928614277146,
      "grad_norm": 10.116689682006836,
      "learning_rate": 2.440511897620476e-05,
      "loss": 1.0739,
      "step": 7680
    },
    {
      "epoch": 1.5376924615076986,
      "grad_norm": 18.53221893310547,
      "learning_rate": 2.4371792308205028e-05,
      "loss": 1.0724,
      "step": 7690
    },
    {
      "epoch": 1.5396920615876826,
      "grad_norm": 12.735157012939453,
      "learning_rate": 2.4338465640205294e-05,
      "loss": 1.0413,
      "step": 7700
    },
    {
      "epoch": 1.5416916616676666,
      "grad_norm": 11.698009490966797,
      "learning_rate": 2.430513897220556e-05,
      "loss": 0.874,
      "step": 7710
    },
    {
      "epoch": 1.5436912617476506,
      "grad_norm": 14.04918384552002,
      "learning_rate": 2.4271812304205828e-05,
      "loss": 0.8528,
      "step": 7720
    },
    {
      "epoch": 1.5456908618276346,
      "grad_norm": 7.109793663024902,
      "learning_rate": 2.423848563620609e-05,
      "loss": 0.7201,
      "step": 7730
    },
    {
      "epoch": 1.5476904619076186,
      "grad_norm": 6.678958892822266,
      "learning_rate": 2.420515896820636e-05,
      "loss": 0.968,
      "step": 7740
    },
    {
      "epoch": 1.5496900619876026,
      "grad_norm": 14.670111656188965,
      "learning_rate": 2.4171832300206625e-05,
      "loss": 0.901,
      "step": 7750
    },
    {
      "epoch": 1.5516896620675866,
      "grad_norm": 3.9040656089782715,
      "learning_rate": 2.4138505632206892e-05,
      "loss": 0.9316,
      "step": 7760
    },
    {
      "epoch": 1.5536892621475706,
      "grad_norm": 19.196956634521484,
      "learning_rate": 2.410517896420716e-05,
      "loss": 1.1955,
      "step": 7770
    },
    {
      "epoch": 1.5556888622275546,
      "grad_norm": 18.603960037231445,
      "learning_rate": 2.4071852296207426e-05,
      "loss": 0.9198,
      "step": 7780
    },
    {
      "epoch": 1.5576884623075387,
      "grad_norm": 15.166921615600586,
      "learning_rate": 2.4038525628207693e-05,
      "loss": 0.8943,
      "step": 7790
    },
    {
      "epoch": 1.5596880623875224,
      "grad_norm": 17.24521827697754,
      "learning_rate": 2.400519896020796e-05,
      "loss": 0.9201,
      "step": 7800
    },
    {
      "epoch": 1.5616876624675065,
      "grad_norm": 7.711451053619385,
      "learning_rate": 2.3971872292208223e-05,
      "loss": 0.7306,
      "step": 7810
    },
    {
      "epoch": 1.5636872625474905,
      "grad_norm": 9.45329475402832,
      "learning_rate": 2.393854562420849e-05,
      "loss": 0.5992,
      "step": 7820
    },
    {
      "epoch": 1.5656868626274745,
      "grad_norm": 10.916348457336426,
      "learning_rate": 2.3905218956208757e-05,
      "loss": 0.9465,
      "step": 7830
    },
    {
      "epoch": 1.5676864627074585,
      "grad_norm": 25.377071380615234,
      "learning_rate": 2.3871892288209027e-05,
      "loss": 0.895,
      "step": 7840
    },
    {
      "epoch": 1.5696860627874425,
      "grad_norm": 27.933053970336914,
      "learning_rate": 2.3838565620209294e-05,
      "loss": 0.9828,
      "step": 7850
    },
    {
      "epoch": 1.5716856628674265,
      "grad_norm": 10.246896743774414,
      "learning_rate": 2.380523895220956e-05,
      "loss": 0.8133,
      "step": 7860
    },
    {
      "epoch": 1.5736852629474105,
      "grad_norm": 13.863579750061035,
      "learning_rate": 2.3771912284209828e-05,
      "loss": 0.9457,
      "step": 7870
    },
    {
      "epoch": 1.5756848630273945,
      "grad_norm": 8.989386558532715,
      "learning_rate": 2.3738585616210095e-05,
      "loss": 0.8622,
      "step": 7880
    },
    {
      "epoch": 1.5776844631073785,
      "grad_norm": 13.264229774475098,
      "learning_rate": 2.370525894821036e-05,
      "loss": 1.1234,
      "step": 7890
    },
    {
      "epoch": 1.5796840631873625,
      "grad_norm": 10.056224822998047,
      "learning_rate": 2.3671932280210625e-05,
      "loss": 0.9853,
      "step": 7900
    },
    {
      "epoch": 1.5816836632673466,
      "grad_norm": 12.802730560302734,
      "learning_rate": 2.3638605612210892e-05,
      "loss": 0.8224,
      "step": 7910
    },
    {
      "epoch": 1.5836832633473306,
      "grad_norm": 16.58254623413086,
      "learning_rate": 2.360527894421116e-05,
      "loss": 0.9976,
      "step": 7920
    },
    {
      "epoch": 1.5856828634273146,
      "grad_norm": 11.844390869140625,
      "learning_rate": 2.3571952276211426e-05,
      "loss": 0.888,
      "step": 7930
    },
    {
      "epoch": 1.5876824635072986,
      "grad_norm": 5.788026332855225,
      "learning_rate": 2.3538625608211693e-05,
      "loss": 0.6935,
      "step": 7940
    },
    {
      "epoch": 1.5896820635872826,
      "grad_norm": 9.781177520751953,
      "learning_rate": 2.350529894021196e-05,
      "loss": 0.6938,
      "step": 7950
    },
    {
      "epoch": 1.5916816636672666,
      "grad_norm": 21.769424438476562,
      "learning_rate": 2.3471972272212227e-05,
      "loss": 1.0358,
      "step": 7960
    },
    {
      "epoch": 1.5936812637472504,
      "grad_norm": 11.132895469665527,
      "learning_rate": 2.343864560421249e-05,
      "loss": 0.8865,
      "step": 7970
    },
    {
      "epoch": 1.5956808638272344,
      "grad_norm": 27.089752197265625,
      "learning_rate": 2.3405318936212757e-05,
      "loss": 0.8815,
      "step": 7980
    },
    {
      "epoch": 1.5976804639072184,
      "grad_norm": 8.712761878967285,
      "learning_rate": 2.3371992268213024e-05,
      "loss": 1.0101,
      "step": 7990
    },
    {
      "epoch": 1.5996800639872024,
      "grad_norm": 5.68848180770874,
      "learning_rate": 2.333866560021329e-05,
      "loss": 0.9092,
      "step": 8000
    },
    {
      "epoch": 1.6016796640671864,
      "grad_norm": 6.3105950355529785,
      "learning_rate": 2.3305338932213558e-05,
      "loss": 0.8517,
      "step": 8010
    },
    {
      "epoch": 1.6036792641471704,
      "grad_norm": 2.6986207962036133,
      "learning_rate": 2.3272012264213825e-05,
      "loss": 1.0083,
      "step": 8020
    },
    {
      "epoch": 1.6056788642271544,
      "grad_norm": 19.223224639892578,
      "learning_rate": 2.323868559621409e-05,
      "loss": 0.8173,
      "step": 8030
    },
    {
      "epoch": 1.6076784643071385,
      "grad_norm": 28.298616409301758,
      "learning_rate": 2.320535892821436e-05,
      "loss": 0.7637,
      "step": 8040
    },
    {
      "epoch": 1.6096780643871225,
      "grad_norm": 18.474109649658203,
      "learning_rate": 2.3172032260214625e-05,
      "loss": 0.9525,
      "step": 8050
    },
    {
      "epoch": 1.6116776644671065,
      "grad_norm": 10.524460792541504,
      "learning_rate": 2.313870559221489e-05,
      "loss": 1.0375,
      "step": 8060
    },
    {
      "epoch": 1.6136772645470905,
      "grad_norm": 10.27405071258545,
      "learning_rate": 2.3105378924215156e-05,
      "loss": 0.9828,
      "step": 8070
    },
    {
      "epoch": 1.6156768646270745,
      "grad_norm": 6.650126934051514,
      "learning_rate": 2.3072052256215423e-05,
      "loss": 0.7584,
      "step": 8080
    },
    {
      "epoch": 1.6176764647070585,
      "grad_norm": 15.64673900604248,
      "learning_rate": 2.303872558821569e-05,
      "loss": 1.0182,
      "step": 8090
    },
    {
      "epoch": 1.6196760647870425,
      "grad_norm": 20.677635192871094,
      "learning_rate": 2.300539892021596e-05,
      "loss": 1.0128,
      "step": 8100
    },
    {
      "epoch": 1.6216756648670265,
      "grad_norm": 16.926610946655273,
      "learning_rate": 2.2972072252216227e-05,
      "loss": 1.1034,
      "step": 8110
    },
    {
      "epoch": 1.6236752649470105,
      "grad_norm": 14.85013198852539,
      "learning_rate": 2.2938745584216494e-05,
      "loss": 0.7246,
      "step": 8120
    },
    {
      "epoch": 1.6256748650269945,
      "grad_norm": 7.353440761566162,
      "learning_rate": 2.2905418916216757e-05,
      "loss": 0.7547,
      "step": 8130
    },
    {
      "epoch": 1.6276744651069786,
      "grad_norm": 8.178986549377441,
      "learning_rate": 2.2872092248217024e-05,
      "loss": 0.8136,
      "step": 8140
    },
    {
      "epoch": 1.6296740651869626,
      "grad_norm": 14.500388145446777,
      "learning_rate": 2.283876558021729e-05,
      "loss": 0.7895,
      "step": 8150
    },
    {
      "epoch": 1.6316736652669466,
      "grad_norm": 15.035855293273926,
      "learning_rate": 2.2805438912217558e-05,
      "loss": 1.2386,
      "step": 8160
    },
    {
      "epoch": 1.6336732653469306,
      "grad_norm": 21.421194076538086,
      "learning_rate": 2.2772112244217825e-05,
      "loss": 1.1336,
      "step": 8170
    },
    {
      "epoch": 1.6356728654269146,
      "grad_norm": 9.28275203704834,
      "learning_rate": 2.273878557621809e-05,
      "loss": 0.858,
      "step": 8180
    },
    {
      "epoch": 1.6376724655068986,
      "grad_norm": 12.54520320892334,
      "learning_rate": 2.270545890821836e-05,
      "loss": 0.8215,
      "step": 8190
    },
    {
      "epoch": 1.6396720655868826,
      "grad_norm": 16.689228057861328,
      "learning_rate": 2.2672132240218625e-05,
      "loss": 0.8317,
      "step": 8200
    },
    {
      "epoch": 1.6416716656668666,
      "grad_norm": 8.203595161437988,
      "learning_rate": 2.2638805572218892e-05,
      "loss": 1.0769,
      "step": 8210
    },
    {
      "epoch": 1.6436712657468506,
      "grad_norm": 7.193220615386963,
      "learning_rate": 2.2605478904219156e-05,
      "loss": 1.1434,
      "step": 8220
    },
    {
      "epoch": 1.6456708658268346,
      "grad_norm": 5.279370307922363,
      "learning_rate": 2.2572152236219423e-05,
      "loss": 1.0614,
      "step": 8230
    },
    {
      "epoch": 1.6476704659068186,
      "grad_norm": 8.350167274475098,
      "learning_rate": 2.253882556821969e-05,
      "loss": 0.963,
      "step": 8240
    },
    {
      "epoch": 1.6496700659868027,
      "grad_norm": 4.027693748474121,
      "learning_rate": 2.2505498900219956e-05,
      "loss": 0.9206,
      "step": 8250
    },
    {
      "epoch": 1.6516696660667867,
      "grad_norm": 23.948497772216797,
      "learning_rate": 2.2472172232220223e-05,
      "loss": 0.8297,
      "step": 8260
    },
    {
      "epoch": 1.6536692661467707,
      "grad_norm": 19.319717407226562,
      "learning_rate": 2.243884556422049e-05,
      "loss": 0.8948,
      "step": 8270
    },
    {
      "epoch": 1.6556688662267547,
      "grad_norm": 22.865127563476562,
      "learning_rate": 2.2405518896220757e-05,
      "loss": 0.9914,
      "step": 8280
    },
    {
      "epoch": 1.6576684663067387,
      "grad_norm": 10.844398498535156,
      "learning_rate": 2.2372192228221024e-05,
      "loss": 0.8465,
      "step": 8290
    },
    {
      "epoch": 1.6596680663867227,
      "grad_norm": 8.119958877563477,
      "learning_rate": 2.233886556022129e-05,
      "loss": 0.6153,
      "step": 8300
    },
    {
      "epoch": 1.6616676664667067,
      "grad_norm": 10.448104858398438,
      "learning_rate": 2.2305538892221554e-05,
      "loss": 0.9316,
      "step": 8310
    },
    {
      "epoch": 1.6636672665466907,
      "grad_norm": 10.083983421325684,
      "learning_rate": 2.227221222422182e-05,
      "loss": 0.9181,
      "step": 8320
    },
    {
      "epoch": 1.6656668666266747,
      "grad_norm": 6.744692802429199,
      "learning_rate": 2.2238885556222088e-05,
      "loss": 0.9771,
      "step": 8330
    },
    {
      "epoch": 1.6676664667066587,
      "grad_norm": 10.539505958557129,
      "learning_rate": 2.2205558888222355e-05,
      "loss": 1.0089,
      "step": 8340
    },
    {
      "epoch": 1.6696660667866428,
      "grad_norm": 15.05086612701416,
      "learning_rate": 2.2172232220222625e-05,
      "loss": 0.822,
      "step": 8350
    },
    {
      "epoch": 1.6716656668666268,
      "grad_norm": 12.63435173034668,
      "learning_rate": 2.2138905552222892e-05,
      "loss": 1.0752,
      "step": 8360
    },
    {
      "epoch": 1.6736652669466108,
      "grad_norm": 15.257124900817871,
      "learning_rate": 2.210557888422316e-05,
      "loss": 0.8581,
      "step": 8370
    },
    {
      "epoch": 1.6756648670265948,
      "grad_norm": 7.550647258758545,
      "learning_rate": 2.2072252216223423e-05,
      "loss": 0.8571,
      "step": 8380
    },
    {
      "epoch": 1.6776644671065788,
      "grad_norm": 12.365416526794434,
      "learning_rate": 2.203892554822369e-05,
      "loss": 0.6975,
      "step": 8390
    },
    {
      "epoch": 1.6796640671865628,
      "grad_norm": 10.107141494750977,
      "learning_rate": 2.2005598880223956e-05,
      "loss": 0.9625,
      "step": 8400
    },
    {
      "epoch": 1.6816636672665468,
      "grad_norm": 6.639669895172119,
      "learning_rate": 2.1972272212224223e-05,
      "loss": 0.8825,
      "step": 8410
    },
    {
      "epoch": 1.6836632673465308,
      "grad_norm": 12.808358192443848,
      "learning_rate": 2.193894554422449e-05,
      "loss": 0.8331,
      "step": 8420
    },
    {
      "epoch": 1.6856628674265148,
      "grad_norm": 9.29654312133789,
      "learning_rate": 2.1905618876224757e-05,
      "loss": 0.8141,
      "step": 8430
    },
    {
      "epoch": 1.6876624675064988,
      "grad_norm": 11.545607566833496,
      "learning_rate": 2.1872292208225024e-05,
      "loss": 0.8114,
      "step": 8440
    },
    {
      "epoch": 1.6896620675864829,
      "grad_norm": 10.409873962402344,
      "learning_rate": 2.183896554022529e-05,
      "loss": 0.7724,
      "step": 8450
    },
    {
      "epoch": 1.6916616676664669,
      "grad_norm": 3.2900426387786865,
      "learning_rate": 2.1805638872225558e-05,
      "loss": 0.7539,
      "step": 8460
    },
    {
      "epoch": 1.6936612677464509,
      "grad_norm": 13.953104972839355,
      "learning_rate": 2.177231220422582e-05,
      "loss": 1.1126,
      "step": 8470
    },
    {
      "epoch": 1.6956608678264347,
      "grad_norm": 12.057395935058594,
      "learning_rate": 2.1738985536226088e-05,
      "loss": 0.7607,
      "step": 8480
    },
    {
      "epoch": 1.6976604679064187,
      "grad_norm": 13.12625789642334,
      "learning_rate": 2.1705658868226355e-05,
      "loss": 1.0034,
      "step": 8490
    },
    {
      "epoch": 1.6996600679864027,
      "grad_norm": 10.311214447021484,
      "learning_rate": 2.1672332200226622e-05,
      "loss": 0.8478,
      "step": 8500
    },
    {
      "epoch": 1.7016596680663867,
      "grad_norm": 17.286720275878906,
      "learning_rate": 2.163900553222689e-05,
      "loss": 0.7595,
      "step": 8510
    },
    {
      "epoch": 1.7036592681463707,
      "grad_norm": 15.64364242553711,
      "learning_rate": 2.1605678864227156e-05,
      "loss": 0.9551,
      "step": 8520
    },
    {
      "epoch": 1.7056588682263547,
      "grad_norm": 19.75090789794922,
      "learning_rate": 2.1572352196227423e-05,
      "loss": 1.0511,
      "step": 8530
    },
    {
      "epoch": 1.7076584683063387,
      "grad_norm": 5.147335529327393,
      "learning_rate": 2.153902552822769e-05,
      "loss": 0.8334,
      "step": 8540
    },
    {
      "epoch": 1.7096580683863227,
      "grad_norm": 7.920611381530762,
      "learning_rate": 2.1505698860227956e-05,
      "loss": 1.0475,
      "step": 8550
    },
    {
      "epoch": 1.7116576684663067,
      "grad_norm": 10.065372467041016,
      "learning_rate": 2.147237219222822e-05,
      "loss": 0.9338,
      "step": 8560
    },
    {
      "epoch": 1.7136572685462907,
      "grad_norm": 10.696304321289062,
      "learning_rate": 2.1439045524228487e-05,
      "loss": 0.844,
      "step": 8570
    },
    {
      "epoch": 1.7156568686262748,
      "grad_norm": 13.630880355834961,
      "learning_rate": 2.1405718856228754e-05,
      "loss": 1.0843,
      "step": 8580
    },
    {
      "epoch": 1.7176564687062588,
      "grad_norm": 10.213717460632324,
      "learning_rate": 2.137239218822902e-05,
      "loss": 0.7853,
      "step": 8590
    },
    {
      "epoch": 1.7196560687862428,
      "grad_norm": 10.085070610046387,
      "learning_rate": 2.1339065520229287e-05,
      "loss": 0.6855,
      "step": 8600
    },
    {
      "epoch": 1.7216556688662268,
      "grad_norm": 12.72898006439209,
      "learning_rate": 2.1305738852229558e-05,
      "loss": 0.9861,
      "step": 8610
    },
    {
      "epoch": 1.7236552689462108,
      "grad_norm": 16.236072540283203,
      "learning_rate": 2.1272412184229825e-05,
      "loss": 1.0747,
      "step": 8620
    },
    {
      "epoch": 1.7256548690261948,
      "grad_norm": 12.683197021484375,
      "learning_rate": 2.1239085516230088e-05,
      "loss": 0.8788,
      "step": 8630
    },
    {
      "epoch": 1.7276544691061788,
      "grad_norm": 12.480019569396973,
      "learning_rate": 2.1205758848230355e-05,
      "loss": 0.8819,
      "step": 8640
    },
    {
      "epoch": 1.7296540691861626,
      "grad_norm": 14.541876792907715,
      "learning_rate": 2.1172432180230622e-05,
      "loss": 0.8669,
      "step": 8650
    },
    {
      "epoch": 1.7316536692661466,
      "grad_norm": 3.758991003036499,
      "learning_rate": 2.113910551223089e-05,
      "loss": 0.8818,
      "step": 8660
    },
    {
      "epoch": 1.7336532693461306,
      "grad_norm": 12.65374755859375,
      "learning_rate": 2.1105778844231156e-05,
      "loss": 1.065,
      "step": 8670
    },
    {
      "epoch": 1.7356528694261146,
      "grad_norm": 11.451781272888184,
      "learning_rate": 2.1072452176231423e-05,
      "loss": 1.1107,
      "step": 8680
    },
    {
      "epoch": 1.7376524695060986,
      "grad_norm": 13.226959228515625,
      "learning_rate": 2.103912550823169e-05,
      "loss": 1.1821,
      "step": 8690
    },
    {
      "epoch": 1.7396520695860826,
      "grad_norm": 19.20694923400879,
      "learning_rate": 2.1005798840231956e-05,
      "loss": 0.8511,
      "step": 8700
    },
    {
      "epoch": 1.7416516696660667,
      "grad_norm": 15.77137565612793,
      "learning_rate": 2.097247217223222e-05,
      "loss": 0.9531,
      "step": 8710
    },
    {
      "epoch": 1.7436512697460507,
      "grad_norm": 17.072975158691406,
      "learning_rate": 2.0939145504232487e-05,
      "loss": 0.8385,
      "step": 8720
    },
    {
      "epoch": 1.7456508698260347,
      "grad_norm": 10.083065032958984,
      "learning_rate": 2.0905818836232754e-05,
      "loss": 1.0256,
      "step": 8730
    },
    {
      "epoch": 1.7476504699060187,
      "grad_norm": 9.85015869140625,
      "learning_rate": 2.087249216823302e-05,
      "loss": 1.0439,
      "step": 8740
    },
    {
      "epoch": 1.7496500699860027,
      "grad_norm": 16.12037467956543,
      "learning_rate": 2.0839165500233287e-05,
      "loss": 1.0087,
      "step": 8750
    },
    {
      "epoch": 1.7516496700659867,
      "grad_norm": 10.927610397338867,
      "learning_rate": 2.0805838832233554e-05,
      "loss": 1.038,
      "step": 8760
    },
    {
      "epoch": 1.7536492701459707,
      "grad_norm": 11.44180679321289,
      "learning_rate": 2.077251216423382e-05,
      "loss": 0.9177,
      "step": 8770
    },
    {
      "epoch": 1.7556488702259547,
      "grad_norm": 11.606925010681152,
      "learning_rate": 2.0739185496234088e-05,
      "loss": 1.0563,
      "step": 8780
    },
    {
      "epoch": 1.7576484703059387,
      "grad_norm": 9.021439552307129,
      "learning_rate": 2.0705858828234355e-05,
      "loss": 0.6828,
      "step": 8790
    },
    {
      "epoch": 1.7596480703859227,
      "grad_norm": 11.329367637634277,
      "learning_rate": 2.067253216023462e-05,
      "loss": 0.8777,
      "step": 8800
    },
    {
      "epoch": 1.7616476704659068,
      "grad_norm": 10.107690811157227,
      "learning_rate": 2.0639205492234885e-05,
      "loss": 0.8922,
      "step": 8810
    },
    {
      "epoch": 1.7636472705458908,
      "grad_norm": 13.615962982177734,
      "learning_rate": 2.0605878824235152e-05,
      "loss": 0.9824,
      "step": 8820
    },
    {
      "epoch": 1.7656468706258748,
      "grad_norm": 17.762266159057617,
      "learning_rate": 2.057255215623542e-05,
      "loss": 1.1563,
      "step": 8830
    },
    {
      "epoch": 1.7676464707058588,
      "grad_norm": 12.241039276123047,
      "learning_rate": 2.0539225488235686e-05,
      "loss": 0.7723,
      "step": 8840
    },
    {
      "epoch": 1.7696460707858428,
      "grad_norm": 9.065059661865234,
      "learning_rate": 2.0505898820235953e-05,
      "loss": 0.8828,
      "step": 8850
    },
    {
      "epoch": 1.7716456708658268,
      "grad_norm": 20.97224998474121,
      "learning_rate": 2.047257215223622e-05,
      "loss": 0.814,
      "step": 8860
    },
    {
      "epoch": 1.7736452709458108,
      "grad_norm": 15.479120254516602,
      "learning_rate": 2.0439245484236487e-05,
      "loss": 0.9221,
      "step": 8870
    },
    {
      "epoch": 1.7756448710257948,
      "grad_norm": 10.539931297302246,
      "learning_rate": 2.0405918816236754e-05,
      "loss": 1.1889,
      "step": 8880
    },
    {
      "epoch": 1.7776444711057788,
      "grad_norm": 9.529964447021484,
      "learning_rate": 2.037259214823702e-05,
      "loss": 0.9212,
      "step": 8890
    },
    {
      "epoch": 1.7796440711857628,
      "grad_norm": 15.20002269744873,
      "learning_rate": 2.0339265480237287e-05,
      "loss": 0.9207,
      "step": 8900
    },
    {
      "epoch": 1.7816436712657469,
      "grad_norm": 16.73724365234375,
      "learning_rate": 2.0305938812237554e-05,
      "loss": 1.0364,
      "step": 8910
    },
    {
      "epoch": 1.7836432713457309,
      "grad_norm": 10.436765670776367,
      "learning_rate": 2.027261214423782e-05,
      "loss": 1.0879,
      "step": 8920
    },
    {
      "epoch": 1.7856428714257149,
      "grad_norm": 12.81459903717041,
      "learning_rate": 2.0239285476238088e-05,
      "loss": 0.7415,
      "step": 8930
    },
    {
      "epoch": 1.7876424715056989,
      "grad_norm": 19.77081298828125,
      "learning_rate": 2.0205958808238355e-05,
      "loss": 1.0254,
      "step": 8940
    },
    {
      "epoch": 1.789642071585683,
      "grad_norm": 10.381092071533203,
      "learning_rate": 2.0172632140238622e-05,
      "loss": 0.9298,
      "step": 8950
    },
    {
      "epoch": 1.791641671665667,
      "grad_norm": 10.86275863647461,
      "learning_rate": 2.0139305472238885e-05,
      "loss": 1.0363,
      "step": 8960
    },
    {
      "epoch": 1.793641271745651,
      "grad_norm": 13.695610046386719,
      "learning_rate": 2.0105978804239152e-05,
      "loss": 0.9721,
      "step": 8970
    },
    {
      "epoch": 1.795640871825635,
      "grad_norm": 15.264116287231445,
      "learning_rate": 2.007265213623942e-05,
      "loss": 0.9164,
      "step": 8980
    },
    {
      "epoch": 1.797640471905619,
      "grad_norm": 14.342418670654297,
      "learning_rate": 2.0039325468239686e-05,
      "loss": 0.8597,
      "step": 8990
    },
    {
      "epoch": 1.799640071985603,
      "grad_norm": 17.930665969848633,
      "learning_rate": 2.0005998800239953e-05,
      "loss": 0.9225,
      "step": 9000
    },
    {
      "epoch": 1.801639672065587,
      "grad_norm": 15.836463928222656,
      "learning_rate": 1.997267213224022e-05,
      "loss": 0.7023,
      "step": 9010
    },
    {
      "epoch": 1.803639272145571,
      "grad_norm": 12.789152145385742,
      "learning_rate": 1.9939345464240487e-05,
      "loss": 0.8973,
      "step": 9020
    },
    {
      "epoch": 1.805638872225555,
      "grad_norm": 12.876086235046387,
      "learning_rate": 1.9906018796240754e-05,
      "loss": 0.81,
      "step": 9030
    },
    {
      "epoch": 1.807638472305539,
      "grad_norm": 7.880180358886719,
      "learning_rate": 1.987269212824102e-05,
      "loss": 0.8356,
      "step": 9040
    },
    {
      "epoch": 1.809638072385523,
      "grad_norm": 13.32410717010498,
      "learning_rate": 1.9839365460241284e-05,
      "loss": 1.1132,
      "step": 9050
    },
    {
      "epoch": 1.811637672465507,
      "grad_norm": 27.95636558532715,
      "learning_rate": 1.980603879224155e-05,
      "loss": 1.0856,
      "step": 9060
    },
    {
      "epoch": 1.813637272545491,
      "grad_norm": 7.560559272766113,
      "learning_rate": 1.9772712124241818e-05,
      "loss": 0.7827,
      "step": 9070
    },
    {
      "epoch": 1.815636872625475,
      "grad_norm": 15.392495155334473,
      "learning_rate": 1.9739385456242085e-05,
      "loss": 0.8442,
      "step": 9080
    },
    {
      "epoch": 1.817636472705459,
      "grad_norm": 17.45170021057129,
      "learning_rate": 1.970605878824235e-05,
      "loss": 0.792,
      "step": 9090
    },
    {
      "epoch": 1.819636072785443,
      "grad_norm": 15.709832191467285,
      "learning_rate": 1.967273212024262e-05,
      "loss": 0.7872,
      "step": 9100
    },
    {
      "epoch": 1.821635672865427,
      "grad_norm": 13.550786972045898,
      "learning_rate": 1.9639405452242885e-05,
      "loss": 0.9873,
      "step": 9110
    },
    {
      "epoch": 1.823635272945411,
      "grad_norm": 15.451420783996582,
      "learning_rate": 1.9606078784243152e-05,
      "loss": 0.7994,
      "step": 9120
    },
    {
      "epoch": 1.825634873025395,
      "grad_norm": 15.823291778564453,
      "learning_rate": 1.957275211624342e-05,
      "loss": 1.0405,
      "step": 9130
    },
    {
      "epoch": 1.827634473105379,
      "grad_norm": 10.39190673828125,
      "learning_rate": 1.9539425448243686e-05,
      "loss": 0.8549,
      "step": 9140
    },
    {
      "epoch": 1.829634073185363,
      "grad_norm": 15.095505714416504,
      "learning_rate": 1.9506098780243953e-05,
      "loss": 0.9611,
      "step": 9150
    },
    {
      "epoch": 1.831633673265347,
      "grad_norm": 16.204410552978516,
      "learning_rate": 1.947277211224422e-05,
      "loss": 0.9971,
      "step": 9160
    },
    {
      "epoch": 1.8336332733453309,
      "grad_norm": 6.4726362228393555,
      "learning_rate": 1.9439445444244487e-05,
      "loss": 0.8253,
      "step": 9170
    },
    {
      "epoch": 1.835632873425315,
      "grad_norm": 11.180807113647461,
      "learning_rate": 1.9406118776244753e-05,
      "loss": 0.8543,
      "step": 9180
    },
    {
      "epoch": 1.837632473505299,
      "grad_norm": 21.38705062866211,
      "learning_rate": 1.937279210824502e-05,
      "loss": 0.9153,
      "step": 9190
    },
    {
      "epoch": 1.839632073585283,
      "grad_norm": 18.402969360351562,
      "learning_rate": 1.9339465440245287e-05,
      "loss": 0.9414,
      "step": 9200
    },
    {
      "epoch": 1.841631673665267,
      "grad_norm": 6.903501033782959,
      "learning_rate": 1.930613877224555e-05,
      "loss": 0.881,
      "step": 9210
    },
    {
      "epoch": 1.843631273745251,
      "grad_norm": 7.905416011810303,
      "learning_rate": 1.9272812104245818e-05,
      "loss": 0.9907,
      "step": 9220
    },
    {
      "epoch": 1.845630873825235,
      "grad_norm": 10.56607723236084,
      "learning_rate": 1.9239485436246085e-05,
      "loss": 0.8483,
      "step": 9230
    },
    {
      "epoch": 1.847630473905219,
      "grad_norm": 14.055285453796387,
      "learning_rate": 1.920615876824635e-05,
      "loss": 1.022,
      "step": 9240
    },
    {
      "epoch": 1.849630073985203,
      "grad_norm": 11.782842636108398,
      "learning_rate": 1.917283210024662e-05,
      "loss": 1.0266,
      "step": 9250
    },
    {
      "epoch": 1.851629674065187,
      "grad_norm": 10.175871849060059,
      "learning_rate": 1.9139505432246885e-05,
      "loss": 0.8575,
      "step": 9260
    },
    {
      "epoch": 1.853629274145171,
      "grad_norm": 9.090228080749512,
      "learning_rate": 1.9106178764247152e-05,
      "loss": 0.9012,
      "step": 9270
    },
    {
      "epoch": 1.855628874225155,
      "grad_norm": 7.652507781982422,
      "learning_rate": 1.907285209624742e-05,
      "loss": 0.8924,
      "step": 9280
    },
    {
      "epoch": 1.857628474305139,
      "grad_norm": 15.255035400390625,
      "learning_rate": 1.9039525428247686e-05,
      "loss": 0.9179,
      "step": 9290
    },
    {
      "epoch": 1.859628074385123,
      "grad_norm": 10.98122787475586,
      "learning_rate": 1.900619876024795e-05,
      "loss": 1.0818,
      "step": 9300
    },
    {
      "epoch": 1.861627674465107,
      "grad_norm": 15.384321212768555,
      "learning_rate": 1.8972872092248216e-05,
      "loss": 0.9836,
      "step": 9310
    },
    {
      "epoch": 1.863627274545091,
      "grad_norm": 26.9388427734375,
      "learning_rate": 1.8939545424248483e-05,
      "loss": 0.8974,
      "step": 9320
    },
    {
      "epoch": 1.865626874625075,
      "grad_norm": 12.916791915893555,
      "learning_rate": 1.890621875624875e-05,
      "loss": 0.7557,
      "step": 9330
    },
    {
      "epoch": 1.8676264747050588,
      "grad_norm": 13.508138656616211,
      "learning_rate": 1.8872892088249017e-05,
      "loss": 0.7722,
      "step": 9340
    },
    {
      "epoch": 1.8696260747850428,
      "grad_norm": 14.396242141723633,
      "learning_rate": 1.8839565420249284e-05,
      "loss": 1.3622,
      "step": 9350
    },
    {
      "epoch": 1.8716256748650268,
      "grad_norm": 10.766502380371094,
      "learning_rate": 1.880623875224955e-05,
      "loss": 0.7723,
      "step": 9360
    },
    {
      "epoch": 1.8736252749450109,
      "grad_norm": 10.913481712341309,
      "learning_rate": 1.8772912084249818e-05,
      "loss": 1.0079,
      "step": 9370
    },
    {
      "epoch": 1.8756248750249949,
      "grad_norm": 20.435184478759766,
      "learning_rate": 1.8739585416250085e-05,
      "loss": 0.6538,
      "step": 9380
    },
    {
      "epoch": 1.8776244751049789,
      "grad_norm": 17.866680145263672,
      "learning_rate": 1.870625874825035e-05,
      "loss": 0.8944,
      "step": 9390
    },
    {
      "epoch": 1.8796240751849629,
      "grad_norm": 16.603431701660156,
      "learning_rate": 1.8672932080250618e-05,
      "loss": 0.8338,
      "step": 9400
    },
    {
      "epoch": 1.881623675264947,
      "grad_norm": 13.20230770111084,
      "learning_rate": 1.8639605412250885e-05,
      "loss": 0.8271,
      "step": 9410
    },
    {
      "epoch": 1.883623275344931,
      "grad_norm": 7.094874858856201,
      "learning_rate": 1.8606278744251152e-05,
      "loss": 1.0168,
      "step": 9420
    },
    {
      "epoch": 1.885622875424915,
      "grad_norm": 10.963252067565918,
      "learning_rate": 1.857295207625142e-05,
      "loss": 0.8206,
      "step": 9430
    },
    {
      "epoch": 1.887622475504899,
      "grad_norm": 16.15831756591797,
      "learning_rate": 1.8539625408251686e-05,
      "loss": 0.7223,
      "step": 9440
    },
    {
      "epoch": 1.889622075584883,
      "grad_norm": 13.24299144744873,
      "learning_rate": 1.8506298740251953e-05,
      "loss": 0.7259,
      "step": 9450
    },
    {
      "epoch": 1.891621675664867,
      "grad_norm": 7.246231555938721,
      "learning_rate": 1.8472972072252216e-05,
      "loss": 0.994,
      "step": 9460
    },
    {
      "epoch": 1.893621275744851,
      "grad_norm": 9.332191467285156,
      "learning_rate": 1.8439645404252483e-05,
      "loss": 0.7954,
      "step": 9470
    },
    {
      "epoch": 1.895620875824835,
      "grad_norm": 17.472156524658203,
      "learning_rate": 1.840631873625275e-05,
      "loss": 1.0061,
      "step": 9480
    },
    {
      "epoch": 1.897620475904819,
      "grad_norm": 11.535503387451172,
      "learning_rate": 1.8372992068253017e-05,
      "loss": 1.0386,
      "step": 9490
    },
    {
      "epoch": 1.899620075984803,
      "grad_norm": 15.05003547668457,
      "learning_rate": 1.8339665400253284e-05,
      "loss": 0.8138,
      "step": 9500
    },
    {
      "epoch": 1.901619676064787,
      "grad_norm": 13.669083595275879,
      "learning_rate": 1.830633873225355e-05,
      "loss": 0.7975,
      "step": 9510
    },
    {
      "epoch": 1.903619276144771,
      "grad_norm": 13.927993774414062,
      "learning_rate": 1.8273012064253818e-05,
      "loss": 1.1258,
      "step": 9520
    },
    {
      "epoch": 1.905618876224755,
      "grad_norm": 1.2419037818908691,
      "learning_rate": 1.8239685396254084e-05,
      "loss": 0.7873,
      "step": 9530
    },
    {
      "epoch": 1.907618476304739,
      "grad_norm": 14.448333740234375,
      "learning_rate": 1.8206358728254348e-05,
      "loss": 0.7384,
      "step": 9540
    },
    {
      "epoch": 1.909618076384723,
      "grad_norm": 13.133004188537598,
      "learning_rate": 1.8173032060254615e-05,
      "loss": 0.6067,
      "step": 9550
    },
    {
      "epoch": 1.911617676464707,
      "grad_norm": 5.566751956939697,
      "learning_rate": 1.8139705392254882e-05,
      "loss": 0.6626,
      "step": 9560
    },
    {
      "epoch": 1.913617276544691,
      "grad_norm": 17.446382522583008,
      "learning_rate": 1.810637872425515e-05,
      "loss": 1.0282,
      "step": 9570
    },
    {
      "epoch": 1.915616876624675,
      "grad_norm": 20.57236671447754,
      "learning_rate": 1.8073052056255416e-05,
      "loss": 0.9497,
      "step": 9580
    },
    {
      "epoch": 1.917616476704659,
      "grad_norm": 14.766932487487793,
      "learning_rate": 1.8039725388255682e-05,
      "loss": 0.8677,
      "step": 9590
    },
    {
      "epoch": 1.919616076784643,
      "grad_norm": 5.919003009796143,
      "learning_rate": 1.800639872025595e-05,
      "loss": 1.1654,
      "step": 9600
    },
    {
      "epoch": 1.921615676864627,
      "grad_norm": 12.143542289733887,
      "learning_rate": 1.7973072052256216e-05,
      "loss": 0.9579,
      "step": 9610
    },
    {
      "epoch": 1.923615276944611,
      "grad_norm": 9.730494499206543,
      "learning_rate": 1.7939745384256483e-05,
      "loss": 0.8149,
      "step": 9620
    },
    {
      "epoch": 1.925614877024595,
      "grad_norm": 16.596120834350586,
      "learning_rate": 1.7906418716256747e-05,
      "loss": 0.9119,
      "step": 9630
    },
    {
      "epoch": 1.9276144771045791,
      "grad_norm": 10.097875595092773,
      "learning_rate": 1.7873092048257017e-05,
      "loss": 1.1861,
      "step": 9640
    },
    {
      "epoch": 1.9296140771845631,
      "grad_norm": 8.297917366027832,
      "learning_rate": 1.7839765380257284e-05,
      "loss": 0.9679,
      "step": 9650
    },
    {
      "epoch": 1.9316136772645471,
      "grad_norm": 7.627737522125244,
      "learning_rate": 1.780643871225755e-05,
      "loss": 0.803,
      "step": 9660
    },
    {
      "epoch": 1.9336132773445311,
      "grad_norm": 8.948898315429688,
      "learning_rate": 1.7773112044257818e-05,
      "loss": 0.9855,
      "step": 9670
    },
    {
      "epoch": 1.9356128774245152,
      "grad_norm": 12.643610000610352,
      "learning_rate": 1.7739785376258084e-05,
      "loss": 1.002,
      "step": 9680
    },
    {
      "epoch": 1.9376124775044992,
      "grad_norm": 11.639910697937012,
      "learning_rate": 1.770645870825835e-05,
      "loss": 0.8557,
      "step": 9690
    },
    {
      "epoch": 1.9396120775844832,
      "grad_norm": 14.669049263000488,
      "learning_rate": 1.7673132040258615e-05,
      "loss": 0.7671,
      "step": 9700
    },
    {
      "epoch": 1.9416116776644672,
      "grad_norm": 16.027828216552734,
      "learning_rate": 1.7639805372258882e-05,
      "loss": 0.7013,
      "step": 9710
    },
    {
      "epoch": 1.9436112777444512,
      "grad_norm": 10.484170913696289,
      "learning_rate": 1.760647870425915e-05,
      "loss": 1.0175,
      "step": 9720
    },
    {
      "epoch": 1.9456108778244352,
      "grad_norm": 14.227584838867188,
      "learning_rate": 1.7573152036259416e-05,
      "loss": 0.9531,
      "step": 9730
    },
    {
      "epoch": 1.9476104779044192,
      "grad_norm": 9.757780075073242,
      "learning_rate": 1.7539825368259682e-05,
      "loss": 0.9216,
      "step": 9740
    },
    {
      "epoch": 1.9496100779844032,
      "grad_norm": 21.43222427368164,
      "learning_rate": 1.750649870025995e-05,
      "loss": 0.8357,
      "step": 9750
    },
    {
      "epoch": 1.9516096780643872,
      "grad_norm": 7.027749538421631,
      "learning_rate": 1.7473172032260216e-05,
      "loss": 0.8046,
      "step": 9760
    },
    {
      "epoch": 1.9536092781443712,
      "grad_norm": 18.131330490112305,
      "learning_rate": 1.7439845364260483e-05,
      "loss": 0.9148,
      "step": 9770
    },
    {
      "epoch": 1.9556088782243553,
      "grad_norm": 13.484237670898438,
      "learning_rate": 1.740651869626075e-05,
      "loss": 0.7505,
      "step": 9780
    },
    {
      "epoch": 1.9576084783043393,
      "grad_norm": 11.152361869812012,
      "learning_rate": 1.7373192028261013e-05,
      "loss": 0.9874,
      "step": 9790
    },
    {
      "epoch": 1.9596080783843233,
      "grad_norm": 9.916866302490234,
      "learning_rate": 1.733986536026128e-05,
      "loss": 1.018,
      "step": 9800
    },
    {
      "epoch": 1.9616076784643073,
      "grad_norm": 12.454010009765625,
      "learning_rate": 1.7306538692261547e-05,
      "loss": 0.9393,
      "step": 9810
    },
    {
      "epoch": 1.9636072785442913,
      "grad_norm": 13.449954986572266,
      "learning_rate": 1.7273212024261814e-05,
      "loss": 1.2293,
      "step": 9820
    },
    {
      "epoch": 1.9656068786242753,
      "grad_norm": 12.880757331848145,
      "learning_rate": 1.723988535626208e-05,
      "loss": 0.7754,
      "step": 9830
    },
    {
      "epoch": 1.9676064787042593,
      "grad_norm": 15.530356407165527,
      "learning_rate": 1.7206558688262348e-05,
      "loss": 0.9341,
      "step": 9840
    },
    {
      "epoch": 1.969606078784243,
      "grad_norm": 7.1057634353637695,
      "learning_rate": 1.7173232020262615e-05,
      "loss": 0.9456,
      "step": 9850
    },
    {
      "epoch": 1.971605678864227,
      "grad_norm": 5.957801818847656,
      "learning_rate": 1.713990535226288e-05,
      "loss": 0.9068,
      "step": 9860
    },
    {
      "epoch": 1.9736052789442111,
      "grad_norm": 16.901195526123047,
      "learning_rate": 1.710657868426315e-05,
      "loss": 1.0635,
      "step": 9870
    },
    {
      "epoch": 1.9756048790241951,
      "grad_norm": 14.567813873291016,
      "learning_rate": 1.7073252016263412e-05,
      "loss": 1.1333,
      "step": 9880
    },
    {
      "epoch": 1.9776044791041791,
      "grad_norm": 11.198480606079102,
      "learning_rate": 1.703992534826368e-05,
      "loss": 0.6127,
      "step": 9890
    },
    {
      "epoch": 1.9796040791841631,
      "grad_norm": 11.764053344726562,
      "learning_rate": 1.700659868026395e-05,
      "loss": 0.8163,
      "step": 9900
    },
    {
      "epoch": 1.9816036792641472,
      "grad_norm": 11.911630630493164,
      "learning_rate": 1.6973272012264216e-05,
      "loss": 0.8993,
      "step": 9910
    },
    {
      "epoch": 1.9836032793441312,
      "grad_norm": 13.472591400146484,
      "learning_rate": 1.6939945344264483e-05,
      "loss": 0.8304,
      "step": 9920
    },
    {
      "epoch": 1.9856028794241152,
      "grad_norm": 13.898326873779297,
      "learning_rate": 1.690661867626475e-05,
      "loss": 0.9375,
      "step": 9930
    },
    {
      "epoch": 1.9876024795040992,
      "grad_norm": 12.9910888671875,
      "learning_rate": 1.6873292008265017e-05,
      "loss": 0.9237,
      "step": 9940
    },
    {
      "epoch": 1.9896020795840832,
      "grad_norm": 10.924692153930664,
      "learning_rate": 1.683996534026528e-05,
      "loss": 0.6581,
      "step": 9950
    },
    {
      "epoch": 1.9916016796640672,
      "grad_norm": 17.41364097595215,
      "learning_rate": 1.6806638672265547e-05,
      "loss": 0.7483,
      "step": 9960
    },
    {
      "epoch": 1.9936012797440512,
      "grad_norm": 10.70089340209961,
      "learning_rate": 1.6773312004265814e-05,
      "loss": 0.8363,
      "step": 9970
    },
    {
      "epoch": 1.9956008798240352,
      "grad_norm": 29.129552841186523,
      "learning_rate": 1.673998533626608e-05,
      "loss": 0.7077,
      "step": 9980
    },
    {
      "epoch": 1.9976004799040192,
      "grad_norm": 15.672335624694824,
      "learning_rate": 1.6706658668266348e-05,
      "loss": 0.844,
      "step": 9990
    },
    {
      "epoch": 1.9996000799840032,
      "grad_norm": 14.855646133422852,
      "learning_rate": 1.6673332000266615e-05,
      "loss": 0.9896,
      "step": 10000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6614677064587082,
      "eval_loss": 1.0968632698059082,
      "eval_runtime": 42.8092,
      "eval_samples_per_second": 233.641,
      "eval_steps_per_second": 29.223,
      "step": 10002
    },
    {
      "epoch": 2.001599680063987,
      "grad_norm": 11.318608283996582,
      "learning_rate": 1.664000533226688e-05,
      "loss": 0.584,
      "step": 10010
    },
    {
      "epoch": 2.003599280143971,
      "grad_norm": 15.153538703918457,
      "learning_rate": 1.660667866426715e-05,
      "loss": 0.6717,
      "step": 10020
    },
    {
      "epoch": 2.005598880223955,
      "grad_norm": 14.017580032348633,
      "learning_rate": 1.6573351996267415e-05,
      "loss": 0.6586,
      "step": 10030
    },
    {
      "epoch": 2.007598480303939,
      "grad_norm": 4.691623687744141,
      "learning_rate": 1.654002532826768e-05,
      "loss": 0.6597,
      "step": 10040
    },
    {
      "epoch": 2.009598080383923,
      "grad_norm": 14.402302742004395,
      "learning_rate": 1.6506698660267946e-05,
      "loss": 0.6418,
      "step": 10050
    },
    {
      "epoch": 2.011597680463907,
      "grad_norm": 9.09472942352295,
      "learning_rate": 1.6473371992268213e-05,
      "loss": 0.6265,
      "step": 10060
    },
    {
      "epoch": 2.013597280543891,
      "grad_norm": 9.717056274414062,
      "learning_rate": 1.644004532426848e-05,
      "loss": 0.4099,
      "step": 10070
    },
    {
      "epoch": 2.015596880623875,
      "grad_norm": 9.503205299377441,
      "learning_rate": 1.6406718656268746e-05,
      "loss": 0.5316,
      "step": 10080
    },
    {
      "epoch": 2.017596480703859,
      "grad_norm": 6.525936126708984,
      "learning_rate": 1.6373391988269013e-05,
      "loss": 0.4249,
      "step": 10090
    },
    {
      "epoch": 2.019596080783843,
      "grad_norm": 10.918753623962402,
      "learning_rate": 1.634006532026928e-05,
      "loss": 0.6012,
      "step": 10100
    },
    {
      "epoch": 2.021595680863827,
      "grad_norm": 5.906491756439209,
      "learning_rate": 1.6306738652269547e-05,
      "loss": 0.662,
      "step": 10110
    },
    {
      "epoch": 2.023595280943811,
      "grad_norm": 21.50567626953125,
      "learning_rate": 1.6273411984269814e-05,
      "loss": 0.711,
      "step": 10120
    },
    {
      "epoch": 2.025594881023795,
      "grad_norm": 11.217774391174316,
      "learning_rate": 1.6240085316270078e-05,
      "loss": 0.7481,
      "step": 10130
    },
    {
      "epoch": 2.027594481103779,
      "grad_norm": 20.378305435180664,
      "learning_rate": 1.6206758648270344e-05,
      "loss": 0.6618,
      "step": 10140
    },
    {
      "epoch": 2.029594081183763,
      "grad_norm": 16.587255477905273,
      "learning_rate": 1.6173431980270615e-05,
      "loss": 0.7107,
      "step": 10150
    },
    {
      "epoch": 2.031593681263747,
      "grad_norm": 6.634256839752197,
      "learning_rate": 1.614010531227088e-05,
      "loss": 0.5643,
      "step": 10160
    },
    {
      "epoch": 2.033593281343731,
      "grad_norm": 21.468273162841797,
      "learning_rate": 1.610677864427115e-05,
      "loss": 0.6519,
      "step": 10170
    },
    {
      "epoch": 2.035592881423715,
      "grad_norm": 7.250454425811768,
      "learning_rate": 1.6073451976271415e-05,
      "loss": 0.374,
      "step": 10180
    },
    {
      "epoch": 2.037592481503699,
      "grad_norm": 10.241340637207031,
      "learning_rate": 1.6040125308271682e-05,
      "loss": 0.9727,
      "step": 10190
    },
    {
      "epoch": 2.039592081583683,
      "grad_norm": 5.849274635314941,
      "learning_rate": 1.6006798640271946e-05,
      "loss": 0.547,
      "step": 10200
    },
    {
      "epoch": 2.0415916816636672,
      "grad_norm": 6.000645160675049,
      "learning_rate": 1.5973471972272213e-05,
      "loss": 0.4727,
      "step": 10210
    },
    {
      "epoch": 2.0435912817436512,
      "grad_norm": 11.792917251586914,
      "learning_rate": 1.594014530427248e-05,
      "loss": 0.525,
      "step": 10220
    },
    {
      "epoch": 2.0455908818236352,
      "grad_norm": 8.502846717834473,
      "learning_rate": 1.5906818636272746e-05,
      "loss": 0.7433,
      "step": 10230
    },
    {
      "epoch": 2.0475904819036193,
      "grad_norm": 9.682768821716309,
      "learning_rate": 1.5873491968273013e-05,
      "loss": 0.4867,
      "step": 10240
    },
    {
      "epoch": 2.0495900819836033,
      "grad_norm": 12.956607818603516,
      "learning_rate": 1.584016530027328e-05,
      "loss": 0.6678,
      "step": 10250
    },
    {
      "epoch": 2.0515896820635873,
      "grad_norm": 13.71900463104248,
      "learning_rate": 1.5806838632273547e-05,
      "loss": 0.4353,
      "step": 10260
    },
    {
      "epoch": 2.0535892821435713,
      "grad_norm": 12.062692642211914,
      "learning_rate": 1.5773511964273814e-05,
      "loss": 0.5584,
      "step": 10270
    },
    {
      "epoch": 2.0555888822235553,
      "grad_norm": 13.833047866821289,
      "learning_rate": 1.574018529627408e-05,
      "loss": 0.6975,
      "step": 10280
    },
    {
      "epoch": 2.0575884823035393,
      "grad_norm": 10.411683082580566,
      "learning_rate": 1.5706858628274344e-05,
      "loss": 0.4674,
      "step": 10290
    },
    {
      "epoch": 2.0595880823835233,
      "grad_norm": 8.020232200622559,
      "learning_rate": 1.567353196027461e-05,
      "loss": 0.4836,
      "step": 10300
    },
    {
      "epoch": 2.0615876824635073,
      "grad_norm": 13.79951286315918,
      "learning_rate": 1.5640205292274878e-05,
      "loss": 0.441,
      "step": 10310
    },
    {
      "epoch": 2.0635872825434913,
      "grad_norm": 7.829061985015869,
      "learning_rate": 1.5606878624275145e-05,
      "loss": 0.4855,
      "step": 10320
    },
    {
      "epoch": 2.0655868826234753,
      "grad_norm": 7.692019939422607,
      "learning_rate": 1.5573551956275412e-05,
      "loss": 0.6118,
      "step": 10330
    },
    {
      "epoch": 2.0675864827034593,
      "grad_norm": 6.131185054779053,
      "learning_rate": 1.554022528827568e-05,
      "loss": 0.6166,
      "step": 10340
    },
    {
      "epoch": 2.0695860827834434,
      "grad_norm": 9.176802635192871,
      "learning_rate": 1.5506898620275946e-05,
      "loss": 0.4619,
      "step": 10350
    },
    {
      "epoch": 2.0715856828634274,
      "grad_norm": 10.416106224060059,
      "learning_rate": 1.5473571952276213e-05,
      "loss": 0.4793,
      "step": 10360
    },
    {
      "epoch": 2.0735852829434114,
      "grad_norm": 11.470463752746582,
      "learning_rate": 1.5440245284276476e-05,
      "loss": 0.4884,
      "step": 10370
    },
    {
      "epoch": 2.0755848830233954,
      "grad_norm": 8.544797897338867,
      "learning_rate": 1.5406918616276743e-05,
      "loss": 0.5118,
      "step": 10380
    },
    {
      "epoch": 2.0775844831033794,
      "grad_norm": 14.47285270690918,
      "learning_rate": 1.537359194827701e-05,
      "loss": 0.4688,
      "step": 10390
    },
    {
      "epoch": 2.0795840831833634,
      "grad_norm": 2.2390661239624023,
      "learning_rate": 1.5340265280277277e-05,
      "loss": 0.4223,
      "step": 10400
    },
    {
      "epoch": 2.0815836832633474,
      "grad_norm": 8.765765190124512,
      "learning_rate": 1.5306938612277547e-05,
      "loss": 0.6159,
      "step": 10410
    },
    {
      "epoch": 2.0835832833433314,
      "grad_norm": 8.668785095214844,
      "learning_rate": 1.5273611944277814e-05,
      "loss": 0.4712,
      "step": 10420
    },
    {
      "epoch": 2.0855828834233154,
      "grad_norm": 21.636411666870117,
      "learning_rate": 1.524028527627808e-05,
      "loss": 0.7581,
      "step": 10430
    },
    {
      "epoch": 2.0875824835032994,
      "grad_norm": 16.146957397460938,
      "learning_rate": 1.5206958608278346e-05,
      "loss": 0.6307,
      "step": 10440
    },
    {
      "epoch": 2.0895820835832835,
      "grad_norm": 19.012500762939453,
      "learning_rate": 1.5173631940278613e-05,
      "loss": 0.5136,
      "step": 10450
    },
    {
      "epoch": 2.0915816836632675,
      "grad_norm": 9.179192543029785,
      "learning_rate": 1.514030527227888e-05,
      "loss": 0.5823,
      "step": 10460
    },
    {
      "epoch": 2.0935812837432515,
      "grad_norm": 14.794758796691895,
      "learning_rate": 1.5106978604279145e-05,
      "loss": 0.5989,
      "step": 10470
    },
    {
      "epoch": 2.0955808838232355,
      "grad_norm": 12.006731986999512,
      "learning_rate": 1.5073651936279412e-05,
      "loss": 0.674,
      "step": 10480
    },
    {
      "epoch": 2.0975804839032195,
      "grad_norm": 18.89928436279297,
      "learning_rate": 1.5040325268279679e-05,
      "loss": 0.6507,
      "step": 10490
    },
    {
      "epoch": 2.0995800839832035,
      "grad_norm": 33.66220474243164,
      "learning_rate": 1.5006998600279946e-05,
      "loss": 0.4387,
      "step": 10500
    },
    {
      "epoch": 2.1015796840631875,
      "grad_norm": 19.67152214050293,
      "learning_rate": 1.4973671932280211e-05,
      "loss": 0.5399,
      "step": 10510
    },
    {
      "epoch": 2.1035792841431715,
      "grad_norm": 13.662841796875,
      "learning_rate": 1.4940345264280478e-05,
      "loss": 0.7671,
      "step": 10520
    },
    {
      "epoch": 2.1055788842231555,
      "grad_norm": 14.769211769104004,
      "learning_rate": 1.4907018596280745e-05,
      "loss": 0.8226,
      "step": 10530
    },
    {
      "epoch": 2.1075784843031395,
      "grad_norm": 13.370759010314941,
      "learning_rate": 1.4873691928281012e-05,
      "loss": 0.6805,
      "step": 10540
    },
    {
      "epoch": 2.1095780843831236,
      "grad_norm": 20.494665145874023,
      "learning_rate": 1.4840365260281277e-05,
      "loss": 0.5771,
      "step": 10550
    },
    {
      "epoch": 2.1115776844631076,
      "grad_norm": 11.046703338623047,
      "learning_rate": 1.4807038592281544e-05,
      "loss": 0.5203,
      "step": 10560
    },
    {
      "epoch": 2.1135772845430916,
      "grad_norm": 26.284290313720703,
      "learning_rate": 1.477371192428181e-05,
      "loss": 0.5604,
      "step": 10570
    },
    {
      "epoch": 2.1155768846230756,
      "grad_norm": 16.779159545898438,
      "learning_rate": 1.4740385256282077e-05,
      "loss": 0.6896,
      "step": 10580
    },
    {
      "epoch": 2.1175764847030596,
      "grad_norm": 11.615691184997559,
      "learning_rate": 1.4707058588282344e-05,
      "loss": 0.8899,
      "step": 10590
    },
    {
      "epoch": 2.1195760847830436,
      "grad_norm": 40.14621353149414,
      "learning_rate": 1.467373192028261e-05,
      "loss": 0.5899,
      "step": 10600
    },
    {
      "epoch": 2.1215756848630276,
      "grad_norm": 8.080934524536133,
      "learning_rate": 1.4640405252282876e-05,
      "loss": 0.6572,
      "step": 10610
    },
    {
      "epoch": 2.123575284943011,
      "grad_norm": 10.289127349853516,
      "learning_rate": 1.4607078584283143e-05,
      "loss": 0.7436,
      "step": 10620
    },
    {
      "epoch": 2.1255748850229956,
      "grad_norm": 9.705771446228027,
      "learning_rate": 1.457375191628341e-05,
      "loss": 0.7149,
      "step": 10630
    },
    {
      "epoch": 2.127574485102979,
      "grad_norm": 8.964634895324707,
      "learning_rate": 1.4540425248283675e-05,
      "loss": 0.4227,
      "step": 10640
    },
    {
      "epoch": 2.129574085182963,
      "grad_norm": 13.344239234924316,
      "learning_rate": 1.4507098580283942e-05,
      "loss": 0.4924,
      "step": 10650
    },
    {
      "epoch": 2.131573685262947,
      "grad_norm": 25.67795753479004,
      "learning_rate": 1.447377191228421e-05,
      "loss": 0.4265,
      "step": 10660
    },
    {
      "epoch": 2.1335732853429312,
      "grad_norm": 17.688236236572266,
      "learning_rate": 1.4440445244284478e-05,
      "loss": 0.5203,
      "step": 10670
    },
    {
      "epoch": 2.1355728854229152,
      "grad_norm": 14.073275566101074,
      "learning_rate": 1.4407118576284745e-05,
      "loss": 0.4762,
      "step": 10680
    },
    {
      "epoch": 2.1375724855028992,
      "grad_norm": 43.72536849975586,
      "learning_rate": 1.4373791908285012e-05,
      "loss": 0.5233,
      "step": 10690
    },
    {
      "epoch": 2.1395720855828833,
      "grad_norm": 12.544296264648438,
      "learning_rate": 1.4340465240285278e-05,
      "loss": 0.6085,
      "step": 10700
    },
    {
      "epoch": 2.1415716856628673,
      "grad_norm": 6.4195661544799805,
      "learning_rate": 1.4307138572285544e-05,
      "loss": 0.483,
      "step": 10710
    },
    {
      "epoch": 2.1435712857428513,
      "grad_norm": 20.395469665527344,
      "learning_rate": 1.427381190428581e-05,
      "loss": 0.509,
      "step": 10720
    },
    {
      "epoch": 2.1455708858228353,
      "grad_norm": 2.944995164871216,
      "learning_rate": 1.4240485236286077e-05,
      "loss": 0.4907,
      "step": 10730
    },
    {
      "epoch": 2.1475704859028193,
      "grad_norm": 11.063433647155762,
      "learning_rate": 1.4207158568286344e-05,
      "loss": 0.7426,
      "step": 10740
    },
    {
      "epoch": 2.1495700859828033,
      "grad_norm": 6.536821365356445,
      "learning_rate": 1.4173831900286611e-05,
      "loss": 0.603,
      "step": 10750
    },
    {
      "epoch": 2.1515696860627873,
      "grad_norm": 16.191728591918945,
      "learning_rate": 1.4140505232286876e-05,
      "loss": 0.5421,
      "step": 10760
    },
    {
      "epoch": 2.1535692861427713,
      "grad_norm": 15.29310131072998,
      "learning_rate": 1.4107178564287143e-05,
      "loss": 0.8745,
      "step": 10770
    },
    {
      "epoch": 2.1555688862227553,
      "grad_norm": 13.173842430114746,
      "learning_rate": 1.407385189628741e-05,
      "loss": 0.7856,
      "step": 10780
    },
    {
      "epoch": 2.1575684863027393,
      "grad_norm": 6.9116034507751465,
      "learning_rate": 1.4040525228287677e-05,
      "loss": 0.6461,
      "step": 10790
    },
    {
      "epoch": 2.1595680863827234,
      "grad_norm": 2.5336718559265137,
      "learning_rate": 1.4007198560287942e-05,
      "loss": 0.443,
      "step": 10800
    },
    {
      "epoch": 2.1615676864627074,
      "grad_norm": 17.665531158447266,
      "learning_rate": 1.397387189228821e-05,
      "loss": 0.7451,
      "step": 10810
    },
    {
      "epoch": 2.1635672865426914,
      "grad_norm": 12.102017402648926,
      "learning_rate": 1.3940545224288476e-05,
      "loss": 0.6746,
      "step": 10820
    },
    {
      "epoch": 2.1655668866226754,
      "grad_norm": 14.095515251159668,
      "learning_rate": 1.3907218556288743e-05,
      "loss": 0.6221,
      "step": 10830
    },
    {
      "epoch": 2.1675664867026594,
      "grad_norm": 8.750543594360352,
      "learning_rate": 1.387389188828901e-05,
      "loss": 0.5416,
      "step": 10840
    },
    {
      "epoch": 2.1695660867826434,
      "grad_norm": 18.82843017578125,
      "learning_rate": 1.3840565220289275e-05,
      "loss": 0.3845,
      "step": 10850
    },
    {
      "epoch": 2.1715656868626274,
      "grad_norm": 23.59303855895996,
      "learning_rate": 1.3807238552289542e-05,
      "loss": 0.6519,
      "step": 10860
    },
    {
      "epoch": 2.1735652869426114,
      "grad_norm": 14.357157707214355,
      "learning_rate": 1.3773911884289809e-05,
      "loss": 0.5936,
      "step": 10870
    },
    {
      "epoch": 2.1755648870225954,
      "grad_norm": 13.472745895385742,
      "learning_rate": 1.3740585216290076e-05,
      "loss": 0.651,
      "step": 10880
    },
    {
      "epoch": 2.1775644871025794,
      "grad_norm": 18.079620361328125,
      "learning_rate": 1.3707258548290341e-05,
      "loss": 0.5297,
      "step": 10890
    },
    {
      "epoch": 2.1795640871825634,
      "grad_norm": 12.995994567871094,
      "learning_rate": 1.3673931880290608e-05,
      "loss": 0.4323,
      "step": 10900
    },
    {
      "epoch": 2.1815636872625475,
      "grad_norm": 8.143168449401855,
      "learning_rate": 1.3640605212290875e-05,
      "loss": 0.3543,
      "step": 10910
    },
    {
      "epoch": 2.1835632873425315,
      "grad_norm": 1.2924525737762451,
      "learning_rate": 1.3607278544291143e-05,
      "loss": 0.5912,
      "step": 10920
    },
    {
      "epoch": 2.1855628874225155,
      "grad_norm": 24.25101661682129,
      "learning_rate": 1.357395187629141e-05,
      "loss": 0.7256,
      "step": 10930
    },
    {
      "epoch": 2.1875624875024995,
      "grad_norm": 9.644801139831543,
      "learning_rate": 1.3540625208291677e-05,
      "loss": 0.5248,
      "step": 10940
    },
    {
      "epoch": 2.1895620875824835,
      "grad_norm": 1.4460257291793823,
      "learning_rate": 1.3507298540291944e-05,
      "loss": 0.5061,
      "step": 10950
    },
    {
      "epoch": 2.1915616876624675,
      "grad_norm": 20.085058212280273,
      "learning_rate": 1.3473971872292209e-05,
      "loss": 0.4676,
      "step": 10960
    },
    {
      "epoch": 2.1935612877424515,
      "grad_norm": 17.60126495361328,
      "learning_rate": 1.3440645204292476e-05,
      "loss": 0.5953,
      "step": 10970
    },
    {
      "epoch": 2.1955608878224355,
      "grad_norm": 11.715899467468262,
      "learning_rate": 1.3407318536292743e-05,
      "loss": 0.545,
      "step": 10980
    },
    {
      "epoch": 2.1975604879024195,
      "grad_norm": 17.914400100708008,
      "learning_rate": 1.337399186829301e-05,
      "loss": 0.4164,
      "step": 10990
    },
    {
      "epoch": 2.1995600879824035,
      "grad_norm": 14.558450698852539,
      "learning_rate": 1.3340665200293275e-05,
      "loss": 0.5677,
      "step": 11000
    },
    {
      "epoch": 2.2015596880623876,
      "grad_norm": 14.023703575134277,
      "learning_rate": 1.3307338532293542e-05,
      "loss": 0.5546,
      "step": 11010
    },
    {
      "epoch": 2.2035592881423716,
      "grad_norm": 9.474443435668945,
      "learning_rate": 1.3274011864293809e-05,
      "loss": 0.3476,
      "step": 11020
    },
    {
      "epoch": 2.2055588882223556,
      "grad_norm": 15.00603199005127,
      "learning_rate": 1.3240685196294076e-05,
      "loss": 0.4777,
      "step": 11030
    },
    {
      "epoch": 2.2075584883023396,
      "grad_norm": 8.407323837280273,
      "learning_rate": 1.3207358528294343e-05,
      "loss": 0.4047,
      "step": 11040
    },
    {
      "epoch": 2.2095580883823236,
      "grad_norm": 21.95122718811035,
      "learning_rate": 1.3174031860294608e-05,
      "loss": 0.752,
      "step": 11050
    },
    {
      "epoch": 2.2115576884623076,
      "grad_norm": 17.050416946411133,
      "learning_rate": 1.3140705192294875e-05,
      "loss": 0.5885,
      "step": 11060
    },
    {
      "epoch": 2.2135572885422916,
      "grad_norm": 16.602582931518555,
      "learning_rate": 1.3107378524295142e-05,
      "loss": 0.3464,
      "step": 11070
    },
    {
      "epoch": 2.2155568886222756,
      "grad_norm": 16.7025089263916,
      "learning_rate": 1.3074051856295408e-05,
      "loss": 0.6606,
      "step": 11080
    },
    {
      "epoch": 2.2175564887022596,
      "grad_norm": 7.772080898284912,
      "learning_rate": 1.3040725188295674e-05,
      "loss": 0.4835,
      "step": 11090
    },
    {
      "epoch": 2.2195560887822436,
      "grad_norm": 26.87282371520996,
      "learning_rate": 1.300739852029594e-05,
      "loss": 0.6742,
      "step": 11100
    },
    {
      "epoch": 2.2215556888622277,
      "grad_norm": 6.625521659851074,
      "learning_rate": 1.2974071852296207e-05,
      "loss": 0.6946,
      "step": 11110
    },
    {
      "epoch": 2.2235552889422117,
      "grad_norm": 11.648713111877441,
      "learning_rate": 1.2940745184296474e-05,
      "loss": 0.5172,
      "step": 11120
    },
    {
      "epoch": 2.2255548890221957,
      "grad_norm": 18.87082290649414,
      "learning_rate": 1.2907418516296741e-05,
      "loss": 0.4913,
      "step": 11130
    },
    {
      "epoch": 2.2275544891021797,
      "grad_norm": 16.07965660095215,
      "learning_rate": 1.2874091848297006e-05,
      "loss": 0.6897,
      "step": 11140
    },
    {
      "epoch": 2.2295540891821637,
      "grad_norm": 3.7056431770324707,
      "learning_rate": 1.2840765180297273e-05,
      "loss": 0.6786,
      "step": 11150
    },
    {
      "epoch": 2.2315536892621477,
      "grad_norm": 14.841525077819824,
      "learning_rate": 1.280743851229754e-05,
      "loss": 0.3882,
      "step": 11160
    },
    {
      "epoch": 2.2335532893421317,
      "grad_norm": 10.920039176940918,
      "learning_rate": 1.2774111844297807e-05,
      "loss": 0.5684,
      "step": 11170
    },
    {
      "epoch": 2.2355528894221157,
      "grad_norm": 12.283342361450195,
      "learning_rate": 1.2740785176298076e-05,
      "loss": 0.7533,
      "step": 11180
    },
    {
      "epoch": 2.2375524895020997,
      "grad_norm": 9.604187965393066,
      "learning_rate": 1.2707458508298343e-05,
      "loss": 0.5366,
      "step": 11190
    },
    {
      "epoch": 2.2395520895820837,
      "grad_norm": 11.542847633361816,
      "learning_rate": 1.267413184029861e-05,
      "loss": 0.6512,
      "step": 11200
    },
    {
      "epoch": 2.2415516896620677,
      "grad_norm": 14.229412078857422,
      "learning_rate": 1.2640805172298875e-05,
      "loss": 0.6343,
      "step": 11210
    },
    {
      "epoch": 2.2435512897420518,
      "grad_norm": 16.062116622924805,
      "learning_rate": 1.2607478504299141e-05,
      "loss": 0.5636,
      "step": 11220
    },
    {
      "epoch": 2.2455508898220358,
      "grad_norm": 5.291207313537598,
      "learning_rate": 1.2574151836299408e-05,
      "loss": 0.435,
      "step": 11230
    },
    {
      "epoch": 2.2475504899020198,
      "grad_norm": 11.268157005310059,
      "learning_rate": 1.2540825168299675e-05,
      "loss": 0.6207,
      "step": 11240
    },
    {
      "epoch": 2.249550089982004,
      "grad_norm": 17.0425968170166,
      "learning_rate": 1.250749850029994e-05,
      "loss": 0.8009,
      "step": 11250
    },
    {
      "epoch": 2.251549690061988,
      "grad_norm": 15.645447731018066,
      "learning_rate": 1.2474171832300207e-05,
      "loss": 0.7877,
      "step": 11260
    },
    {
      "epoch": 2.2535492901419714,
      "grad_norm": 25.07251739501953,
      "learning_rate": 1.2440845164300474e-05,
      "loss": 0.6626,
      "step": 11270
    },
    {
      "epoch": 2.255548890221956,
      "grad_norm": 11.72342586517334,
      "learning_rate": 1.2407518496300741e-05,
      "loss": 0.6256,
      "step": 11280
    },
    {
      "epoch": 2.2575484903019394,
      "grad_norm": 16.757556915283203,
      "learning_rate": 1.2374191828301008e-05,
      "loss": 0.5565,
      "step": 11290
    },
    {
      "epoch": 2.259548090381924,
      "grad_norm": 9.767590522766113,
      "learning_rate": 1.2340865160301273e-05,
      "loss": 0.398,
      "step": 11300
    },
    {
      "epoch": 2.2615476904619074,
      "grad_norm": 21.518444061279297,
      "learning_rate": 1.230753849230154e-05,
      "loss": 0.6029,
      "step": 11310
    },
    {
      "epoch": 2.263547290541892,
      "grad_norm": 8.869854927062988,
      "learning_rate": 1.2274211824301807e-05,
      "loss": 0.4389,
      "step": 11320
    },
    {
      "epoch": 2.2655468906218754,
      "grad_norm": 10.270552635192871,
      "learning_rate": 1.2240885156302074e-05,
      "loss": 0.6443,
      "step": 11330
    },
    {
      "epoch": 2.26754649070186,
      "grad_norm": 11.926810264587402,
      "learning_rate": 1.2207558488302339e-05,
      "loss": 0.6035,
      "step": 11340
    },
    {
      "epoch": 2.2695460907818434,
      "grad_norm": 4.358675956726074,
      "learning_rate": 1.2174231820302606e-05,
      "loss": 0.7307,
      "step": 11350
    },
    {
      "epoch": 2.2715456908618274,
      "grad_norm": 15.835958480834961,
      "learning_rate": 1.2140905152302873e-05,
      "loss": 0.7209,
      "step": 11360
    },
    {
      "epoch": 2.2735452909418115,
      "grad_norm": 10.729216575622559,
      "learning_rate": 1.2107578484303141e-05,
      "loss": 0.5699,
      "step": 11370
    },
    {
      "epoch": 2.2755448910217955,
      "grad_norm": 20.29210090637207,
      "learning_rate": 1.2074251816303407e-05,
      "loss": 0.6044,
      "step": 11380
    },
    {
      "epoch": 2.2775444911017795,
      "grad_norm": 19.996835708618164,
      "learning_rate": 1.2040925148303674e-05,
      "loss": 0.5533,
      "step": 11390
    },
    {
      "epoch": 2.2795440911817635,
      "grad_norm": 10.225603103637695,
      "learning_rate": 1.200759848030394e-05,
      "loss": 0.6198,
      "step": 11400
    },
    {
      "epoch": 2.2815436912617475,
      "grad_norm": 19.842330932617188,
      "learning_rate": 1.1974271812304207e-05,
      "loss": 0.771,
      "step": 11410
    },
    {
      "epoch": 2.2835432913417315,
      "grad_norm": 22.350147247314453,
      "learning_rate": 1.1940945144304473e-05,
      "loss": 0.3774,
      "step": 11420
    },
    {
      "epoch": 2.2855428914217155,
      "grad_norm": 11.58170223236084,
      "learning_rate": 1.190761847630474e-05,
      "loss": 0.5336,
      "step": 11430
    },
    {
      "epoch": 2.2875424915016995,
      "grad_norm": 23.77650260925293,
      "learning_rate": 1.1874291808305006e-05,
      "loss": 0.5826,
      "step": 11440
    },
    {
      "epoch": 2.2895420915816835,
      "grad_norm": 19.99730110168457,
      "learning_rate": 1.1840965140305273e-05,
      "loss": 0.7202,
      "step": 11450
    },
    {
      "epoch": 2.2915416916616675,
      "grad_norm": 14.766118049621582,
      "learning_rate": 1.1807638472305538e-05,
      "loss": 0.6642,
      "step": 11460
    },
    {
      "epoch": 2.2935412917416516,
      "grad_norm": 7.530274868011475,
      "learning_rate": 1.1774311804305805e-05,
      "loss": 0.7542,
      "step": 11470
    },
    {
      "epoch": 2.2955408918216356,
      "grad_norm": 9.783807754516602,
      "learning_rate": 1.1740985136306072e-05,
      "loss": 0.5662,
      "step": 11480
    },
    {
      "epoch": 2.2975404919016196,
      "grad_norm": 17.59334945678711,
      "learning_rate": 1.1707658468306339e-05,
      "loss": 0.7065,
      "step": 11490
    },
    {
      "epoch": 2.2995400919816036,
      "grad_norm": 17.628835678100586,
      "learning_rate": 1.1674331800306606e-05,
      "loss": 0.6066,
      "step": 11500
    },
    {
      "epoch": 2.3015396920615876,
      "grad_norm": 16.270254135131836,
      "learning_rate": 1.1641005132306873e-05,
      "loss": 0.3371,
      "step": 11510
    },
    {
      "epoch": 2.3035392921415716,
      "grad_norm": 4.281142234802246,
      "learning_rate": 1.160767846430714e-05,
      "loss": 0.5849,
      "step": 11520
    },
    {
      "epoch": 2.3055388922215556,
      "grad_norm": 9.598099708557129,
      "learning_rate": 1.1574351796307407e-05,
      "loss": 0.8021,
      "step": 11530
    },
    {
      "epoch": 2.3075384923015396,
      "grad_norm": 9.947530746459961,
      "learning_rate": 1.1541025128307672e-05,
      "loss": 0.4957,
      "step": 11540
    },
    {
      "epoch": 2.3095380923815236,
      "grad_norm": 16.309497833251953,
      "learning_rate": 1.1507698460307939e-05,
      "loss": 0.421,
      "step": 11550
    },
    {
      "epoch": 2.3115376924615076,
      "grad_norm": 13.818100929260254,
      "learning_rate": 1.1474371792308206e-05,
      "loss": 0.6523,
      "step": 11560
    },
    {
      "epoch": 2.3135372925414917,
      "grad_norm": 19.245159149169922,
      "learning_rate": 1.1441045124308472e-05,
      "loss": 0.684,
      "step": 11570
    },
    {
      "epoch": 2.3155368926214757,
      "grad_norm": 12.386053085327148,
      "learning_rate": 1.140771845630874e-05,
      "loss": 0.5219,
      "step": 11580
    },
    {
      "epoch": 2.3175364927014597,
      "grad_norm": 13.053339004516602,
      "learning_rate": 1.1374391788309005e-05,
      "loss": 0.8128,
      "step": 11590
    },
    {
      "epoch": 2.3195360927814437,
      "grad_norm": 12.088335990905762,
      "learning_rate": 1.1341065120309271e-05,
      "loss": 0.6133,
      "step": 11600
    },
    {
      "epoch": 2.3215356928614277,
      "grad_norm": 13.49221420288086,
      "learning_rate": 1.1307738452309538e-05,
      "loss": 0.4219,
      "step": 11610
    },
    {
      "epoch": 2.3235352929414117,
      "grad_norm": 9.659981727600098,
      "learning_rate": 1.1274411784309805e-05,
      "loss": 0.5709,
      "step": 11620
    },
    {
      "epoch": 2.3255348930213957,
      "grad_norm": 26.332202911376953,
      "learning_rate": 1.1241085116310072e-05,
      "loss": 0.6958,
      "step": 11630
    },
    {
      "epoch": 2.3275344931013797,
      "grad_norm": 13.590984344482422,
      "learning_rate": 1.1207758448310339e-05,
      "loss": 0.6917,
      "step": 11640
    },
    {
      "epoch": 2.3295340931813637,
      "grad_norm": 10.476688385009766,
      "learning_rate": 1.1174431780310606e-05,
      "loss": 0.7418,
      "step": 11650
    },
    {
      "epoch": 2.3315336932613477,
      "grad_norm": 19.020002365112305,
      "learning_rate": 1.1141105112310873e-05,
      "loss": 0.5086,
      "step": 11660
    },
    {
      "epoch": 2.3335332933413317,
      "grad_norm": 14.222344398498535,
      "learning_rate": 1.1107778444311138e-05,
      "loss": 0.763,
      "step": 11670
    },
    {
      "epoch": 2.3355328934213158,
      "grad_norm": 16.690549850463867,
      "learning_rate": 1.1074451776311405e-05,
      "loss": 0.6753,
      "step": 11680
    },
    {
      "epoch": 2.3375324935012998,
      "grad_norm": 1.9978805780410767,
      "learning_rate": 1.1041125108311672e-05,
      "loss": 0.3713,
      "step": 11690
    },
    {
      "epoch": 2.339532093581284,
      "grad_norm": 14.09689712524414,
      "learning_rate": 1.1007798440311939e-05,
      "loss": 0.5547,
      "step": 11700
    },
    {
      "epoch": 2.341531693661268,
      "grad_norm": 26.346694946289062,
      "learning_rate": 1.0974471772312204e-05,
      "loss": 0.9158,
      "step": 11710
    },
    {
      "epoch": 2.343531293741252,
      "grad_norm": 7.0537309646606445,
      "learning_rate": 1.094114510431247e-05,
      "loss": 0.6363,
      "step": 11720
    },
    {
      "epoch": 2.345530893821236,
      "grad_norm": 8.25812816619873,
      "learning_rate": 1.0907818436312738e-05,
      "loss": 0.417,
      "step": 11730
    },
    {
      "epoch": 2.34753049390122,
      "grad_norm": 24.66217613220215,
      "learning_rate": 1.0874491768313005e-05,
      "loss": 0.7675,
      "step": 11740
    },
    {
      "epoch": 2.349530093981204,
      "grad_norm": 10.74952220916748,
      "learning_rate": 1.084116510031327e-05,
      "loss": 0.3974,
      "step": 11750
    },
    {
      "epoch": 2.351529694061188,
      "grad_norm": 4.5490803718566895,
      "learning_rate": 1.0807838432313538e-05,
      "loss": 0.6782,
      "step": 11760
    },
    {
      "epoch": 2.353529294141172,
      "grad_norm": 14.05595874786377,
      "learning_rate": 1.0774511764313805e-05,
      "loss": 0.6764,
      "step": 11770
    },
    {
      "epoch": 2.355528894221156,
      "grad_norm": 9.925400733947754,
      "learning_rate": 1.0741185096314072e-05,
      "loss": 0.709,
      "step": 11780
    },
    {
      "epoch": 2.35752849430114,
      "grad_norm": 15.740494728088379,
      "learning_rate": 1.0707858428314337e-05,
      "loss": 0.5414,
      "step": 11790
    },
    {
      "epoch": 2.359528094381124,
      "grad_norm": 9.643181800842285,
      "learning_rate": 1.0674531760314604e-05,
      "loss": 0.6886,
      "step": 11800
    },
    {
      "epoch": 2.361527694461108,
      "grad_norm": 13.431319236755371,
      "learning_rate": 1.0641205092314871e-05,
      "loss": 0.789,
      "step": 11810
    },
    {
      "epoch": 2.363527294541092,
      "grad_norm": 11.768959045410156,
      "learning_rate": 1.0607878424315138e-05,
      "loss": 0.4384,
      "step": 11820
    },
    {
      "epoch": 2.365526894621076,
      "grad_norm": 6.0796122550964355,
      "learning_rate": 1.0574551756315403e-05,
      "loss": 0.529,
      "step": 11830
    },
    {
      "epoch": 2.36752649470106,
      "grad_norm": 19.6099910736084,
      "learning_rate": 1.054122508831567e-05,
      "loss": 0.4962,
      "step": 11840
    },
    {
      "epoch": 2.369526094781044,
      "grad_norm": 7.308025360107422,
      "learning_rate": 1.0507898420315937e-05,
      "loss": 0.4933,
      "step": 11850
    },
    {
      "epoch": 2.371525694861028,
      "grad_norm": 12.721258163452148,
      "learning_rate": 1.0474571752316204e-05,
      "loss": 0.6855,
      "step": 11860
    },
    {
      "epoch": 2.373525294941012,
      "grad_norm": 19.223920822143555,
      "learning_rate": 1.044124508431647e-05,
      "loss": 0.5349,
      "step": 11870
    },
    {
      "epoch": 2.375524895020996,
      "grad_norm": 19.38271713256836,
      "learning_rate": 1.0407918416316736e-05,
      "loss": 0.73,
      "step": 11880
    },
    {
      "epoch": 2.37752449510098,
      "grad_norm": 3.0825042724609375,
      "learning_rate": 1.0374591748317004e-05,
      "loss": 0.5162,
      "step": 11890
    },
    {
      "epoch": 2.379524095180964,
      "grad_norm": 14.279580116271973,
      "learning_rate": 1.0341265080317271e-05,
      "loss": 0.6362,
      "step": 11900
    },
    {
      "epoch": 2.381523695260948,
      "grad_norm": 21.02758026123047,
      "learning_rate": 1.0307938412317537e-05,
      "loss": 0.625,
      "step": 11910
    },
    {
      "epoch": 2.383523295340932,
      "grad_norm": 13.18588638305664,
      "learning_rate": 1.0274611744317803e-05,
      "loss": 0.743,
      "step": 11920
    },
    {
      "epoch": 2.385522895420916,
      "grad_norm": 12.393308639526367,
      "learning_rate": 1.024128507631807e-05,
      "loss": 0.7205,
      "step": 11930
    },
    {
      "epoch": 2.3875224955009,
      "grad_norm": 9.1727933883667,
      "learning_rate": 1.0207958408318337e-05,
      "loss": 0.4464,
      "step": 11940
    },
    {
      "epoch": 2.389522095580884,
      "grad_norm": 15.382210731506348,
      "learning_rate": 1.0174631740318604e-05,
      "loss": 0.7505,
      "step": 11950
    },
    {
      "epoch": 2.3915216956608676,
      "grad_norm": 16.295930862426758,
      "learning_rate": 1.014130507231887e-05,
      "loss": 0.5766,
      "step": 11960
    },
    {
      "epoch": 2.393521295740852,
      "grad_norm": 11.994336128234863,
      "learning_rate": 1.0107978404319136e-05,
      "loss": 0.5636,
      "step": 11970
    },
    {
      "epoch": 2.3955208958208356,
      "grad_norm": 12.524641036987305,
      "learning_rate": 1.0074651736319403e-05,
      "loss": 0.5083,
      "step": 11980
    },
    {
      "epoch": 2.39752049590082,
      "grad_norm": 9.42490291595459,
      "learning_rate": 1.004132506831967e-05,
      "loss": 0.4216,
      "step": 11990
    },
    {
      "epoch": 2.3995200959808036,
      "grad_norm": 13.851692199707031,
      "learning_rate": 1.0007998400319935e-05,
      "loss": 0.5269,
      "step": 12000
    },
    {
      "epoch": 2.401519696060788,
      "grad_norm": 26.13701057434082,
      "learning_rate": 9.974671732320202e-06,
      "loss": 0.5534,
      "step": 12010
    },
    {
      "epoch": 2.4035192961407716,
      "grad_norm": 16.549779891967773,
      "learning_rate": 9.94134506432047e-06,
      "loss": 0.5926,
      "step": 12020
    },
    {
      "epoch": 2.405518896220756,
      "grad_norm": 26.0358943939209,
      "learning_rate": 9.908018396320738e-06,
      "loss": 0.5438,
      "step": 12030
    },
    {
      "epoch": 2.4075184963007397,
      "grad_norm": 10.562979698181152,
      "learning_rate": 9.874691728321003e-06,
      "loss": 0.5719,
      "step": 12040
    },
    {
      "epoch": 2.4095180963807237,
      "grad_norm": 10.537091255187988,
      "learning_rate": 9.84136506032127e-06,
      "loss": 0.6697,
      "step": 12050
    },
    {
      "epoch": 2.4115176964607077,
      "grad_norm": 18.198246002197266,
      "learning_rate": 9.808038392321537e-06,
      "loss": 0.6724,
      "step": 12060
    },
    {
      "epoch": 2.4135172965406917,
      "grad_norm": 13.42924690246582,
      "learning_rate": 9.774711724321803e-06,
      "loss": 0.5263,
      "step": 12070
    },
    {
      "epoch": 2.4155168966206757,
      "grad_norm": 4.630577087402344,
      "learning_rate": 9.741385056322069e-06,
      "loss": 0.3521,
      "step": 12080
    },
    {
      "epoch": 2.4175164967006597,
      "grad_norm": 16.36670684814453,
      "learning_rate": 9.708058388322336e-06,
      "loss": 0.7447,
      "step": 12090
    },
    {
      "epoch": 2.4195160967806437,
      "grad_norm": 9.173528671264648,
      "learning_rate": 9.674731720322602e-06,
      "loss": 0.5313,
      "step": 12100
    },
    {
      "epoch": 2.4215156968606277,
      "grad_norm": 12.70084285736084,
      "learning_rate": 9.64140505232287e-06,
      "loss": 0.5891,
      "step": 12110
    },
    {
      "epoch": 2.4235152969406117,
      "grad_norm": 16.868223190307617,
      "learning_rate": 9.608078384323135e-06,
      "loss": 0.6569,
      "step": 12120
    },
    {
      "epoch": 2.4255148970205958,
      "grad_norm": 22.838045120239258,
      "learning_rate": 9.574751716323401e-06,
      "loss": 0.9753,
      "step": 12130
    },
    {
      "epoch": 2.4275144971005798,
      "grad_norm": 13.253143310546875,
      "learning_rate": 9.54142504832367e-06,
      "loss": 0.5533,
      "step": 12140
    },
    {
      "epoch": 2.4295140971805638,
      "grad_norm": 9.40952205657959,
      "learning_rate": 9.508098380323937e-06,
      "loss": 0.6487,
      "step": 12150
    },
    {
      "epoch": 2.431513697260548,
      "grad_norm": 9.089274406433105,
      "learning_rate": 9.474771712324202e-06,
      "loss": 0.5739,
      "step": 12160
    },
    {
      "epoch": 2.433513297340532,
      "grad_norm": 8.880586624145508,
      "learning_rate": 9.441445044324469e-06,
      "loss": 0.5695,
      "step": 12170
    },
    {
      "epoch": 2.435512897420516,
      "grad_norm": 7.152127265930176,
      "learning_rate": 9.408118376324736e-06,
      "loss": 0.5008,
      "step": 12180
    },
    {
      "epoch": 2.4375124975005,
      "grad_norm": 13.143566131591797,
      "learning_rate": 9.374791708325003e-06,
      "loss": 0.5481,
      "step": 12190
    },
    {
      "epoch": 2.439512097580484,
      "grad_norm": 8.949190139770508,
      "learning_rate": 9.341465040325268e-06,
      "loss": 0.5397,
      "step": 12200
    },
    {
      "epoch": 2.441511697660468,
      "grad_norm": 16.111167907714844,
      "learning_rate": 9.308138372325535e-06,
      "loss": 0.4237,
      "step": 12210
    },
    {
      "epoch": 2.443511297740452,
      "grad_norm": 10.680604934692383,
      "learning_rate": 9.274811704325802e-06,
      "loss": 0.5702,
      "step": 12220
    },
    {
      "epoch": 2.445510897820436,
      "grad_norm": 10.475625991821289,
      "learning_rate": 9.241485036326069e-06,
      "loss": 0.897,
      "step": 12230
    },
    {
      "epoch": 2.44751049790042,
      "grad_norm": 16.944637298583984,
      "learning_rate": 9.208158368326335e-06,
      "loss": 0.6947,
      "step": 12240
    },
    {
      "epoch": 2.449510097980404,
      "grad_norm": 12.11412239074707,
      "learning_rate": 9.1748317003266e-06,
      "loss": 0.5153,
      "step": 12250
    },
    {
      "epoch": 2.451509698060388,
      "grad_norm": 17.67329216003418,
      "learning_rate": 9.141505032326868e-06,
      "loss": 0.5022,
      "step": 12260
    },
    {
      "epoch": 2.453509298140372,
      "grad_norm": 12.563699722290039,
      "learning_rate": 9.108178364327136e-06,
      "loss": 0.5405,
      "step": 12270
    },
    {
      "epoch": 2.455508898220356,
      "grad_norm": 5.974327087402344,
      "learning_rate": 9.074851696327401e-06,
      "loss": 0.5853,
      "step": 12280
    },
    {
      "epoch": 2.45750849830034,
      "grad_norm": 13.396454811096191,
      "learning_rate": 9.041525028327668e-06,
      "loss": 0.4053,
      "step": 12290
    },
    {
      "epoch": 2.459508098380324,
      "grad_norm": 20.838762283325195,
      "learning_rate": 9.008198360327935e-06,
      "loss": 0.6379,
      "step": 12300
    },
    {
      "epoch": 2.461507698460308,
      "grad_norm": 15.742447853088379,
      "learning_rate": 8.974871692328202e-06,
      "loss": 0.5982,
      "step": 12310
    },
    {
      "epoch": 2.463507298540292,
      "grad_norm": 7.493224143981934,
      "learning_rate": 8.941545024328469e-06,
      "loss": 0.4713,
      "step": 12320
    },
    {
      "epoch": 2.465506898620276,
      "grad_norm": 13.934260368347168,
      "learning_rate": 8.908218356328734e-06,
      "loss": 0.6053,
      "step": 12330
    },
    {
      "epoch": 2.46750649870026,
      "grad_norm": 13.47364330291748,
      "learning_rate": 8.874891688329001e-06,
      "loss": 0.5782,
      "step": 12340
    },
    {
      "epoch": 2.469506098780244,
      "grad_norm": 4.644449710845947,
      "learning_rate": 8.841565020329268e-06,
      "loss": 0.4915,
      "step": 12350
    },
    {
      "epoch": 2.471505698860228,
      "grad_norm": 17.42030906677246,
      "learning_rate": 8.808238352329535e-06,
      "loss": 0.4482,
      "step": 12360
    },
    {
      "epoch": 2.473505298940212,
      "grad_norm": 13.277440071105957,
      "learning_rate": 8.7749116843298e-06,
      "loss": 0.4698,
      "step": 12370
    },
    {
      "epoch": 2.475504899020196,
      "grad_norm": 30.356813430786133,
      "learning_rate": 8.741585016330067e-06,
      "loss": 0.8057,
      "step": 12380
    },
    {
      "epoch": 2.47750449910018,
      "grad_norm": 36.46942138671875,
      "learning_rate": 8.708258348330334e-06,
      "loss": 0.5038,
      "step": 12390
    },
    {
      "epoch": 2.479504099180164,
      "grad_norm": 13.212912559509277,
      "learning_rate": 8.674931680330602e-06,
      "loss": 0.6971,
      "step": 12400
    },
    {
      "epoch": 2.481503699260148,
      "grad_norm": 21.727676391601562,
      "learning_rate": 8.641605012330868e-06,
      "loss": 0.6492,
      "step": 12410
    },
    {
      "epoch": 2.483503299340132,
      "grad_norm": 11.941498756408691,
      "learning_rate": 8.608278344331134e-06,
      "loss": 0.5729,
      "step": 12420
    },
    {
      "epoch": 2.485502899420116,
      "grad_norm": 14.389888763427734,
      "learning_rate": 8.574951676331401e-06,
      "loss": 0.6843,
      "step": 12430
    },
    {
      "epoch": 2.4875024995001,
      "grad_norm": 9.098570823669434,
      "learning_rate": 8.541625008331668e-06,
      "loss": 0.6255,
      "step": 12440
    },
    {
      "epoch": 2.489502099580084,
      "grad_norm": 12.779458045959473,
      "learning_rate": 8.508298340331933e-06,
      "loss": 0.8381,
      "step": 12450
    },
    {
      "epoch": 2.491501699660068,
      "grad_norm": 16.81780242919922,
      "learning_rate": 8.4749716723322e-06,
      "loss": 0.6193,
      "step": 12460
    },
    {
      "epoch": 2.493501299740052,
      "grad_norm": 9.114025115966797,
      "learning_rate": 8.441645004332467e-06,
      "loss": 0.4256,
      "step": 12470
    },
    {
      "epoch": 2.495500899820036,
      "grad_norm": 8.595528602600098,
      "learning_rate": 8.408318336332734e-06,
      "loss": 0.4811,
      "step": 12480
    },
    {
      "epoch": 2.49750049990002,
      "grad_norm": 3.272085189819336,
      "learning_rate": 8.374991668333001e-06,
      "loss": 0.5048,
      "step": 12490
    },
    {
      "epoch": 2.499500099980004,
      "grad_norm": 9.186380386352539,
      "learning_rate": 8.341665000333266e-06,
      "loss": 0.7568,
      "step": 12500
    },
    {
      "epoch": 2.501499700059988,
      "grad_norm": 17.42115592956543,
      "learning_rate": 8.308338332333533e-06,
      "loss": 0.6048,
      "step": 12510
    },
    {
      "epoch": 2.503499300139972,
      "grad_norm": 14.61000919342041,
      "learning_rate": 8.2750116643338e-06,
      "loss": 0.5895,
      "step": 12520
    },
    {
      "epoch": 2.505498900219956,
      "grad_norm": 6.576747417449951,
      "learning_rate": 8.241684996334067e-06,
      "loss": 0.4881,
      "step": 12530
    },
    {
      "epoch": 2.50749850029994,
      "grad_norm": 18.86730194091797,
      "learning_rate": 8.208358328334334e-06,
      "loss": 0.4938,
      "step": 12540
    },
    {
      "epoch": 2.509498100379924,
      "grad_norm": 37.26467514038086,
      "learning_rate": 8.1750316603346e-06,
      "loss": 0.3617,
      "step": 12550
    },
    {
      "epoch": 2.511497700459908,
      "grad_norm": 8.493760108947754,
      "learning_rate": 8.141704992334868e-06,
      "loss": 0.467,
      "step": 12560
    },
    {
      "epoch": 2.513497300539892,
      "grad_norm": 19.020151138305664,
      "learning_rate": 8.108378324335134e-06,
      "loss": 0.4197,
      "step": 12570
    },
    {
      "epoch": 2.515496900619876,
      "grad_norm": 5.2024245262146,
      "learning_rate": 8.0750516563354e-06,
      "loss": 0.5758,
      "step": 12580
    },
    {
      "epoch": 2.5174965006998598,
      "grad_norm": 17.538755416870117,
      "learning_rate": 8.041724988335666e-06,
      "loss": 0.6286,
      "step": 12590
    },
    {
      "epoch": 2.519496100779844,
      "grad_norm": 2.9791266918182373,
      "learning_rate": 8.008398320335933e-06,
      "loss": 0.3471,
      "step": 12600
    },
    {
      "epoch": 2.5214957008598278,
      "grad_norm": 11.246206283569336,
      "learning_rate": 7.9750716523362e-06,
      "loss": 0.4868,
      "step": 12610
    },
    {
      "epoch": 2.5234953009398122,
      "grad_norm": 21.717144012451172,
      "learning_rate": 7.941744984336465e-06,
      "loss": 0.7255,
      "step": 12620
    },
    {
      "epoch": 2.525494901019796,
      "grad_norm": 5.407480239868164,
      "learning_rate": 7.908418316336732e-06,
      "loss": 0.6225,
      "step": 12630
    },
    {
      "epoch": 2.5274945010997802,
      "grad_norm": 13.930490493774414,
      "learning_rate": 7.875091648337e-06,
      "loss": 0.4309,
      "step": 12640
    },
    {
      "epoch": 2.529494101179764,
      "grad_norm": 8.326804161071777,
      "learning_rate": 7.841764980337266e-06,
      "loss": 0.8262,
      "step": 12650
    },
    {
      "epoch": 2.5314937012597483,
      "grad_norm": 17.03827667236328,
      "learning_rate": 7.808438312337533e-06,
      "loss": 0.8919,
      "step": 12660
    },
    {
      "epoch": 2.533493301339732,
      "grad_norm": 9.503889083862305,
      "learning_rate": 7.7751116443378e-06,
      "loss": 0.5092,
      "step": 12670
    },
    {
      "epoch": 2.5354929014197163,
      "grad_norm": 3.377648115158081,
      "learning_rate": 7.741784976338067e-06,
      "loss": 0.4835,
      "step": 12680
    },
    {
      "epoch": 2.5374925014997,
      "grad_norm": 20.875797271728516,
      "learning_rate": 7.708458308338334e-06,
      "loss": 0.4908,
      "step": 12690
    },
    {
      "epoch": 2.5394921015796843,
      "grad_norm": 7.098145484924316,
      "learning_rate": 7.675131640338599e-06,
      "loss": 0.575,
      "step": 12700
    },
    {
      "epoch": 2.541491701659668,
      "grad_norm": 15.141084671020508,
      "learning_rate": 7.641804972338866e-06,
      "loss": 0.5363,
      "step": 12710
    },
    {
      "epoch": 2.5434913017396523,
      "grad_norm": 27.534709930419922,
      "learning_rate": 7.608478304339133e-06,
      "loss": 0.5318,
      "step": 12720
    },
    {
      "epoch": 2.545490901819636,
      "grad_norm": 25.963417053222656,
      "learning_rate": 7.575151636339399e-06,
      "loss": 0.5276,
      "step": 12730
    },
    {
      "epoch": 2.5474905018996203,
      "grad_norm": 19.011091232299805,
      "learning_rate": 7.541824968339666e-06,
      "loss": 0.6401,
      "step": 12740
    },
    {
      "epoch": 2.549490101979604,
      "grad_norm": 5.041396617889404,
      "learning_rate": 7.508498300339932e-06,
      "loss": 0.6119,
      "step": 12750
    },
    {
      "epoch": 2.551489702059588,
      "grad_norm": 7.438084125518799,
      "learning_rate": 7.4751716323401985e-06,
      "loss": 0.6134,
      "step": 12760
    },
    {
      "epoch": 2.553489302139572,
      "grad_norm": 10.56796646118164,
      "learning_rate": 7.441844964340465e-06,
      "loss": 0.4239,
      "step": 12770
    },
    {
      "epoch": 2.555488902219556,
      "grad_norm": 10.227032661437988,
      "learning_rate": 7.4085182963407315e-06,
      "loss": 0.5336,
      "step": 12780
    },
    {
      "epoch": 2.55748850229954,
      "grad_norm": 34.48260498046875,
      "learning_rate": 7.375191628340999e-06,
      "loss": 0.529,
      "step": 12790
    },
    {
      "epoch": 2.559488102379524,
      "grad_norm": 5.544749736785889,
      "learning_rate": 7.341864960341266e-06,
      "loss": 0.5846,
      "step": 12800
    },
    {
      "epoch": 2.561487702459508,
      "grad_norm": 4.110152721405029,
      "learning_rate": 7.308538292341532e-06,
      "loss": 0.5689,
      "step": 12810
    },
    {
      "epoch": 2.563487302539492,
      "grad_norm": 13.744709014892578,
      "learning_rate": 7.275211624341799e-06,
      "loss": 0.4719,
      "step": 12820
    },
    {
      "epoch": 2.565486902619476,
      "grad_norm": 14.52332878112793,
      "learning_rate": 7.241884956342065e-06,
      "loss": 0.3098,
      "step": 12830
    },
    {
      "epoch": 2.56748650269946,
      "grad_norm": 14.913752555847168,
      "learning_rate": 7.208558288342332e-06,
      "loss": 0.7067,
      "step": 12840
    },
    {
      "epoch": 2.569486102779444,
      "grad_norm": 17.13742446899414,
      "learning_rate": 7.175231620342599e-06,
      "loss": 0.5029,
      "step": 12850
    },
    {
      "epoch": 2.571485702859428,
      "grad_norm": 21.338747024536133,
      "learning_rate": 7.141904952342865e-06,
      "loss": 0.5553,
      "step": 12860
    },
    {
      "epoch": 2.573485302939412,
      "grad_norm": 21.355684280395508,
      "learning_rate": 7.108578284343132e-06,
      "loss": 0.8324,
      "step": 12870
    },
    {
      "epoch": 2.575484903019396,
      "grad_norm": 0.5910969972610474,
      "learning_rate": 7.075251616343398e-06,
      "loss": 0.2667,
      "step": 12880
    },
    {
      "epoch": 2.57748450309938,
      "grad_norm": 15.016289710998535,
      "learning_rate": 7.041924948343665e-06,
      "loss": 0.6957,
      "step": 12890
    },
    {
      "epoch": 2.579484103179364,
      "grad_norm": 9.142326354980469,
      "learning_rate": 7.008598280343931e-06,
      "loss": 0.5976,
      "step": 12900
    },
    {
      "epoch": 2.581483703259348,
      "grad_norm": 19.072019577026367,
      "learning_rate": 6.9752716123441985e-06,
      "loss": 0.4492,
      "step": 12910
    },
    {
      "epoch": 2.583483303339332,
      "grad_norm": 9.069493293762207,
      "learning_rate": 6.941944944344465e-06,
      "loss": 0.6067,
      "step": 12920
    },
    {
      "epoch": 2.585482903419316,
      "grad_norm": 14.266447067260742,
      "learning_rate": 6.908618276344732e-06,
      "loss": 0.3372,
      "step": 12930
    },
    {
      "epoch": 2.5874825034993,
      "grad_norm": 38.33949279785156,
      "learning_rate": 6.875291608344998e-06,
      "loss": 0.5613,
      "step": 12940
    },
    {
      "epoch": 2.589482103579284,
      "grad_norm": 17.566486358642578,
      "learning_rate": 6.841964940345265e-06,
      "loss": 0.4831,
      "step": 12950
    },
    {
      "epoch": 2.591481703659268,
      "grad_norm": 9.912144660949707,
      "learning_rate": 6.808638272345531e-06,
      "loss": 0.7555,
      "step": 12960
    },
    {
      "epoch": 2.593481303739252,
      "grad_norm": 19.810230255126953,
      "learning_rate": 6.775311604345798e-06,
      "loss": 0.6847,
      "step": 12970
    },
    {
      "epoch": 2.595480903819236,
      "grad_norm": 12.721879959106445,
      "learning_rate": 6.741984936346064e-06,
      "loss": 0.5761,
      "step": 12980
    },
    {
      "epoch": 2.59748050389922,
      "grad_norm": 8.640756607055664,
      "learning_rate": 6.708658268346331e-06,
      "loss": 0.3363,
      "step": 12990
    },
    {
      "epoch": 2.599480103979204,
      "grad_norm": 15.793054580688477,
      "learning_rate": 6.675331600346597e-06,
      "loss": 0.5528,
      "step": 13000
    },
    {
      "epoch": 2.601479704059188,
      "grad_norm": 10.260008811950684,
      "learning_rate": 6.642004932346864e-06,
      "loss": 0.6058,
      "step": 13010
    },
    {
      "epoch": 2.603479304139172,
      "grad_norm": 7.875279903411865,
      "learning_rate": 6.60867826434713e-06,
      "loss": 0.367,
      "step": 13020
    },
    {
      "epoch": 2.605478904219156,
      "grad_norm": 9.028461456298828,
      "learning_rate": 6.575351596347397e-06,
      "loss": 0.3958,
      "step": 13030
    },
    {
      "epoch": 2.60747850429914,
      "grad_norm": 21.9251766204834,
      "learning_rate": 6.542024928347665e-06,
      "loss": 0.7335,
      "step": 13040
    },
    {
      "epoch": 2.609478104379124,
      "grad_norm": 14.919160842895508,
      "learning_rate": 6.5086982603479316e-06,
      "loss": 0.5608,
      "step": 13050
    },
    {
      "epoch": 2.611477704459108,
      "grad_norm": 19.682401657104492,
      "learning_rate": 6.475371592348198e-06,
      "loss": 0.8029,
      "step": 13060
    },
    {
      "epoch": 2.613477304539092,
      "grad_norm": 19.229663848876953,
      "learning_rate": 6.4420449243484645e-06,
      "loss": 0.9213,
      "step": 13070
    },
    {
      "epoch": 2.6154769046190762,
      "grad_norm": 12.799182891845703,
      "learning_rate": 6.4087182563487305e-06,
      "loss": 0.4886,
      "step": 13080
    },
    {
      "epoch": 2.6174765046990602,
      "grad_norm": 16.168075561523438,
      "learning_rate": 6.3753915883489974e-06,
      "loss": 0.5005,
      "step": 13090
    },
    {
      "epoch": 2.6194761047790442,
      "grad_norm": 19.274185180664062,
      "learning_rate": 6.3420649203492635e-06,
      "loss": 0.5877,
      "step": 13100
    },
    {
      "epoch": 2.6214757048590283,
      "grad_norm": 7.645822048187256,
      "learning_rate": 6.30873825234953e-06,
      "loss": 0.5354,
      "step": 13110
    },
    {
      "epoch": 2.6234753049390123,
      "grad_norm": 9.337759971618652,
      "learning_rate": 6.275411584349796e-06,
      "loss": 0.3963,
      "step": 13120
    },
    {
      "epoch": 2.6254749050189963,
      "grad_norm": 16.781728744506836,
      "learning_rate": 6.242084916350063e-06,
      "loss": 0.9311,
      "step": 13130
    },
    {
      "epoch": 2.6274745050989803,
      "grad_norm": 12.485671997070312,
      "learning_rate": 6.20875824835033e-06,
      "loss": 0.3368,
      "step": 13140
    },
    {
      "epoch": 2.6294741051789643,
      "grad_norm": 11.107965469360352,
      "learning_rate": 6.175431580350597e-06,
      "loss": 0.506,
      "step": 13150
    },
    {
      "epoch": 2.6314737052589483,
      "grad_norm": 16.485212326049805,
      "learning_rate": 6.142104912350863e-06,
      "loss": 0.5418,
      "step": 13160
    },
    {
      "epoch": 2.6334733053389323,
      "grad_norm": 30.844396591186523,
      "learning_rate": 6.10877824435113e-06,
      "loss": 0.5319,
      "step": 13170
    },
    {
      "epoch": 2.6354729054189163,
      "grad_norm": 20.146305084228516,
      "learning_rate": 6.075451576351397e-06,
      "loss": 0.6581,
      "step": 13180
    },
    {
      "epoch": 2.6374725054989003,
      "grad_norm": 5.293666839599609,
      "learning_rate": 6.042124908351663e-06,
      "loss": 0.311,
      "step": 13190
    },
    {
      "epoch": 2.6394721055788843,
      "grad_norm": 5.372297286987305,
      "learning_rate": 6.00879824035193e-06,
      "loss": 0.4462,
      "step": 13200
    },
    {
      "epoch": 2.6414717056588684,
      "grad_norm": 9.616671562194824,
      "learning_rate": 5.975471572352197e-06,
      "loss": 0.4858,
      "step": 13210
    },
    {
      "epoch": 2.6434713057388524,
      "grad_norm": 14.654561042785645,
      "learning_rate": 5.942144904352464e-06,
      "loss": 0.5405,
      "step": 13220
    },
    {
      "epoch": 2.6454709058188364,
      "grad_norm": 13.36971664428711,
      "learning_rate": 5.90881823635273e-06,
      "loss": 0.4343,
      "step": 13230
    },
    {
      "epoch": 2.6474705058988204,
      "grad_norm": 12.211657524108887,
      "learning_rate": 5.8754915683529966e-06,
      "loss": 0.5082,
      "step": 13240
    },
    {
      "epoch": 2.6494701059788044,
      "grad_norm": 13.699516296386719,
      "learning_rate": 5.842164900353263e-06,
      "loss": 0.5363,
      "step": 13250
    },
    {
      "epoch": 2.6514697060587884,
      "grad_norm": 13.40745735168457,
      "learning_rate": 5.8088382323535295e-06,
      "loss": 0.6164,
      "step": 13260
    },
    {
      "epoch": 2.6534693061387724,
      "grad_norm": 10.285922050476074,
      "learning_rate": 5.775511564353796e-06,
      "loss": 0.4393,
      "step": 13270
    },
    {
      "epoch": 2.655468906218756,
      "grad_norm": 19.922325134277344,
      "learning_rate": 5.742184896354063e-06,
      "loss": 0.6245,
      "step": 13280
    },
    {
      "epoch": 2.6574685062987404,
      "grad_norm": 14.490461349487305,
      "learning_rate": 5.708858228354329e-06,
      "loss": 0.4339,
      "step": 13290
    },
    {
      "epoch": 2.659468106378724,
      "grad_norm": 6.020258903503418,
      "learning_rate": 5.675531560354596e-06,
      "loss": 0.7426,
      "step": 13300
    },
    {
      "epoch": 2.6614677064587084,
      "grad_norm": 17.88728904724121,
      "learning_rate": 5.642204892354862e-06,
      "loss": 0.7636,
      "step": 13310
    },
    {
      "epoch": 2.663467306538692,
      "grad_norm": 26.86644172668457,
      "learning_rate": 5.608878224355129e-06,
      "loss": 0.6355,
      "step": 13320
    },
    {
      "epoch": 2.6654669066186765,
      "grad_norm": 17.492700576782227,
      "learning_rate": 5.575551556355396e-06,
      "loss": 0.5429,
      "step": 13330
    },
    {
      "epoch": 2.66746650669866,
      "grad_norm": 14.670713424682617,
      "learning_rate": 5.542224888355663e-06,
      "loss": 0.5673,
      "step": 13340
    },
    {
      "epoch": 2.6694661067786445,
      "grad_norm": 12.59174919128418,
      "learning_rate": 5.508898220355929e-06,
      "loss": 0.6268,
      "step": 13350
    },
    {
      "epoch": 2.671465706858628,
      "grad_norm": 10.525565147399902,
      "learning_rate": 5.475571552356196e-06,
      "loss": 0.563,
      "step": 13360
    },
    {
      "epoch": 2.6734653069386125,
      "grad_norm": 14.022921562194824,
      "learning_rate": 5.442244884356462e-06,
      "loss": 0.4363,
      "step": 13370
    },
    {
      "epoch": 2.675464907018596,
      "grad_norm": 1.9461108446121216,
      "learning_rate": 5.408918216356729e-06,
      "loss": 0.4214,
      "step": 13380
    },
    {
      "epoch": 2.6774645070985805,
      "grad_norm": 9.70068359375,
      "learning_rate": 5.375591548356995e-06,
      "loss": 0.4907,
      "step": 13390
    },
    {
      "epoch": 2.679464107178564,
      "grad_norm": 12.678255081176758,
      "learning_rate": 5.3422648803572626e-06,
      "loss": 0.4601,
      "step": 13400
    },
    {
      "epoch": 2.6814637072585485,
      "grad_norm": 14.712223052978516,
      "learning_rate": 5.308938212357529e-06,
      "loss": 0.5422,
      "step": 13410
    },
    {
      "epoch": 2.683463307338532,
      "grad_norm": 22.19735336303711,
      "learning_rate": 5.2756115443577955e-06,
      "loss": 0.4236,
      "step": 13420
    },
    {
      "epoch": 2.6854629074185166,
      "grad_norm": 15.676470756530762,
      "learning_rate": 5.2422848763580615e-06,
      "loss": 0.4624,
      "step": 13430
    },
    {
      "epoch": 2.6874625074985,
      "grad_norm": 9.925934791564941,
      "learning_rate": 5.2089582083583284e-06,
      "loss": 0.4791,
      "step": 13440
    },
    {
      "epoch": 2.689462107578484,
      "grad_norm": 13.470216751098633,
      "learning_rate": 5.175631540358595e-06,
      "loss": 0.5248,
      "step": 13450
    },
    {
      "epoch": 2.691461707658468,
      "grad_norm": 16.03627586364746,
      "learning_rate": 5.142304872358862e-06,
      "loss": 0.6045,
      "step": 13460
    },
    {
      "epoch": 2.693461307738452,
      "grad_norm": 14.277728080749512,
      "learning_rate": 5.108978204359128e-06,
      "loss": 0.5235,
      "step": 13470
    },
    {
      "epoch": 2.695460907818436,
      "grad_norm": 13.573565483093262,
      "learning_rate": 5.075651536359395e-06,
      "loss": 0.638,
      "step": 13480
    },
    {
      "epoch": 2.69746050789842,
      "grad_norm": 24.365530014038086,
      "learning_rate": 5.042324868359662e-06,
      "loss": 0.7378,
      "step": 13490
    },
    {
      "epoch": 2.699460107978404,
      "grad_norm": 34.868507385253906,
      "learning_rate": 5.008998200359928e-06,
      "loss": 0.6722,
      "step": 13500
    },
    {
      "epoch": 2.701459708058388,
      "grad_norm": 4.850193500518799,
      "learning_rate": 4.975671532360195e-06,
      "loss": 0.634,
      "step": 13510
    },
    {
      "epoch": 2.703459308138372,
      "grad_norm": 27.427978515625,
      "learning_rate": 4.942344864360462e-06,
      "loss": 0.4975,
      "step": 13520
    },
    {
      "epoch": 2.705458908218356,
      "grad_norm": 20.063011169433594,
      "learning_rate": 4.909018196360728e-06,
      "loss": 0.7614,
      "step": 13530
    },
    {
      "epoch": 2.7074585082983402,
      "grad_norm": 1.7363289594650269,
      "learning_rate": 4.875691528360995e-06,
      "loss": 0.5836,
      "step": 13540
    },
    {
      "epoch": 2.7094581083783242,
      "grad_norm": 17.92790412902832,
      "learning_rate": 4.842364860361262e-06,
      "loss": 0.3437,
      "step": 13550
    },
    {
      "epoch": 2.7114577084583082,
      "grad_norm": 23.28191375732422,
      "learning_rate": 4.809038192361528e-06,
      "loss": 0.5777,
      "step": 13560
    },
    {
      "epoch": 2.7134573085382923,
      "grad_norm": 12.007708549499512,
      "learning_rate": 4.775711524361795e-06,
      "loss": 0.5883,
      "step": 13570
    },
    {
      "epoch": 2.7154569086182763,
      "grad_norm": 18.63507843017578,
      "learning_rate": 4.742384856362061e-06,
      "loss": 0.3825,
      "step": 13580
    },
    {
      "epoch": 2.7174565086982603,
      "grad_norm": 12.776521682739258,
      "learning_rate": 4.709058188362328e-06,
      "loss": 0.7107,
      "step": 13590
    },
    {
      "epoch": 2.7194561087782443,
      "grad_norm": 16.876869201660156,
      "learning_rate": 4.6757315203625944e-06,
      "loss": 0.6322,
      "step": 13600
    },
    {
      "epoch": 2.7214557088582283,
      "grad_norm": 7.57696533203125,
      "learning_rate": 4.642404852362861e-06,
      "loss": 0.5628,
      "step": 13610
    },
    {
      "epoch": 2.7234553089382123,
      "grad_norm": 22.190950393676758,
      "learning_rate": 4.609078184363127e-06,
      "loss": 0.6134,
      "step": 13620
    },
    {
      "epoch": 2.7254549090181963,
      "grad_norm": 23.76473045349121,
      "learning_rate": 4.575751516363394e-06,
      "loss": 0.4993,
      "step": 13630
    },
    {
      "epoch": 2.7274545090981803,
      "grad_norm": 11.775202751159668,
      "learning_rate": 4.54242484836366e-06,
      "loss": 0.6419,
      "step": 13640
    },
    {
      "epoch": 2.7294541091781643,
      "grad_norm": 11.343125343322754,
      "learning_rate": 4.509098180363928e-06,
      "loss": 0.4315,
      "step": 13650
    },
    {
      "epoch": 2.7314537092581483,
      "grad_norm": 39.57400131225586,
      "learning_rate": 4.475771512364194e-06,
      "loss": 0.6092,
      "step": 13660
    },
    {
      "epoch": 2.7334533093381324,
      "grad_norm": 10.83895206451416,
      "learning_rate": 4.442444844364461e-06,
      "loss": 0.4761,
      "step": 13670
    },
    {
      "epoch": 2.7354529094181164,
      "grad_norm": 19.728008270263672,
      "learning_rate": 4.409118176364727e-06,
      "loss": 0.9474,
      "step": 13680
    },
    {
      "epoch": 2.7374525094981004,
      "grad_norm": 6.1983795166015625,
      "learning_rate": 4.375791508364994e-06,
      "loss": 0.5419,
      "step": 13690
    },
    {
      "epoch": 2.7394521095780844,
      "grad_norm": 15.460896492004395,
      "learning_rate": 4.34246484036526e-06,
      "loss": 0.6285,
      "step": 13700
    },
    {
      "epoch": 2.7414517096580684,
      "grad_norm": 16.292394638061523,
      "learning_rate": 4.309138172365527e-06,
      "loss": 0.6125,
      "step": 13710
    },
    {
      "epoch": 2.7434513097380524,
      "grad_norm": 20.7459659576416,
      "learning_rate": 4.275811504365794e-06,
      "loss": 0.7853,
      "step": 13720
    },
    {
      "epoch": 2.7454509098180364,
      "grad_norm": 21.469526290893555,
      "learning_rate": 4.242484836366061e-06,
      "loss": 0.4768,
      "step": 13730
    },
    {
      "epoch": 2.7474505098980204,
      "grad_norm": 5.022580623626709,
      "learning_rate": 4.209158168366327e-06,
      "loss": 0.3569,
      "step": 13740
    },
    {
      "epoch": 2.7494501099780044,
      "grad_norm": 5.815395355224609,
      "learning_rate": 4.1758315003665936e-06,
      "loss": 0.4662,
      "step": 13750
    },
    {
      "epoch": 2.7514497100579884,
      "grad_norm": 17.712839126586914,
      "learning_rate": 4.14250483236686e-06,
      "loss": 0.6925,
      "step": 13760
    },
    {
      "epoch": 2.7534493101379725,
      "grad_norm": 20.534475326538086,
      "learning_rate": 4.1091781643671265e-06,
      "loss": 0.8115,
      "step": 13770
    },
    {
      "epoch": 2.7554489102179565,
      "grad_norm": 14.485713958740234,
      "learning_rate": 4.075851496367393e-06,
      "loss": 0.6108,
      "step": 13780
    },
    {
      "epoch": 2.7574485102979405,
      "grad_norm": 5.722243785858154,
      "learning_rate": 4.04252482836766e-06,
      "loss": 0.644,
      "step": 13790
    },
    {
      "epoch": 2.7594481103779245,
      "grad_norm": 10.285431861877441,
      "learning_rate": 4.009198160367926e-06,
      "loss": 0.5421,
      "step": 13800
    },
    {
      "epoch": 2.7614477104579085,
      "grad_norm": 15.874489784240723,
      "learning_rate": 3.975871492368193e-06,
      "loss": 0.505,
      "step": 13810
    },
    {
      "epoch": 2.7634473105378925,
      "grad_norm": 12.441349983215332,
      "learning_rate": 3.94254482436846e-06,
      "loss": 0.6291,
      "step": 13820
    },
    {
      "epoch": 2.7654469106178765,
      "grad_norm": 11.45032024383545,
      "learning_rate": 3.909218156368726e-06,
      "loss": 0.477,
      "step": 13830
    },
    {
      "epoch": 2.7674465106978605,
      "grad_norm": 17.62639045715332,
      "learning_rate": 3.875891488368993e-06,
      "loss": 0.5818,
      "step": 13840
    },
    {
      "epoch": 2.7694461107778445,
      "grad_norm": 16.507877349853516,
      "learning_rate": 3.84256482036926e-06,
      "loss": 0.6363,
      "step": 13850
    },
    {
      "epoch": 2.7714457108578285,
      "grad_norm": 1.9221603870391846,
      "learning_rate": 3.8092381523695264e-06,
      "loss": 0.512,
      "step": 13860
    },
    {
      "epoch": 2.7734453109378125,
      "grad_norm": 15.194464683532715,
      "learning_rate": 3.775911484369793e-06,
      "loss": 0.5896,
      "step": 13870
    },
    {
      "epoch": 2.7754449110177966,
      "grad_norm": 29.974884033203125,
      "learning_rate": 3.7425848163700593e-06,
      "loss": 0.8048,
      "step": 13880
    },
    {
      "epoch": 2.7774445110977806,
      "grad_norm": 9.313289642333984,
      "learning_rate": 3.7092581483703258e-06,
      "loss": 0.3893,
      "step": 13890
    },
    {
      "epoch": 2.7794441111777646,
      "grad_norm": 11.778144836425781,
      "learning_rate": 3.6759314803705923e-06,
      "loss": 0.5578,
      "step": 13900
    },
    {
      "epoch": 2.7814437112577486,
      "grad_norm": 9.581108093261719,
      "learning_rate": 3.6426048123708596e-06,
      "loss": 0.547,
      "step": 13910
    },
    {
      "epoch": 2.7834433113377326,
      "grad_norm": 13.550057411193848,
      "learning_rate": 3.609278144371126e-06,
      "loss": 0.4063,
      "step": 13920
    },
    {
      "epoch": 2.7854429114177166,
      "grad_norm": 26.30320930480957,
      "learning_rate": 3.5759514763713925e-06,
      "loss": 0.7171,
      "step": 13930
    },
    {
      "epoch": 2.7874425114977006,
      "grad_norm": 18.064531326293945,
      "learning_rate": 3.542624808371659e-06,
      "loss": 0.6811,
      "step": 13940
    },
    {
      "epoch": 2.7894421115776846,
      "grad_norm": 18.481672286987305,
      "learning_rate": 3.5092981403719254e-06,
      "loss": 0.4158,
      "step": 13950
    },
    {
      "epoch": 2.7914417116576686,
      "grad_norm": 10.972966194152832,
      "learning_rate": 3.4759714723721923e-06,
      "loss": 0.6467,
      "step": 13960
    },
    {
      "epoch": 2.793441311737652,
      "grad_norm": 13.392404556274414,
      "learning_rate": 3.442644804372459e-06,
      "loss": 0.6439,
      "step": 13970
    },
    {
      "epoch": 2.7954409118176367,
      "grad_norm": 3.3258767127990723,
      "learning_rate": 3.4093181363727257e-06,
      "loss": 0.5647,
      "step": 13980
    },
    {
      "epoch": 2.79744051189762,
      "grad_norm": 18.826549530029297,
      "learning_rate": 3.375991468372992e-06,
      "loss": 0.6081,
      "step": 13990
    },
    {
      "epoch": 2.7994401119776047,
      "grad_norm": 19.940580368041992,
      "learning_rate": 3.342664800373259e-06,
      "loss": 0.4172,
      "step": 14000
    },
    {
      "epoch": 2.8014397120575882,
      "grad_norm": 6.139245510101318,
      "learning_rate": 3.3093381323735255e-06,
      "loss": 0.4753,
      "step": 14010
    },
    {
      "epoch": 2.8034393121375727,
      "grad_norm": 18.37113380432129,
      "learning_rate": 3.276011464373792e-06,
      "loss": 0.5045,
      "step": 14020
    },
    {
      "epoch": 2.8054389122175563,
      "grad_norm": 6.731175422668457,
      "learning_rate": 3.2426847963740584e-06,
      "loss": 0.5113,
      "step": 14030
    },
    {
      "epoch": 2.8074385122975407,
      "grad_norm": 23.27562141418457,
      "learning_rate": 3.2093581283743257e-06,
      "loss": 0.5649,
      "step": 14040
    },
    {
      "epoch": 2.8094381123775243,
      "grad_norm": 17.660573959350586,
      "learning_rate": 3.1760314603745922e-06,
      "loss": 0.7752,
      "step": 14050
    },
    {
      "epoch": 2.8114377124575087,
      "grad_norm": 15.328600883483887,
      "learning_rate": 3.1427047923748587e-06,
      "loss": 0.5278,
      "step": 14060
    },
    {
      "epoch": 2.8134373125374923,
      "grad_norm": 11.44608211517334,
      "learning_rate": 3.109378124375125e-06,
      "loss": 0.7157,
      "step": 14070
    },
    {
      "epoch": 2.8154369126174768,
      "grad_norm": 8.359935760498047,
      "learning_rate": 3.0760514563753916e-06,
      "loss": 0.4476,
      "step": 14080
    },
    {
      "epoch": 2.8174365126974603,
      "grad_norm": 8.152077674865723,
      "learning_rate": 3.0427247883756585e-06,
      "loss": 0.6356,
      "step": 14090
    },
    {
      "epoch": 2.8194361127774448,
      "grad_norm": 22.831867218017578,
      "learning_rate": 3.009398120375925e-06,
      "loss": 0.5782,
      "step": 14100
    },
    {
      "epoch": 2.8214357128574283,
      "grad_norm": 2.591573476791382,
      "learning_rate": 2.9760714523761914e-06,
      "loss": 0.3773,
      "step": 14110
    },
    {
      "epoch": 2.823435312937413,
      "grad_norm": 10.44777774810791,
      "learning_rate": 2.9427447843764583e-06,
      "loss": 0.5362,
      "step": 14120
    },
    {
      "epoch": 2.8254349130173964,
      "grad_norm": 12.45600700378418,
      "learning_rate": 2.909418116376725e-06,
      "loss": 0.5005,
      "step": 14130
    },
    {
      "epoch": 2.8274345130973804,
      "grad_norm": 15.265743255615234,
      "learning_rate": 2.8760914483769913e-06,
      "loss": 0.5084,
      "step": 14140
    },
    {
      "epoch": 2.8294341131773644,
      "grad_norm": 13.875303268432617,
      "learning_rate": 2.842764780377258e-06,
      "loss": 0.4148,
      "step": 14150
    },
    {
      "epoch": 2.8314337132573484,
      "grad_norm": 10.879239082336426,
      "learning_rate": 2.8094381123775246e-06,
      "loss": 0.6792,
      "step": 14160
    },
    {
      "epoch": 2.8334333133373324,
      "grad_norm": 25.765941619873047,
      "learning_rate": 2.776111444377791e-06,
      "loss": 0.6758,
      "step": 14170
    },
    {
      "epoch": 2.8354329134173164,
      "grad_norm": 18.371763229370117,
      "learning_rate": 2.742784776378058e-06,
      "loss": 0.4978,
      "step": 14180
    },
    {
      "epoch": 2.8374325134973004,
      "grad_norm": 10.96041202545166,
      "learning_rate": 2.7094581083783244e-06,
      "loss": 0.42,
      "step": 14190
    },
    {
      "epoch": 2.8394321135772844,
      "grad_norm": 16.85222816467285,
      "learning_rate": 2.676131440378591e-06,
      "loss": 0.6553,
      "step": 14200
    },
    {
      "epoch": 2.8414317136572684,
      "grad_norm": 11.678606033325195,
      "learning_rate": 2.642804772378858e-06,
      "loss": 0.5001,
      "step": 14210
    },
    {
      "epoch": 2.8434313137372524,
      "grad_norm": 19.509906768798828,
      "learning_rate": 2.6094781043791243e-06,
      "loss": 0.5437,
      "step": 14220
    },
    {
      "epoch": 2.8454309138172365,
      "grad_norm": 10.584684371948242,
      "learning_rate": 2.5761514363793907e-06,
      "loss": 0.5243,
      "step": 14230
    },
    {
      "epoch": 2.8474305138972205,
      "grad_norm": 6.294673442840576,
      "learning_rate": 2.542824768379657e-06,
      "loss": 0.5467,
      "step": 14240
    },
    {
      "epoch": 2.8494301139772045,
      "grad_norm": 17.39594078063965,
      "learning_rate": 2.509498100379924e-06,
      "loss": 0.4896,
      "step": 14250
    },
    {
      "epoch": 2.8514297140571885,
      "grad_norm": 23.229782104492188,
      "learning_rate": 2.4761714323801906e-06,
      "loss": 0.4479,
      "step": 14260
    },
    {
      "epoch": 2.8534293141371725,
      "grad_norm": 31.52298927307129,
      "learning_rate": 2.442844764380457e-06,
      "loss": 0.4278,
      "step": 14270
    },
    {
      "epoch": 2.8554289142171565,
      "grad_norm": 21.423580169677734,
      "learning_rate": 2.409518096380724e-06,
      "loss": 0.5439,
      "step": 14280
    },
    {
      "epoch": 2.8574285142971405,
      "grad_norm": 25.274927139282227,
      "learning_rate": 2.3761914283809904e-06,
      "loss": 0.5638,
      "step": 14290
    },
    {
      "epoch": 2.8594281143771245,
      "grad_norm": 15.378521919250488,
      "learning_rate": 2.3428647603812573e-06,
      "loss": 0.5795,
      "step": 14300
    },
    {
      "epoch": 2.8614277144571085,
      "grad_norm": 11.926351547241211,
      "learning_rate": 2.3095380923815237e-06,
      "loss": 0.468,
      "step": 14310
    },
    {
      "epoch": 2.8634273145370925,
      "grad_norm": 18.080425262451172,
      "learning_rate": 2.2762114243817906e-06,
      "loss": 0.5226,
      "step": 14320
    },
    {
      "epoch": 2.8654269146170765,
      "grad_norm": 17.540319442749023,
      "learning_rate": 2.242884756382057e-06,
      "loss": 0.6111,
      "step": 14330
    },
    {
      "epoch": 2.8674265146970606,
      "grad_norm": 13.671778678894043,
      "learning_rate": 2.209558088382324e-06,
      "loss": 0.4435,
      "step": 14340
    },
    {
      "epoch": 2.8694261147770446,
      "grad_norm": 27.564682006835938,
      "learning_rate": 2.1762314203825905e-06,
      "loss": 0.534,
      "step": 14350
    },
    {
      "epoch": 2.8714257148570286,
      "grad_norm": 6.427083492279053,
      "learning_rate": 2.142904752382857e-06,
      "loss": 0.4097,
      "step": 14360
    },
    {
      "epoch": 2.8734253149370126,
      "grad_norm": 6.469577312469482,
      "learning_rate": 2.1095780843831234e-06,
      "loss": 0.634,
      "step": 14370
    },
    {
      "epoch": 2.8754249150169966,
      "grad_norm": 19.02607536315918,
      "learning_rate": 2.0762514163833903e-06,
      "loss": 0.567,
      "step": 14380
    },
    {
      "epoch": 2.8774245150969806,
      "grad_norm": 17.007678985595703,
      "learning_rate": 2.0429247483836567e-06,
      "loss": 0.6596,
      "step": 14390
    },
    {
      "epoch": 2.8794241151769646,
      "grad_norm": 30.116016387939453,
      "learning_rate": 2.009598080383923e-06,
      "loss": 0.657,
      "step": 14400
    },
    {
      "epoch": 2.8814237152569486,
      "grad_norm": 8.298633575439453,
      "learning_rate": 1.97627141238419e-06,
      "loss": 0.4472,
      "step": 14410
    },
    {
      "epoch": 2.8834233153369326,
      "grad_norm": 12.958552360534668,
      "learning_rate": 1.9429447443844566e-06,
      "loss": 0.518,
      "step": 14420
    },
    {
      "epoch": 2.8854229154169166,
      "grad_norm": 25.83853530883789,
      "learning_rate": 1.909618076384723e-06,
      "loss": 0.3887,
      "step": 14430
    },
    {
      "epoch": 2.8874225154969007,
      "grad_norm": 44.05921173095703,
      "learning_rate": 1.87629140838499e-06,
      "loss": 0.5415,
      "step": 14440
    },
    {
      "epoch": 2.8894221155768847,
      "grad_norm": 4.984256267547607,
      "learning_rate": 1.8429647403852564e-06,
      "loss": 0.2923,
      "step": 14450
    },
    {
      "epoch": 2.8914217156568687,
      "grad_norm": 11.381597518920898,
      "learning_rate": 1.8096380723855229e-06,
      "loss": 0.5309,
      "step": 14460
    },
    {
      "epoch": 2.8934213157368527,
      "grad_norm": 13.73614501953125,
      "learning_rate": 1.7763114043857897e-06,
      "loss": 0.4467,
      "step": 14470
    },
    {
      "epoch": 2.8954209158168367,
      "grad_norm": 39.62202835083008,
      "learning_rate": 1.7429847363860562e-06,
      "loss": 0.599,
      "step": 14480
    },
    {
      "epoch": 2.8974205158968207,
      "grad_norm": 3.6323046684265137,
      "learning_rate": 1.7096580683863227e-06,
      "loss": 0.7858,
      "step": 14490
    },
    {
      "epoch": 2.8994201159768047,
      "grad_norm": 11.077032089233398,
      "learning_rate": 1.6763314003865896e-06,
      "loss": 0.5407,
      "step": 14500
    },
    {
      "epoch": 2.9014197160567887,
      "grad_norm": 12.311812400817871,
      "learning_rate": 1.643004732386856e-06,
      "loss": 0.6419,
      "step": 14510
    },
    {
      "epoch": 2.9034193161367727,
      "grad_norm": 18.423051834106445,
      "learning_rate": 1.6096780643871225e-06,
      "loss": 0.5325,
      "step": 14520
    },
    {
      "epoch": 2.9054189162167567,
      "grad_norm": 17.692392349243164,
      "learning_rate": 1.5763513963873892e-06,
      "loss": 0.4418,
      "step": 14530
    },
    {
      "epoch": 2.9074185162967408,
      "grad_norm": 8.055622100830078,
      "learning_rate": 1.5430247283876559e-06,
      "loss": 0.7052,
      "step": 14540
    },
    {
      "epoch": 2.9094181163767248,
      "grad_norm": 13.298259735107422,
      "learning_rate": 1.5096980603879225e-06,
      "loss": 0.4347,
      "step": 14550
    },
    {
      "epoch": 2.9114177164567088,
      "grad_norm": 19.459041595458984,
      "learning_rate": 1.4763713923881892e-06,
      "loss": 0.5099,
      "step": 14560
    },
    {
      "epoch": 2.913417316536693,
      "grad_norm": 14.877883911132812,
      "learning_rate": 1.4430447243884557e-06,
      "loss": 0.5074,
      "step": 14570
    },
    {
      "epoch": 2.915416916616677,
      "grad_norm": 19.423240661621094,
      "learning_rate": 1.4097180563887224e-06,
      "loss": 0.4813,
      "step": 14580
    },
    {
      "epoch": 2.917416516696661,
      "grad_norm": 12.206805229187012,
      "learning_rate": 1.376391388388989e-06,
      "loss": 0.6038,
      "step": 14590
    },
    {
      "epoch": 2.919416116776645,
      "grad_norm": 17.770164489746094,
      "learning_rate": 1.3430647203892555e-06,
      "loss": 0.749,
      "step": 14600
    },
    {
      "epoch": 2.921415716856629,
      "grad_norm": 16.320253372192383,
      "learning_rate": 1.3097380523895222e-06,
      "loss": 0.4477,
      "step": 14610
    },
    {
      "epoch": 2.923415316936613,
      "grad_norm": 28.990144729614258,
      "learning_rate": 1.2764113843897889e-06,
      "loss": 0.7053,
      "step": 14620
    },
    {
      "epoch": 2.925414917016597,
      "grad_norm": 1.9013293981552124,
      "learning_rate": 1.2430847163900553e-06,
      "loss": 0.4643,
      "step": 14630
    },
    {
      "epoch": 2.927414517096581,
      "grad_norm": 9.360819816589355,
      "learning_rate": 1.209758048390322e-06,
      "loss": 0.4151,
      "step": 14640
    },
    {
      "epoch": 2.929414117176565,
      "grad_norm": 4.456939220428467,
      "learning_rate": 1.1764313803905885e-06,
      "loss": 0.5068,
      "step": 14650
    },
    {
      "epoch": 2.9314137172565484,
      "grad_norm": 14.130677223205566,
      "learning_rate": 1.1431047123908552e-06,
      "loss": 0.4683,
      "step": 14660
    },
    {
      "epoch": 2.933413317336533,
      "grad_norm": 13.29350757598877,
      "learning_rate": 1.1097780443911218e-06,
      "loss": 0.3565,
      "step": 14670
    },
    {
      "epoch": 2.9354129174165164,
      "grad_norm": 11.790821075439453,
      "learning_rate": 1.0764513763913883e-06,
      "loss": 0.527,
      "step": 14680
    },
    {
      "epoch": 2.937412517496501,
      "grad_norm": 21.527326583862305,
      "learning_rate": 1.043124708391655e-06,
      "loss": 0.4684,
      "step": 14690
    },
    {
      "epoch": 2.9394121175764845,
      "grad_norm": 1.0521749258041382,
      "learning_rate": 1.0097980403919217e-06,
      "loss": 0.3784,
      "step": 14700
    },
    {
      "epoch": 2.941411717656469,
      "grad_norm": 9.798148155212402,
      "learning_rate": 9.764713723921883e-07,
      "loss": 0.5002,
      "step": 14710
    },
    {
      "epoch": 2.9434113177364525,
      "grad_norm": 9.51648235321045,
      "learning_rate": 9.431447043924549e-07,
      "loss": 0.6873,
      "step": 14720
    },
    {
      "epoch": 2.945410917816437,
      "grad_norm": 26.999128341674805,
      "learning_rate": 9.098180363927215e-07,
      "loss": 0.4847,
      "step": 14730
    },
    {
      "epoch": 2.9474105178964205,
      "grad_norm": 0.7184821963310242,
      "learning_rate": 8.764913683929882e-07,
      "loss": 0.5151,
      "step": 14740
    },
    {
      "epoch": 2.949410117976405,
      "grad_norm": 18.339258193969727,
      "learning_rate": 8.431647003932548e-07,
      "loss": 0.4812,
      "step": 14750
    },
    {
      "epoch": 2.9514097180563885,
      "grad_norm": 7.9660515785217285,
      "learning_rate": 8.098380323935213e-07,
      "loss": 0.6189,
      "step": 14760
    },
    {
      "epoch": 2.953409318136373,
      "grad_norm": 8.69536018371582,
      "learning_rate": 7.76511364393788e-07,
      "loss": 0.4588,
      "step": 14770
    },
    {
      "epoch": 2.9554089182163565,
      "grad_norm": 4.547765731811523,
      "learning_rate": 7.431846963940546e-07,
      "loss": 0.5058,
      "step": 14780
    },
    {
      "epoch": 2.957408518296341,
      "grad_norm": 12.75902271270752,
      "learning_rate": 7.098580283943211e-07,
      "loss": 0.6433,
      "step": 14790
    },
    {
      "epoch": 2.9594081183763246,
      "grad_norm": 13.338336944580078,
      "learning_rate": 6.765313603945878e-07,
      "loss": 0.5607,
      "step": 14800
    },
    {
      "epoch": 2.961407718456309,
      "grad_norm": 24.034025192260742,
      "learning_rate": 6.432046923948544e-07,
      "loss": 0.4389,
      "step": 14810
    },
    {
      "epoch": 2.9634073185362926,
      "grad_norm": 8.493785858154297,
      "learning_rate": 6.098780243951211e-07,
      "loss": 0.6741,
      "step": 14820
    },
    {
      "epoch": 2.9654069186162766,
      "grad_norm": 11.784059524536133,
      "learning_rate": 5.765513563953876e-07,
      "loss": 0.5557,
      "step": 14830
    },
    {
      "epoch": 2.9674065186962606,
      "grad_norm": 15.59613037109375,
      "learning_rate": 5.432246883956542e-07,
      "loss": 0.4253,
      "step": 14840
    },
    {
      "epoch": 2.9694061187762446,
      "grad_norm": 17.322940826416016,
      "learning_rate": 5.098980203959209e-07,
      "loss": 0.5787,
      "step": 14850
    },
    {
      "epoch": 2.9714057188562286,
      "grad_norm": 12.68842887878418,
      "learning_rate": 4.7657135239618745e-07,
      "loss": 0.7395,
      "step": 14860
    },
    {
      "epoch": 2.9734053189362126,
      "grad_norm": 6.8614702224731445,
      "learning_rate": 4.43244684396454e-07,
      "loss": 0.661,
      "step": 14870
    },
    {
      "epoch": 2.9754049190161966,
      "grad_norm": 22.277143478393555,
      "learning_rate": 4.0991801639672065e-07,
      "loss": 0.366,
      "step": 14880
    },
    {
      "epoch": 2.9774045190961806,
      "grad_norm": 24.938249588012695,
      "learning_rate": 3.765913483969873e-07,
      "loss": 0.7213,
      "step": 14890
    },
    {
      "epoch": 2.9794041191761647,
      "grad_norm": 17.126893997192383,
      "learning_rate": 3.432646803972539e-07,
      "loss": 0.4661,
      "step": 14900
    },
    {
      "epoch": 2.9814037192561487,
      "grad_norm": 13.546854972839355,
      "learning_rate": 3.0993801239752047e-07,
      "loss": 0.874,
      "step": 14910
    },
    {
      "epoch": 2.9834033193361327,
      "grad_norm": 4.7275238037109375,
      "learning_rate": 2.7661134439778715e-07,
      "loss": 0.5204,
      "step": 14920
    },
    {
      "epoch": 2.9854029194161167,
      "grad_norm": 20.718353271484375,
      "learning_rate": 2.432846763980537e-07,
      "loss": 0.3449,
      "step": 14930
    },
    {
      "epoch": 2.9874025194961007,
      "grad_norm": 16.450010299682617,
      "learning_rate": 2.0995800839832035e-07,
      "loss": 0.7197,
      "step": 14940
    },
    {
      "epoch": 2.9894021195760847,
      "grad_norm": 44.79452896118164,
      "learning_rate": 1.7663134039858697e-07,
      "loss": 0.7921,
      "step": 14950
    },
    {
      "epoch": 2.9914017196560687,
      "grad_norm": 20.52993392944336,
      "learning_rate": 1.4330467239885357e-07,
      "loss": 0.4128,
      "step": 14960
    },
    {
      "epoch": 2.9934013197360527,
      "grad_norm": 11.352556228637695,
      "learning_rate": 1.0997800439912019e-07,
      "loss": 0.5608,
      "step": 14970
    },
    {
      "epoch": 2.9954009198160367,
      "grad_norm": 23.399635314941406,
      "learning_rate": 7.665133639938679e-08,
      "loss": 0.7629,
      "step": 14980
    },
    {
      "epoch": 2.9974005198960207,
      "grad_norm": 19.426925659179688,
      "learning_rate": 4.3324668399653404e-08,
      "loss": 0.4915,
      "step": 14990
    },
    {
      "epoch": 2.9994001199760048,
      "grad_norm": 7.316257953643799,
      "learning_rate": 9.998000399920016e-09,
      "loss": 0.5148,
      "step": 15000
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6666666666666666,
      "eval_loss": 1.2304457426071167,
      "eval_runtime": 42.8274,
      "eval_samples_per_second": 233.542,
      "eval_steps_per_second": 29.21,
      "step": 15003
    }
  ],
  "logging_steps": 10,
  "max_steps": 15003,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7895477408845824.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
